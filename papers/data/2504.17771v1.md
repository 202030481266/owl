## **Integrating Learning-Based Manipulation and Physics-Based** **Locomotion for Whole-Body Badminton Robot Control**
### Haochen Wang [1], Zhiwei Shi [1], Chengxi Zhu [1], Yafei Qiao [1], Cheng Zhang [2], Fan Yang [3], Pengjie Ren [1], Lan Lu [4] [†], and Dong Xuan [1]


***Abstract*** **— Learning-based methods, such as imitation learn-**
**ing (IL) and reinforcement learning (RL), can produce ex-**
**cel control policies over challenging agile robot tasks, such**
**as sports robot. However, no existing work has harmonized**
**learning-based policy with model-based methods to reduce**
**training complexity and ensure the safety and stability for**
**agile badminton robot control. In this paper, we introduce**
**Hamlet, a novel hybrid control system for agile badminton**
**robots. Specifically, we propose a model-based strategy for**
**chassis locomotion which provides a base for arm policy. We**
**introduce a physics-informed “IL+RL” training framework for**
**learning-based arm policy. In this train framework, a model-**
**based strategy with privileged information is used to guide arm**
**policy training during both IL and RL phases. In addition,**
**we train the critic model during IL phase to alleviate the**
**performance drop issue when transitioning from IL to RL.**
**We present results on our self-engineered badminton robot,**
**achieving 94.5% success rate against the serving machine and**
**90.7% success rate against human players. Our system can**
**be easily generalized to other agile mobile manipulation tasks**
**such as agile catching and table tennis. Our project website:**
**[https://dreamstarring.github.io/HAMLET/.](https://dreamstarring.github.io/HAMLET/)**

I. I NTRODUCTION

Badminton is a competitive sport that requires high-speed
reactions. Participants are required to intercept high-velocity
ball, reaching speeds up to 426 km/h, under an approximate
one-second temporal constraint. The complexity of synchronizing visual and motor responses in such scenarios extends
to robotics. Here, the objective is the prompt unification
of perception, decision-making, and actuation to accurately
return the shuttlecock. Additionally, robotic systems must
surmount the challenge of modeling the dynamics of flexible
objects like shuttlecocks.
We develop a real robot, consisting of an omnidirectional
chassis and an arm, to execute badminton strokes within a
badminton court. There are already some badminton robots.
Kengoro [1], a humanoid robot, demonstrates racket swinging but lacks genuine badminton play. Robomintoner [2]
achieves this yet with a simplistic arm mechanism limiting
complex strokes like smash. In contrast, our intricate arm
design facilitates a more diversified and agile operational
range than Robomintor. There are also some works [3]–[11]
similar to badminton robots. However, given the requirement
of swiftly navigating in a badminton court, implementing
linear axes as used is challenging.

1 School of Computer Science and Technology, Shandong University
2 Robotics Institute, Carnegie Mellon University
3 DeepCode Robotics
4 Department of Sports, Shanghai Jiao Tong University

*†* Corresponding author: lulan871017@hotmail.com


Fig. 1. **Agile badminton robot system.** The badminton robot on the left,
which consists of an omnidirectional chassis and a 5-DOF arm, is playing
badminton against a human on the right. The robot can move flexibly in
the court and rapidly swing the racket to return the ball.

Controlling such robots broadly involves two strategies:
model-based and learning-based. Model-based strategies

[12], [13] offer both stability and safety. However, they
depend heavily on accurate environmental models. This
reliance poses distinct challenges when regarding changes
in the environment. Additionally, these strategies often require complex manual adjustments. This complexity further
compromises their environmental robustness and flexibility.
On the other hand, learning-based strategies [3]–[8], [14]
show a higher success rate for tasks, particularly within the
training data distribution. They also adapt more effectively
to the environment. Nonetheless, they face particular difficulties, especially in policy network convergence. Another
significant issue is the unexplainability of neural networks,
which poses potential safety risks. Merging model-based
and learning-based strategies to leverage their respective
strengths presents an avenue worth exploring. [3], [15]–[17]
compare or integrate these two approaches. However, they
primarily employ learning-based policies to assist physical
strategies. And to the best of our knowledge, there is
currently no such existing hybrid control system specifically
designed for badminton robots.

Reinforcement learning is often utilized to train learningbased policies. These policies can potentially surpass humanlevel performance. However, the time-consuming nature of
RL training and challenges posed by tasks with sparse
rewards are significant. Hence, an increasing number of


-----

works [18]–[27] use offline data for the warming up of
reinforcement learning methods or get control policy directly.
Yet, the confined range of offline data distribution and potential performance drops during the transition from offline to
online learning call for better solutions. For agile tasks, such
as badminton robots, collecting human demonstrative offline
data is challenging. It makes us need to try necessitating
alternative avenues for model warming up.
We propose Hamlet, a novel hybrid control system for
agile badminton robots. We harmonize model-based and
learning-based methods for whole body control and arm
policy training. For whole body control, we employ modelbased strategies for chassis movement and learning-based
strategies for arm control. This decoupling simplifies arm
control training and facilitates zero-shot policy transfer from
simulation to reality. In additional, arm policy does not need
retraining when switching chassis, aiding future hardware
adaptation. For arm policy training, we introduce a physicsinformed “IL+RL” training recipe. We develop a modelbased strategy with privileged information to guide arm
policy training during both IL and RL phases. To mitigate
performance drop issue during the transition from IL to
RL, we additionally trained the critic during the IL phase.
Our training scheme eliminates the need for complex reward
shaping or curriculum learning, allowing training with sparse
rewards. And soft boundaries built by model-based policy
supervision make policy network exploration in RL more
efficient and safe. Our main contributions are:

*•* We develop Hamlet, the first whole body control system
for agile badminton robots that integrates model-based
and learning-based control strategies.

*•* We propose the physics-informed training pipeline, including a model-based strategy with privileged information to warm up the actor and critic during imitation
learning phase, and supervise further policy exploration
during reinforcement learning phase.

*•* We validate our framework in real-world environments

and demonstrate that Hamlet achieves zero-shot generalization to multiple chassis without arm policy re-training.

*•* Overall, Hamlet achieves 94.5% success rate against a
badminton serving machine, while 90.7% success rate
against human players, with maximum rally length of 40.

Our proposed system can be well generalized to other
mobile manipulation tasks, especially those related to agile
robot control, such as high speed object catching.

II. P RELIMINARY

*A. Badminton Robot Architecture*

The badminton robot consists of a robotic arm and a

high-speed chassis (see Fig. 1). The robotic arm holding
a badminton racket at its end has 5 rotating joints whose
angular position and velocity can be controlled. The highspeed chassis is able to move in all directions with a
maximum speed of 5 *m/s* . Visual perception relies on high
frame rate binocular cameras behind the robot.


*B. Model-based Control Strategy for Badminton Robots*

The model-based strategy used to control the robot in this
work is suggested by the work [13]. We denote the robot
configuration vector by *q ∈* R *[7]*, where *q* *1* : *2* *∈* R *[2]* represents
the chassis’s planar position in court, and *q* *3* : *7* *∈* R *[5]* represents the joint angles of the robotic arm. Inverse kinematics
function *IK* ( *p* *r* *, R* *r* *, v* *r* ) = ( *q, ˙q* ) resolves joints’ angle *q* and
velocity *˙q* from the racket’s position *p* *r* *∈* R *[3]*, orientation
*R* *r* *∈* *SO* ( *3* ) and velocity *v* *r* *∈* R *[3]* . *F* *o* maps time *t* to
detected ball’s position *p* *o* ( *t* ) and velocity *v* *o* ( *t* ). *F* *[ˆ]* *o* predicts
ball’s future position *ˆp* *o* ( *t* ; *θ* ) and velocity *ˆv* *o* ( *t* ; *θ* ) with a
set of learnable parameters *θ* . For the process of hitting
a ball, we first identify the trajectory of the shuttlecock,
generating *F* *o* and *F* *[ˆ]* *o* . Using *F* *[ˆ]* *o*, we determine the appropriate arrival position ( *p* *h* ) and hit time ( *t* *h* ), where
*p* *h* = *ˆp* *o* ( *t* *h* ; *θ* ). Subsequently, we use *ˆv* *o* ( *t* *h* ) to calculate
the racket orientation ( *R* *h* ) and velocity ( *v* *h* ) at *t* *h* . Desired
*ˆq* ( *t* *h* ) and *ˆq* *[˙]* ( *t* *h* ) are acquired through *IK* . Lastly, a low-level
controller coordinates the joints to hit the ball.
Model-based methods offer stable hitting but need an accurate dynamic model and struggle in complex environments.
In contrast, learning-based methods are more adaptable but
hard to apply to complex tasks. Can we combine both to
achieve whole-body control of the badminton robot?

III. H A M L E T : H A RMONIZING L E ARNING -B ASED

M ANIPULATION WITH M ODEL -B ASED L OCOMO T ION FOR

W HOLE -B ODY B ADMINTON R OBOT C ONTROL

*A. Whole-Body Control Strategy Architecture*

The robot controlled by our system has two parts: the
chassis and the arm. The chassis moves fast and is more

prone to dangerous behavior, making it unsafe for black box
network control. We also find that the chassis is harder to
control than the robotic arm in sim2real (see Sec. IV-A),
complicating the deployment of learning methods. For these
reasons, we use a model-based method for the chassis and a
learning-based method for the robotic arm (see Fig. 2).
*1) Overview:* We define the ball hitting process so that the
chassis provides a rough initial position *p* *base*, and the arm
performs fine actions at *p* *base* . When moving to the target
position *p* *tar*, we use the low-level controller parameters
and the current motion state of the chassis to estimate

the reachable position as *p* *base*, and sends it to the arm
policy. This means that the arm policy operates on the
premise that the robot has reached *p* *base* . The arm policy
does not directly use *p* *base* as input; rather, it integrates
the ball trajectory data. The ball trajectory *F* *o* is translated
from the world to the robot coordinate system originating
at *p* *base*, with an unchanged coordinate axis orientation.
Given the robot’s unavoidable deflection at high speeds, in
addition to translating the trajectory using *p* *base*, we rotate
it corresponding to the robot’s orientation *α* . The process
results in a rigid transformation of the ball trajectory:

*p* � *o* = *R* *o* *· p* *o* *[T]* [+] *[ T]* *[o]* *[,]* (1)


-----

Chassis
Ball Trajectory
Target Position

Fig. 2. Overview of the components of Hamlet.The green boxes represent the processing steps, including the detection and prediction of the ball
trajectory. The blue boxes represent the model-based strategy for controlling the chassis, as discussed in III-A.2. The dark orange box represents the rigid
transformation of ball trajectory coordinates, as discussed in III-A.1. The yellow box represents the learning-based policy for controlling the robotic arm,
as discussed in III-A.3.


where � *p* *o* denotes the transformed ball trajectory. *T* *o* is the
translation vector calculated by *p* *base* . *R* *o* is the rotation
matrix calculated by *α* .
*2) Chassis Control Strategy:* We first determine the remaining action execution time as *t* *avail* = *t* *h* *−* *t* *cur*, where
*t* *cur* is the current time. We then segment the selection of
*p* *tar* based on *t* *avail* . When *t* *avail* *> T* *b*, we use a fixed
hitting height *H* *b* within the ball trajectory representation
*ˆF* *o* to select *p* *h*, and compute *q* *1* : *2* as *p* *tar* using *IK* . Due
to *F* *[ˆ]* *o* ’s inaccuracy when short ball trajectory are detected,
calculating *p* *tar* with *H* *b* yields a more stable motion for the
chassis. In addition, we use a trust factor *σ* to further make
the chassis more stable in the initial stage:

*σ* = min( *[t]* *[cur]* *,* 1) *,* (2)

*T* *init*

where *T* *init* is a constant indicating the time length of the
initial stage. Thus, the target position for the controlled
movement of the chassis is:

*p* ˆ *tar* = *p* *init* + *σ* ( *p* *tar* *−* *p* *init* ) *,* (3)

where *p* *init* represents the initial position of the chassis.
As *F* *[ˆ]* *o* is updated, *p* *tar* is recalculated continuously. When
*t* *avail* *<* = *T* *b*, there’s insufficient time for the chassis to
adjust to significant changes in target position, so *p* *tar* is no
longer recalculated to maintain the stability of the platform’s
motion.

*3) Arm Control Policy:* The badminton task can be
formulated as a Markov Decision Process (MDP), which
consists of a state space *S*, action space *A*, reward function
*R* : *S × A →* R, and dynamics function *P* : *S × A →S* .
We parameterize a policy *π* : *S →A* as a neural network
with parameters *θ ∈* Θ, denoted as *π* *θ* .
The policy network we use for the arm is a four-layer
MLP. The MLP receives a tensor of shape ( *N, S* + 3) as
input, where *N* is the number of past observations, and *S*


**Al** **g** **orithm 1** Ph y sics-informed “IL+RL” trainin g reci p e
**Input:** ˆ *π* *P M* *, λ*, M, Q

1: Initialize policy *π* *θ*, critic *V* *ϕ*
2: *//* Phase 1: Imitation Learning
3: **for** i= 1 **to** M **do**

4: Collect ( *s* *t* *, a* *t* *, r* *t* *, s* *t* + *1* ) *∼* *π* *θ* .
5: Update *θ* by a gradient method w.r.t *J* *[SUP]* ( *θ* ). (4)
6: Update *ϕ* by a gradient method w.r.t *L* *BL* ( *ϕ* ). (5)
7: **end for**
8: *//* Phase 2: Reinforcement Learning
9: **for** i= 1 **to** Q **do**

10: Collect ( *s* *t* *, a* *t* *, r* *t* *, s* *t* + *1* ) *∼* *π* *θ* .
11: Estimate advantages *A* [ˆ] *t* . (6)
12: Update *θ* by a gradient method w.r.t *J* *[RL]* ( *θ* ). (8)
13: Update *ϕ* by a gradient method w.r.t *L* *BL* ( *ϕ* ). (5)
14: **end for**

**Output:** *π* *θ*

corresponds to the joint angle measurements of the *S DOF*
arm. The “+3” corresponds to the 3D position of the ball
*p* � *o* as mentioned in III-A.1. In our experiments, *S* = 5 and
*N* = 8, so the input has shape (8, 8). The output of the
MLP is a tensor with length *S*, which represents the velocity
control command for each joint of the arm.

*B. Physics-Informed “IL+RL” Policy Training Method*

We construct a physics-informed training recipe consisting
of “IL+RL” to train *π* *θ* . Specifically, a model-based control
strategy ˆ *π* *P M* access to privileged information is developed
based on [13] in simulation at first. ˆ *π* *P M* serves as a
teaching guide for *π* *θ* and a critic model *V* *ϕ* training via
IL. Subsequently, we advance the training of *π* *θ* via RL
supervised by ˆ *π* *P M* . An overview of the training recipe is
give in Algorithm 1 and details are discussed below.


-----

*1) Preparations:* To enhance the training of *π* *θ*, we have
made the following preparations:

*•* **Teacher access to privileged Information :** In simulation, we gain access to privileged information that is
difficult to obtain in the real world and the completed
badminton trajectory is one of them. We develop a modelbased strategy ˆ *π* *P M*, has direct access to the complete
badminton trajectory, to minimize control errors caused by
ball trajectory prediction. We leverage ˆ *π* *P M* to supervise
policy network training in both IL and RL phase.

*•* **Augmented Real-world Ball Trajectories :** For training
data, we map real-world badminton trajectories into the
simulation to narrow the gap between simulation and reality. To address the sparsity in these trajectories, we apply
translation and flipping to them. This data augmentation
technique not only expands the training dataset but also
ensures an even distribution of ball landing points across
the court’s left and right halves.

*•* **Chassis Control Strategy with Noise :** Throughout
training, we consistently employ the same control strategy
for the chassis as used in the real robot. To minimize

the gap between simulation and reality and achieve zeroshot transfer from simulation to reality in the policy
network, we inject noise into the control of the simulated
environment chassis.

*2) IL Phase:* In order to speed up the training of the
policy, we use IL for a warm-up of *π* *θ* . To avoid covariance
drift problems, we use Dataset Aggregation (DAgger) [28] as
the algorithm of the IL phase. In this phase, our optimization
objective is to maximize :

*J* *[SUP]* ( *θ* ) = E *s* *t* *∼π* *θ* [ln *π* *θ* ( *πˆ* *PM* ( *s* *t* ) *|s* *t* )] *.* (4)

In order to meet the needs of a actor model and a critic

model for later RL training, we make some modifications
to DAgger. While we train *π* *θ*, we also use sampled data to
train a critic model *V* *ϕ* by minimize :


*′*
*−t* 2

� *γ* *[t]* *r* *t* *′* *−* *V* *ϕ* ) *,* (5)

*t* *[′]* *>t*


*π* *θ* ( *a* *t* *|* *s* *t* )
where *p* *t* ( *θ* ) = *π* *θold* ( *a* *t* *|s* *t* ) [. In order to avoid excessive bias]
in the exploration of *π* *θ*, supervision from ˆ *π* *P M* denoted as
*J* *[SUP]* ( *θ* ) is integrated into the PPO optimization objective
with a weight of *λ* . Therefore, the optimization objective in
our RL phase is to maximize :

*J* *[RL]* ( *θ* ) = *J* *[PPO]* ( *θ* ) + *λJ* *[SUP]* ( *θ* ) *,* (8)

where *λ* is a The reward function we used during the RL
training is designed as follows:

*•* **Hitting and Success Reward:** A sparse reward will be
given when the racket touches / returns the ball;

*•* **Penalties** **for** **Exceeding** **Dynamic** **Constraints:** A
penalty is given when the robot’s angle / velocity /
acceleration / jerk surpass predefined limits in an episode.

*•* **Penalties for Unsafe Actions:** A penalty is given when
the base joint angle of the robotic arm is too backward
/ the robot self collides / the robot collides with other

objects / the racket is below a certain height.

*•* **Ball Position Reward:** A dense reward based on the

ball’s closest distance to the paddle’s center, with smaller
distances yielding higher rewards.

*•* **Net Crossing Reward:** A sparse reward is given when
the robot hits the ball over the net.

*•* **Ball Height above Net Reward:** A dense reward based
on the ball’s height when crossing the net.

*•* **Landing Position Reward:** A dense reward calculated
by the distance from the center of the opponent’s court to
where the ball lands after being hit back.
**Remark** : The superiority of our proposed learning-based
policy over the model-based strategy for robotic arm control
can be attributed to the following reasons:

i) **Higher Upper Bound in IL Phase** : The model-based
strategy ˆ *π* *P M* access to privileged information provides
a higher performance upper bound than the real modelbased strategy *π* *P M* during the IL phase.
ii) **Greater Potential in RL Phase** : The explorative training
in the learning-based policy during the RL phase provides
an opportunity to learn actions beyond the capabilities of
model-based strategies.

IV. E XPERIMENTS

We evaluate our system in both simulation and real-world.
The simulation environment is built using PyBullet [30]. In
the real world, the hardware of the robot we used is shown in
Fig. 3. It has two parts: a binocular vision module and a robot
body. We gather 10,000 raw badminton trajectories in the
real-world. And we use 9,000 trajectories enhanced through
data augmentation to train the learning-based policy network
in simulation, while the remaining 1,000 served as the test
dataset. Experiments are conducted in the real world with our
self-engineered badminton robot against both a badminton
serving machine and human.
The metrics we use to evaluate our system is as follows:

*•* **Hit Rate** : the rate of the robot racket touching the ball.

*•* **Success Rate** : the rate of the robot successfully hitting
the ball to the designated area on the opponent’s court.


*L* *BL* ( *ϕ* ) = *−*


*T*
�


�(�

*t* =1 *t* *[′]* *>t*


where *γ* is a discount factor. During the policy training,
not only the data of correct actions will be generated, but
also some data of wrong actions will be obtained. And the
data with balanced positive and negative samples is very
beneficial for the critic model’s training [20].
*3) RL Phase:* In order to further improve the performance
of *π* *θ*, we use RL to allow *π* *θ* to further explore to achieve
a higher success rate. We use Proximal Policy Optimization
(PPO) [29] as the RL algorithm with several modifications.
With pre-trained critic *V* *ϕ* and the discount factor *γ*, we
estimate the advantages:

ˆ *′* *−t*
*A* *t* = � *γ* *[t]* *r* *t* *′* *−* *V* *ϕ* *.* (6)

*t* *[′]* *>t*

The optimization objective of PPO is to maximize :

*J* *[PPO]* ( *θ* ) =

(7)
E *t* [min ( *p* *t* ( *θ* ) *A* *[ˆ]* *t* *,clip* ( *p* *t* ( *θ* ) *,1 −* *ϵ, 1* + *ϵ* ) *A* *[ˆ]* *t* )] *,*


-----

(a)


(b)


Fig. 3. **Hardware of badminton robot system.** Our badminton robot
system consists of two parts: (a) vision module and (b) robot body.The
visual module handles scene perception, including ball recognition, tracking,
and robot positioning. The computer on the robot body executes the control
algorithm and send control commands to each motor.













Fig. 4. **Serving machine experiments setup.** We set 6 positions for the
serving machine on the right side of the court. We configure 20 combinations
of machine positions, power, and angle to ensure diverse trajectories. The
circles on the left side of the court show the ball landing points. The subplots
on the left and right show different views of the real court.

*•* **Max Rally** : the maximum number of consecutive rally
rounds between the robot and the human.

*A. Gap Analysis from Simulation to Reality*

We test the difference between simulation and reality for
the robot’s chassis and manipulator separately, using the
same 194 control trajectories in both environments. The
difference for the chassis is measured by its final position,
while for the manipulator, it is the final position of the racket.
Results in Fig. 5 show the manipulator’s gap is smaller,
with the chassis error showing more variation. This may be
because the chassis must contact the ground, involving more
complex physical processes and varying contact conditions.
In contrast, controlling the robot arm is relatively simpler.
This suggests that chassis control is less stable and more
affected by real-world conditions, which is why we use
a learning method for the manipulator and a model-based
method for the chassis.

*B. Model-based Strategy vs. Learning-based Policy*

We compare the performance of the model-based strategy
and our learning-based policy for the arm control utilizing
the test dataset in simulation. The model-based strategy
achieve a hit rate of 91% and a success rate of 77.5%, while
our learning-based policy attained a hit rate of 97% and
a success rate of 94.5%. It reveals that the learning-based
policy outperforms the model-based strategy in both hit and
success rates, with a notably higher success rate.


(a) (b)
Fig. 5. **Comparison of the control gap between simulation and real**
**world of chassis and arm.** We test the difference between the mobile
platform and the robot’s end position in the simulation and real world under
the same control commands. Box plot (a) visualizes the end distance error.
We show the standard deviation of the end distance error for the chassis
and arm in chart (b). The results show that the chassis is more volatile to
changes in the environment and is less conducive to sim2real.

Fig. 6. **Ablation Study.** We sequentially strip components from our
proposed training recipe and observe the ensuing reward fluctuations during
training. **IL** : The IL phase of our training recipe detailed in III-B.2. **Ours** :
The RL phase of our training recipe detailed in III-B.3. **PPO only** : Employ
solely PPO to train the policy network and learn from scratch. **Ours w/o**
**critic** : The omission of critic warm-up. **Ours w/o PM** : The absence of
model-based strategy supervision during RL. **Ours w/o critic&PM** : Neither
critic warm-up nor model-based strategy supervision in the RL phase.

*C. Ablation Study*

We conduct ablation study on our proposed arm policy
training recipe. As shown in Fig. 6, the inclusion of the IL
phase can rapidly enhance the performance of the model
compared to initiating RL from scratch. Our training recipe
demonstrates further optimization of the policy network’s
performance during the RL phase. Without the supervision of
a pre-trained critic or model-based strategy, policy networks
experience a significant initial drop in reward during reinforcement learning, undoubtedly extending the exploration
phase. Notably, the lack of supervision from a model-based
strategy exacerbates this drop in reward and leads to greater
fluctuations. Notably, we find that the absence of modelbased policy supervision exacerbates reward degradation and
leads to greater volatility when the policy network is trained.

*D. Robot vs. Serving Machine*

We configure the serving machine using combinations of
three positions, three angles, and two strengths, creating 20
scenarios (see Fig. 4).
For each scenario, we collect 10 test outcomes for each
of the model-based and learning-based strategies, resulting


-----

TABLE I

C OMPARISON R ESULTS B ETWEEN M ODEL -B ASED P OLICY AND O URS

IN R EAL W ORLD

Hit Rate Success Rate

Model-Based 91.0 77.5

Ours **97.0** **94.5**

Fig. 7. **Robot vs. Human Performance.** We collect three sets of data
from robot-human matches. In each set, human and robot play 100 rounds,
and we record the robot’s return success rate and the maximum rally. We
find that our system performs well during play with humans.

in 200 outcomes per strategy.
As shown in Tab. I, the hit rate and success rate of robots
deployed with the learning-based policy is significantly better
than the model-based strategy.

*E. Human Robot Interaction*

To verify the performance of the robot and human in
continuous combat, we collected three sets of human-robot

combat data. Each set of data contains 100 rounds of battles

between humans and robots. We recorded the robot’s return

success rate and the longest round in the game with humans,
as shown in Fig. 7. We found that our method still showed
good return results in the process of playing against humans,
with an average return success rate of 90.7%, and could play
up to 40 rounds with human players.

V. R ELATED W ORK

*A. Sports Robots*

As far as we know, there have been some studies on
ball sports robots. Xiao et al. [31] and Voeikov et al. [32]
proposed an information perception method for ball sports
scenes. Google team proposed a table tennis robot [4], [5],

[7], [8], [33]. They used model-free reinforcement learning
algorithms to complete the learning of the robot’s control
policy network. Ji et al. [34] used a self-designed six-degreeof-freedom robot system to realize the table tennis return task
through a control method based on a physical model. Xiong
et al. [35] implemented two humanoid robots to complete
the table tennis pulling task. Zaidi et al. [36] proposed the
first open source autonomous wheelchair robot for regular
tennis matches, establishing the first baseline for replicable
wheelchair robots used in regular singles matches. In our
system, The robot that is controlled has a more flexible joint
setup. And to the best of our knowledge, our system is the
first to integrate model-based and learning-based control of
an agile badminton robot.


*B. Integration of RL and IL*

There are some works that combine IL and RL to speed
up training while reaching ultra-high levels beyond humans.
Ramrakhya et al. [18] proposed a two-stage learning scheme
of “IL+RL” on navigation tasks. Nair et al. [19] provides
an integrated framework combining IL and RL that can pretrain policies using large amounts of offline data and then
quickly fine-tune them through online interactions to learn
complex behaviors. Lu et al. [20] proposed a training plan
that balances positive and negative samples in critic model
training and only uses successful data to update Actors,
solving the problem of sudden performance drop in the initial
stage of migration from imitation learning to reinforcement
learning. Rajeswaran et al. [22] used real-person operation
data obtained from VR devices to enhance reinforcement

learning policies to learn complex dexterous hand operation
tasks. In our “IL+RL” training recipe, a model-based strategy accessing privileged information guides and sets soft
boundaries for policy network training, rather than producing
offline data. We also warm up the critic model during IL
phase, alleviating performance drop-off from IL to RL.

*C. Combined Model-Based and Learning-Based Control*

In recent years, more and more works analyze and combine the advantages of both and design integrated control
algorithms. Abeyruwan et al. [3] compares the advantages
and disadvantages of physical model-based policies and
learning-based policies on the task of catching a high-speed
flying ball. Dong et al. [15] proposes a hierarchical control
algorithm to complete the capturing task. They use SQP and
QP for motion trajectory planning and then use a learningbased controller to track the trajectory. Pandala et al. [16]
proposed a nonlinear controller based on virtual constraints
and low-level quadratic programming, and used RL to train
a neural network that computes the RMPC uncertainty set
for a robust walking task on rough roads for Legged robots.
In our system, model-based and learning-based strategies
are separately applied to varied components of the robotic
control. For the arm policy, the model-based strategy plays a
guiding role, while the neural network provides accelerated
running speed and facilitates easier deployment.

VI. C ONCLUSION

We present Hamlet, a new whole-body control system for an agile badminton robot, combining model-based
and learning-based control methods. Our system leverages
model-based control for the chassis and learning-based control for the robot arm. The arm policy training employs an
“IL + RL” strategy, where model-based techniques pre-train
the actor and critic in IL, enhancing subsequent RL policy
training. Our policy enables zero-shot transfer from simulation to reality and can seamlessly integrate into different
chassis without requiring retraining. Extensive experiments
demonstrate a 94.5% success rate against a serving machine,
90.7% against a human opponent, and up to 40 consecutive
hits. Our system is robust, adaptable, and applicable to other
agile robot control tasks.


-----

R EFERENCES

[1] K. Kawaharazuka *et al.*, “Human mimetic forearm design with radioulnar joint using miniature bone-muscle modules and its applications,”
in *Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.*, Sep. 2017, pp. 4956–
4962.

[2] D. GROSSMAN. (2024) Finally, A Robot That Can Beat You
[at Badminton. https://www.popularmechanics.com/technology/robots/](https://www.popularmechanics.com/technology/robots/a21422/badminton-playing-robot/)
[a21422/badminton-playing-robot/.](https://www.popularmechanics.com/technology/robots/a21422/badminton-playing-robot/)

[3] S. Abeyruwan *et al.*, “Agile Catching with Whole-Body MPC and
Blackbox Policy Learning,” in *Proc. Learn. Dyn. Control Conf.*, ser.
Proceedings of Machine Learning Research, N. Matni, M. Morari, and
G. J. Pappas, Eds., vol. 211, 2023, pp. 851–863.

[4] S. W. Abeyruwan *et al.*, “i-Sim2Real: Reinforcement Learning of
Robotic Policies in Tight Human-Robot Interaction Loops,” in *Proc.*
*Conf. Robot Learn.*, ser. Proceedings of Machine Learning Research,
K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205, 2022, pp. 212–224.

[5] T. Ding *et al.*, “Learning high speed precision table tennis on a
physical robot,” in *Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.*, Oct.
2022, pp. 10 780–10 787.

[6] D. B¨uchler, S. Guist, R. Calandra, V. Berenz, B. Sch¨olkopf, and
J. Peters, “Learning to play table tennis from scratch using muscular
robots,” *IEEE Trans. Robotics*, vol. 38, no. 6, pp. 3850–3860, Dec.
2022.

[7] W. Gao *et al.*, “Robotic table tennis with model-free reinforcement
learning,” in *Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.*, 2020, pp.
5556–5563.

[8] D. B. D’Ambrosio, S. Abeyruwan, L. Graesser, A. Iscen, H. B. Amor,
A. Bewley, B. J. Reed, K. Reymann, L. Takayama, Y. Tassa, *et al.*,
“Achieving human level competitive robot table tennis,” *arXiv preprint*
*arXiv:2408.03906*, 2024.

[9] Y. Ji, Y. Mao, F. Suo, X. Hu, Y. Hou, and Y. Yuan, “Opponent hitting
behavior prediction and ball location control for a table tennis robot,”
*Biomimetics*, vol. 8, no. 2, p. 229, 2023.

[10] K. Zhang, Z. Cao, J. Liu, Z. Fang, and M. Tan, “Real-time visual
measurement with opponent hitting behavior for table tennis robot,”
*IEEE Transactions on Instrumentation and Measurement*, vol. 67,
no. 4, pp. 811–820, 2018.

[11] Y. Wang, Y. Luo, H. Zhang, W. Zhang, K. Dong, Q. He, Q. Zhang,
E. Cheng, Z. Sun, and B. Song, “A table-tennis robot control strategy
for returning high-speed spinning ball,” *IEEE/ASME Transactions on*
*Mechatronics*, 2023.

[12] N. Mizuno *et al.*, “Development of automatic badminton playing robot
with distance image sensor,” *IFAC-PapersOnLine*, vol. 52, no. 8, pp.
67–72, 2019.

[13] F. Yang, Z. Shi, S. Ye, J. Qian, W. Wang, and D. Xuan, “Varsm: Versatile autonomous racquet sports machine,” in *Proc. 13th ACM/IEEE*
*Int. Conf. Cyber-Phys. Syst.*, 2022, pp. 203–214.

[14] Z. Fu, X. Cheng, and D. Pathak, “Deep whole-body control: Learning
a unified policy for manipulation and locomotion,” in *Conference on*
*Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New*
*Zealand*, ser. Proceedings of Machine Learning Research, K. Liu,
D. Kulic, and J. Ichnowski, Eds., vol. 205. PMLR, 2022, pp. 138–149.

[[Online]. Available: https://proceedings.mlr.press/v205/fu23a.html](https://proceedings.mlr.press/v205/fu23a.html)

[15] K. Dong, K. Pereida, F. Shkurti, and A. P. Schoellig, “Catch the Ball:
Accurate High-Speed Motions for Mobile Manipulators via Inverse
Dynamics Learning,” in *Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst.*,
2020, pp. 6718–6725.

[16] A. Pandala, R. T. Fawcett, U. Rosolia, A. D. Ames, and K. A. Hamed,
“Robust Predictive Control for Quadrupedal Locomotion: Learning
to Close the Gap Between Reduced- and Full-Order Models,” *IEEE*
*Robotics Autom. Lett.*, vol. 7, no. 3, pp. 6622–6629, Jul. 2022.

[17] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, “Combining
learning-based locomotion policy with model-based manipulation for
legged mobile manipulators,” *IEEE Robotics and Automation Letters*,
vol. 7, no. 2, pp. 2377–2384, 2022.

[18] R. Ramrakhya, D. Batra, E. Wijmans, and A. Das, “PIRLNav:
Pretraining with imitation and RL finetuning for OBJECTNAV,” in
*Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops*, Jun.
2023, pp. 17 896–17 906.

[19] A. Nair, M. Dalal, A. Gupta, and S. Levine, “Accelerating Online
Reinforcement Learning with Offline Datasets,” vol. abs/2006.09359,
[2020. [Online]. Available: https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359)

[20] Y. Lu *et al.*, “Aw-opt: Learning robotic skills with imitation andreinforcement at scale,” in *Proc. Conf. Robot Learn.*, ser. Proceedings of


Machine Learning Research, A. Faust, D. Hsu, and G. Neumann, Eds.,
vol. 164, 2021, pp. 1078–1088.

[21] D. Kalashnikov *et al.*, “Scalable deep reinforcement learning for
vision-based robotic manipulation,” in *Proc. Conf. Robot Learn.*, ser.
Proceedings of Machine Learning Research, vol. 87, Oct. 2018, pp.
651–673.

[22] A. Rajeswaran *et al.*, “Learning complex dexterous manipulation with
deep reinforcement learning and demonstrations,” in *Proc. Robot.: Sci.*
*Syst.*, H. Kress-Gazit, S. S. Srinivasa, T. Howard, and N. Atanasov,
Eds., 2018.

[23] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained
bimanual manipulation with low-cost hardware,” in *Proc. Robot.: Sci.*
*Syst.*, K. E. Bekris, K. Hauser, S. L. Herbert, and J. Yu, Eds., 2023.

[24] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile ALOHA: learning bimanual
mobile manipulation with low-cost whole-body teleoperation,” vol.
[abs/2401.02117, 2024. [Online]. Available: https://doi.org/10.48550/](https://doi.org/10.48550/arXiv.2401.02117)
[arXiv.2401.02117](https://doi.org/10.48550/arXiv.2401.02117)

[25] Z. Yu and X. Zhang, “Actor-critic alignment for offline-to-online
reinforcement learning,” in *International Conference on Machine*
*Learning* . PMLR, 2023, pp. 40 452–40 474.

[26] B. Wexler, E. Sarafian, and S. Kraus, “Analyzing and overcoming
degradation in warm-start reinforcement learning,” in *2022 IEEE/RSJ*
*International Conference on Intelligent Robots and Systems (IROS)* .
IEEE, 2022, pp. 4048–4055.

[27] Y. Zhang, L. Ke, A. Deshpande, A. Gupta, and S. S. Srinivasa,
“Cherry-picking with reinforcement learning,” in *Robotics: Science*
*and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023*, K. E.
Bekris, K. Hauser, S. L. Herbert, and J. Yu, Eds., 2023. [Online].
[Available: https://doi.org/10.15607/RSS.2023.XIX.021](https://doi.org/10.15607/RSS.2023.XIX.021)

[28] S. Ross, G. J. Gordon, and D. Bagnell, “A Reduction of Imitation
Learning and Structured Prediction to No-Regret Online Learning,” in
*Proc. 14th Int. Conf. Artif. Intell. Statist. JMLR Workshop Conf. Proc.*,
ser. JMLR Proceedings, G. J. Gordon, D. B. Dunson, and M. Dud´ık,
Eds., vol. 15, 2011, pp. 627–635.

[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal Policy Optimization Algorithms,” vol. abs/1707.06347,
[2017. [Online]. Available: http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)

[30] E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” 2016.

[31] Q. Xiao, Z. Zaidi, and M. C. Gombolay, “Multi-camera asynchronous
ball localization and trajectory prediction with factor graphs and
human poses,” vol. abs/2401.17185, 2024. [Online]. Available:
[https://doi.org/10.48550/arXiv.2401.17185](https://doi.org/10.48550/arXiv.2401.17185)

[32] R. Voeikov, N. Falaleev, and R. Baikulov, “TTNet: Real-time temporal
and spatial video analysis of table tennis,” in *Proc. IEEE/CVF Conf.*
*Comput. Vis. Pattern Recognit. Workshops*, Jun. 2020, pp. 3866–3874.

[33] D. B. D’Ambrosio *et al.*, “Robotic table tennis: A case study into a
high speed learning system,” 2023.

[34] Y. Ji *et al.*, “Model-based trajectory prediction and hitting velocity
control for a new table tennis robot,” in *Proc. IEEE/RSJ Int. Conf.*
*Intell. Robots Syst.*, 2021, pp. 2728–2734.

[35] R. Xiong, Y. Sun, Q. Zhu, J. Wu, and J. Chu, “Impedance control
and its effects on a humanoid robot playing table tennis,” *Int. J. Adv.*
*Robot. Syst.*, vol. 9, no. 5, p. 178, Nov. 2012.

[36] Z. Zaidi *et al.*, “Athletic Mobile Manipulator System for Robotic
Wheelchair Tennis,” *IEEE Robotics Autom. Lett.*, vol. 8, no. 4, pp.
2245–2252, Apr. 2023.


-----

