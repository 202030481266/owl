## **TimelyGPT: Extrapolatable Transformer Pre-training for** **Long-term Time-Series Forecasting in Healthcare**

#### Ziyang Song, Qincheng Lu, Hao Xu, He Zhu
###### School of Computer Science, McGill University Montreal, QC, Canada
##### **ABSTRACT**

#### David Buckeridge
###### School of Population and Global Health, McGill University Montreal, Quebec, Canada

#### Yue Li
###### School of Computer Science, McGill University Mila Quebec AI institute Montreal, QC, Canada


**Motivation:** Large-scale pre-trained models (PTMs) such as BERT
and GPT have recently achieved great success in Natural Language
Processing and Computer Vision domains. However, the development of PTMs on healthcare time-series data is lagging behind.
This underscores the limitations of the existing transformer-based
architectures, particularly their scalability to handle large-scale
time series and ability to capture long-term temporal dependencies.
**Methods:** In this study, we present Timely Generative Pre-trained
Transformer (TimelyGPT). TimelyGPT employs an extrapolatable
position (xPos) embedding to encode trend and periodic patterns
into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture
global-local temporal dependencies.
**Materials:** We evaluated TimelyGPT on two large-scale healthcare time series datasets corresponding to continuous biosignals
and irregularly-sampled time series, respectively: (1) the Sleep EDF
dataset consisting of over 1.2 billion timesteps collected from 197
whole-night polysomnographic sleep recordings, containing EEG,
EOG, EMG, and event marker; (2) the longitudinal healthcare administrative database PopHR, comprising 489,000 patients randomly
sampled from the Montreal population.
**Results:** Our experiments show that during pre-training, TimelyGPT excels in learning time-series representations from continuously monitored biosignals and irregularly-sampled time series
data commonly observed in longitudinal electronic health records
(EHRs), which can aid in healthcare time-series forecasting tasks.
In forecasting continuous biosignals, TimelyGPT achieves accurate
extrapolation up to 6,000 timesteps of body temperature during the
sleep stage transition, given a short look-up window (i.e., prompt)
containing only 2,000 timesteps. For irregularly-sampled time series,
TimelyGPT with a proposed time-specific inference demonstrates
high top recall scores in predicting future diagnoses using early
diagnostic records, effectively handling irregular intervals between
clinical records. Together, we envision TimelyGPT to be useful in
a broad spectrum of health domains, including long-term patient
health state forecasting and patient risk trajectory prediction.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
*ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China*
¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
[https://doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)

##### **CCS CONCEPTS**

- **Applied computing** ‚Üí **Bioinformatics** ; ‚Ä¢ **Computing method-**
**ologies** ‚Üí **Transfer learning** .
##### **KEYWORDS**

Time-series forecasting, Time-series pre-training, transfer learning,
irregularly-sampled time series, biosignals, clinical diagnosis

**ACM Reference Format:**

Ziyang Song, Qincheng Lu, Hao Xu, He Zhu, David Buckeridge, and Yue
Li. 2024. TimelyGPT: Extrapolatable Transformer Pre-training for Longterm Time-Series Forecasting in Healthcare. In *Proceedings of The 15th ACM*
*Conference on Bioinformatics, Computational Biology, and Health Informat-*
*ics (ACM BCB ‚Äô24).* [ACM, New York, NY, USA, 17 pages. https://doi.org/](https://doi.org/XXXXXXX.XXXXXXX)

[XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)
##### **1 INTRODUCTION**

Time-series forecasting holds significant importance in healthcare,
given its potential to trace patient health trajectories and predict
medical diagnoses [ 7, 20 ]. In the field of healthcare, there are two primary categories: continuously monitored and irregularly-sampled
time series data. Continuous time-series, such as biosignals, have
been extensively studied in various applications, including health
monitoring [ 38 ], disease classification [ 25 ], and physical activity prediction [ 31 ]. Irregularly-sampled time series are commonly
found in clinical records, where spontaneous updates are made due
to outpatient hospital visits or inpatient hospital stays [ 57 ]. The key
challenge is to extract meaningful contextualized representations
from these time-series to make accurate long-term forecasting. A
promising approach is to adopt transfer learning [ 20 ]. Initially, a
model is pre-trained on large-scale datasets to learn contextualized
temporal representations. This pre-trained model (PTM) is then
fine-tuned to forecast target sequences.
The recent impressive achievements of Transformer PTMs in
Natural Language Processing (NLP) and Computer Vision (CV)
domains have inspired growing interest in time-series Transformerbased PTMs. Time-Series Transformer (TST) uses a mask-andreconstruction pre-training strategy to extract contextualized representations from time series [ 54 ]. Cross-Reconstruction Transformer (CRT) learns temporal representations by dropping and
reconstructing certain segments from time series [ 56 ]. Additionally,
Transformer PTMs have been applied to traffic [ 58 ], tabular [ 22 ],
and speech time-series [18, 19].
Transfer learning by pre-training on large time-series data followed by fine-tuning for long-term time series forecasting (LTSF)
tasks is a promising avenue. However, existing studies primarily
focus on training from scratch on limited data for LTSF tasks [ 20 ].


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.
###### **a. TimelyGPT Architecture**






















**Figure 1: TimelyGPT overview. a. TimelyGPT architecture. TimelyGPT consists of a convolution-subsampling tokenizer**
**followed by** *ùêø* **decoder layers, with detailed overflow provided in Appendix B.3. b. Generative decoder with xPos embedding.**
**Each decoder layer is coupled with extrapolatable position embedding (Section 3.1) that encodes trend and periodic patterns**
**into representations, facilitating forecasting with extrapolation ability. c. Chunk-wise Retention. This module consists of**
**parallel intra-chunk Retention and recurrent inter-chunk Retention, effectively handling long sequences in continuously**
**monitored biosignals (Appendix B.2). d. Temporal Convolution (Section 3.3) captures nuanced local interactions from time-**
**series representations.**


These studies often introduce tailored architectures and attention

modules to extract complex temporal dependencies [ 48, 59, 60 ].
However, the scalability of these transformers on large datasets for
LTSF tasks remains an open question [ 13 ]. A recent study argues
that the permutation-invariant nature of self-attention causes the
loss of temporal information [ 53 ]. As a result, transformers often
underperform compared to convolution-based models, potentially
due to their struggles with local features and multi-scale features

[ 42, 52 ]. Overall, existing research on time-series transformers often
lacks rigorous evaluation on large datasets and does not consistently
outperform conventional approaches on small data.
In this study, we provide an in-depth analysis of existing timeseries Transformer models, covering key aspects such as the attention mechanism and position embedding. We argue that the
seeming inadequacy of current transformer-based models for timeseries data is due to their inability to model large-scale time series.


Once these challenges are resolved, we would observe the typical
scaling law found in NLP and CV domains [ 13, 55 ]. Motivated by
this insight, we present a novel framework called **Timely Genera-**
**tive Pre-trained Transformer (TimelyGPT)** (Fig. 1) that utilizes
an extrapolatable position (xPos) embedding to encode trend and
periodic patterns into time-series representations [ 41 ]. TimelyGPT
integrates recurrent attention (also known as Retention) and convolution modules for effectively capturing both global temporal
dependencies and nuanced local interactions [10, 40].
The key contributions of our research are threefold:

(1) We employ extrapolatable xPos embedding (Fig. 1b) to encode both trend and periodic patterns into time-series representations, facilitating long-term forecasting.
(2) We extend recurrent attention (Fig. 1c) to handle both continuous and irregularly-sampled time series data;


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China


(3) We introduce convolution subsampling tokenizer (Fig. 1a) to
extract features from raw time-series and temporal convolution (Fig. 1d) to sift local features among the timesteps.

Overall, our experimental results reveal that TimelyGPT effectively
extrapolates temporal representations for long-term forecasting.
This leads to highly effective pre-training on large-scale time-series
biosignals and longitudinal EHR data, and ultimately superior taskspecific fine-tuning performance compared to the existing methods.
##### **2 RELATED WORK** **2.1 Self-attention in Transformer**

Transformer employs an encoder-decoder architecture composed
of *ùêø* layers of Transformer blocks [ 45 ]. Each block consists of a
self-attention layer followed by a feed-forward layer. For an input
embedding *ùëø* ‚àà R *[ùëÅ]* [√ó] *[ùëë]*, where *ùëÅ* is the number of tokens and *ùëë* is
the hidden size, the self-attention mechanism is defined as:


*ùë∏* *ùë≤* ‚ä§
Attention( *ùëø* ) = Softmax
ÔøΩ ‚àö *ùëë*


*ùëΩ* (1)
ÔøΩ


techniques, xPos embedding utilizes both rotation and exponential
decay to effectively capture long-term dependencies [41].
One challenge for Transformer is *extrapolation*, i.e., forecasting
sequences longer than those seen during training, due to the difficulty in generalizing position embeddings to unseen positions

[ 27 ]. Encoder-decoder architectures often concatenate the input
sequence with a zero-padded placeholder for the target sequence
and predict all timesteps at once, while encoder-only models encode
input sequence for forecasting [ 21, 59 ]. Both approaches struggle
with extrapolation and rely heavily on their linear layer for forecasting [ 17 ], limiting their effectiveness in LTSF tasks. To address the
issue, Attention with Linear Biases (ALiBi) adjusts attention with
penalties linearly correlated with token distances [ 27 ]. Building on
this, xPos embedding employs exponential decay to assign penalties
based on relative distances [ 41 ]. Consequently, xPos can handle
inference lengths up to eight times the training length while maintaining comparable performance. Our TimelyGPT extends xPos
from the NLP domain to long-term forecasting in the time-series
domain, focusing on exploring the underlying mechanisms that
enable the temporal extrapolation.
##### **3 TIMELYGPT METHODOLOGY**

Our proposed TimelyGPT effectively pre-trains on unlabeled data
using next-token prediction task to learn temporal representations
(Fig. 1). It first processes time-series inputs using a convolutionsubsampling tokenizer for token embedding (Fig. 1a). To extract
meaningful temporal patterns, TimelyGPT integrates three technical contributions. First, TimelyGPT utilizes extrapolatable xPos
embedding to encode trend and periodic patterns (Fig. 1b, Section
3.1). Second, TimelyGPT utilizes the Retention module to capture
global content (Fig. 1c, Section 3.2). Third, TimelyGPT deploys the
convolution module to capture the local content (Fig. 1d, Section
3.3). Integrating Retention and Convolution modules enables the
modeling of interactions between global and local content.
##### **3.1 Extrapolatable position embedding encodes** **temporal patterns**

As our first contribution, TimelyGPT employs xPos to encode relative positional information into token embeddings based on the
distance *ùëõ* ‚àí *ùëö* between token *ùëõ* and *ùëö* [ 41 ]. Given an input embedding *ùëø* ‚àà R *[ùëÅ]* [√ó] *[ùëë]* for *ùëÅ* tokens at *ùëë* embedding dimensions, xPos
is integrated into the *ùëõ* -th token embedding *ùëø* *ùëõ* through rotation
matrix *ùëí* *[ùëñùúÉùëõ]* and exponential decay *ùõæ* *[ùëõ]* :

Àú Àú ÀÜ ÀÜ
*ùë∏* *ùëõ* *ùë≤* *ùëö* = *ùëø* *ùëõ* *ùëæ* *ùëÑ* ( *ùõæùëí* *[ùëñùúÉ]* ) *[ùëõ]* [‚àí] *[ùëö]* *ùëø* *ùëö* *ùëæ* *ùêæ* = *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* *ùë∏* *ùëõ* *ùë≤* *ùëö*

ÀÜ
where *ùë∏* *ùëõ* = *ùëø* *ùëõ* *ùëæ* *ùëÑ* *ùëí* *[ùëñùúÉùëõ]* *,* ÀÜ *ùë≤* *ùëö* = *ùëø* *ùëö* *ùëæ* *ùêæ* *ùëí* [‚àí] *[ùëñùúÉùëö]* (2)

where *ùúÉ* and *ùõæ* indicate position-dependent rotation and decay hyperparameters [ 39, 41 ]. The exponential decay *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* determines the
intensity of remembering historical information, while the rotation
matrix *ùëí* *[ùëñùúÉùëõ]* captures the oscillation frequencies. This decay mechanism effectively attenuates the influence of distant tokens, aiding
in capturing long-term dependencies and enhancing extrapolation
ability [41].
While initially designed for language modeling, xPos provides a
compelling way for time-series modeling, mirroring the seasonaltrend decomposition (Fig. 1c). Its exponential decay *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* naturally


‚àö


*ùëë*


where *ùë∏, ùë≤, ùëΩ* = *ùëøùëæ* *ùëÑ* *, ùëøùëæ* *ùêæ* *, ùëøùëæ* *ùëâ* ‚àà R *[ùëÅ]* [√ó] *[ùëë]* are the Query, Key,
and Value matrices, respectively. The attention mechanism allows
Transformer to model long-term dependencies effectively, making
it extensively utilized in NLP and CV domains.
As one of the prominent time-series transformers, Conformer
utilizes the self-attention mechanism to capture long-range global
contexts in speech data [ 10 ]. When combined with convolution
modules, Conformer enhances self-attention by exploiting finegrained local patterns. Although widely successful, the quadratic
complexity of self-attention with respect to sequence length has
spurred the exploration of attention-free modules such as MultiLayer Perceptron (MLP) [ 43 ], implicit long convolution [ 26 ], and
Recurrent Neural Network (RNN) [ 24, 40 ]. In particular, RNN-based
attention modules have scaled up to 14 billion parameters while
maintaining competitive performance with linear training and constant inference complexities. These modules are particularly wellsuited for time-series modeling by effectively capturing sequential
dependencies [ 9 ]. In this study, TimelyGPT integrates the Retention
mechanism and convolution modules to effectively capture both
global and local contexts.
##### **2.2 Position embedding in Transformer**

Transformer relies on position embedding to capture temporal
relations, since the self-attention mechanism alone does not inherently discern token order [ 34 ]. *Absolute* position embedding,
which commonly employs sinusoidal functions, adds positional
encoding directly to token embeddings. However, this method only
encodes discrete position indexes, making it less effective for continuous timescales such as trend and periodic patterns in time-series
data [ 53 ]. In contrast, speech transformers utilize *relative* position
embedding to handle continuous time by encoding positional information relative to token distances [ 10 ]. Rotary Position Embedding
(RoPE), prevalent in numerous large language models [ 2, 23, 44 ], applies rotation matrices to encode time information from relative distances [ 39 ]. Additionally, the RNN-based Transformer Receptance
Weighted Key Value (RWKV) uses exponential decay to encode
time information based on relative distance [ 24 ]. Bridging these


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


concentrates on recent times while diminishing the influence of
distant times, reflecting the trend momentum of time-series. The
rotation matrix *ùëí* *[ùëñùúÉ]* [(] *[ùëõ]* [‚àí] *[ùëö]* [)] captures the seasonal component of timeseries through sinusoidal oscillations.
In healthcare time-series, xPos embedding effectively encodes
both trend and periodic patterns crucial for modeling continuous
biosignals and irregular clinical records. For continuous biosignals,
trend patterns such as body temperature and vital signs are key
health indicators, while electrocardiograms (ECGs) exhibit periodic
patterns reflecting the physiological rhythms of the human body.
In irregularly-sampled clinical records, age-related susceptibility
to illnesses is observed in longitudinal population studies using
administrative health data [ 1, 37 ]. Some EHRs also exhibit periodic
patterns, especially for chronic diseases like COPD, which have
alternating exacerbation and recovery cycles.
We hypothesize that xPos embedding can encode these trend
and periodic patterns into token embeddings. By harnessing xPos,
TimelyGPT can effectively model long-term dependencies essential
for time-series forecasting. In Section 6.2, 6.3, and 6.4, we validated
our hypothesis and explored the underlying mechanisms driving
temporal extrapolation for forecasting beyond training length.
##### **3.2 Retention for continuous and** **irregularly-sampled time series**

As our second contribution, we adapt the Retention mechanism to
effectively handle continuous time-series data [ 40 ]. The Retention
mechanism based on xPos can be reformulated as an RNN to nat
urally model time-series data. Given the xPos embedding in Eq 2,
the forward-pass of the Retention mechanism can be computed in
parallel over all tokens with a linear training complexity:

ÀÜ
*ùë∏* *ùëõ* = *ùëø* *ùëõ* *ùëæ* *ùëÑ* *ùëí* *[ùëñùúÉùëõ]* *,* ÀÜ *ùë≤* *ùëö* = *ùëø* *ùëö* *ùëæ* *ùêæ* *ùëí* [‚àí] *[ùëñùúÉùëö]* *, ùëΩ* = *ùëøùëæ* *ùëâ*


**Figure 2: Two inference strategies for forecasting irregularly-**
**sampled time series. (a) Trajectory-based inference. Time-**
**lyGPT autoregressively predicts the entire sequence at equal**
**time intervals. The target intervals can then be taken from**
**part of the inferred trajectory. (b) Time-specific inference.**
**TimelyGPT directly predicts the target data point using his-**
**torical hidden states and the gap between the target timestep**
**and the last observed timestep.**

For next token pre-training, the retention incorporates Œî *ùë°* *ùëõ,ùëõ* ‚àí1
into the recurrent state variable *ùë∫* *ùëõ* ‚àà *ùëÖ* *[ùëë]* [√ó] *[ùëë]*, :

*ùë∫* *ùëõ* = *ùõæ* [Œî] *[ùë°]* *[ùëõ,ùëõ]* [‚àí][1] *ùë∫* *ùëõ* ‚àí1 + *ùë≤* *ùëõ* [‚ä§] *[ùëΩ]* *[ùëõ]*

Ret( *ùëã* *ùëõ* ) = *ùë∏* *ùëõ* *ùë∫* *ùëõ* (5)

where the base case *ùë∫* 1 = 0 in this recurrent relation.
At inference time, to forecast irregularly-sampled time series,
we consider two recurrent inference strategies, namely trajectorybased inference and time-specific inference (Fig. 2). Both strategies
make predictions based on a look-up window. The former autoregressively predicts a trajectory at equal time intervals. The latter
directly makes prediction at a specific time point *ùë†* *ùëõ* [‚Ä≤] = ( *ùë•* *ùëõ* [‚Ä≤] *,ùë°* *ùëõ* [‚Ä≤] ) .
Specifically, knowing the target timestep *ùë°* *ùëõ* [‚Ä≤] and the last observed
sample *ùë†* *ùëõ* = ( *ùë•* *ùëõ* *,ùë°* *ùëõ* ), TimelyGPT outputs the embedding of the target token Ret( *ùëã* *ùëõ* [‚Ä≤] ) = *ùë∏* *ùëõ* [‚Ä≤] *ùë∫* *ùëõ* [‚Ä≤], taking into account the time gap
Œî *ùë°* *ùëõ* [‚Ä≤] *,ùëõ* = *ùë°* *ùëõ* [‚Ä≤] ‚àí *ùë°* *ùëõ* and the recurrent state then becomes *ùë∫* *ùëõ* [‚Ä≤] =
*ùõæ* [Œî] *[ùë°]* *[ùëõ]* [‚Ä≤] *[,ùëõ]* *ùë∫* *ùëõ* + *ùë≤* *ùëõ* [‚ä§] *[ùëΩ]* *[ùëõ]* [.]
##### **3.3 Convolution modules for local interaction**

Convolution methods excel at identifying localized interactions
from time series [ 16 ]. As the first part of our third contribution, we
propose a **convolution-subsampling tokenizer** for feature extraction from the raw time-series input (Fig. 1a). Briefly, it uses multiple 1-D convolution layers to condense the time dimension and extract local features of the time-series. The convolution-subsampling
tokenizer consists of two 1-D convolution layers with kernel size 3
and stride 2, reducing the sequence length to 1/4. Unlike the prevalent patching technique, which merely segments adjacent timesteps
and features [ 21 ], the convolution tokenizer effectively captures
local temporal interactions. More details are provided in Appendix

B.3.


Ret( *ùëø* ) = ( *ùë∏* [ÀÜ] *ùë≤* [ÀÜ] [‚ä§] ‚äô *ùë´* ) *ùëΩ, ùë´* *ùëõùëö* =


*ùõæ* *[ùëõ]* [‚àí] *[ùëö]* *,* *ùëõ* ‚â• *ùëö*
(3)
0 *,* *ùëõ* *< ùëö*
ÔøΩ


where the decay matrix *ùë´* ‚àà *ùëÖ* *[ùëÅ]* [√ó] *[ùëÅ]* and rotation matrix *ùëí* *[ùëñùúÉ]* [(] *[ùëõ]* [‚àí] *[ùëö]* [)]

encode trend and periodic patterns into token embedding, taking
into account the distance between tokens *ùëõ* ‚àí *ùëö* . When reformulated

as an RNN, the Retention in Eq. 3 can be manifested in a recurrent
forward-pass with a constant inference complexity. This reformulated RNN excels in capturing sequential dependencies from the
time-series. To handle long sequences, we use chunk-wise Retention by segmenting the sequence into multiple, non-overlapping
chunks (Fig. 1c). Consequently, chunk-wise Retention maintains a
linear complexity for long sequences. We provide details about the
three Retention forward-passes in Appendix B.2.
To accommodate irregularly-sampled time series, we modify the
Retention mechanism as follows. Given *ùëÅ* samples { *ùë†* 1 *, . . .,ùë†* *ùëÅ* },
each sample *ùë†* *ùëõ* is represented as a tuple ( *ùë•* *ùëõ* *,ùë°* *ùëõ* ), consisting of an
observation *ùë•* *ùëõ* and a timestep *ùë°* *ùëõ* . Given two samples *ùë†* *ùëõ* and *ùë†* *ùëö*, the
decay mask *ùë´* is adapted according to the time gap Œî *ùë°* *ùëõ,ùëö* = *ùë°* *ùëõ* ‚àí *ùë°* *ùëö* :


Ret( *ùëø* ) = ( *ùë∏ùë≤* [‚ä§] ‚äô *ùë´* ) *ùëΩ, ùë´* *ùëõùëö* =


*ùõæ* [Œî] *[ùë°]* *[ùëõ,ùëö]* *,* *ùë°* *ùëõ* ‚â• *ùë°* *ùëö*
(4)
ÔøΩ0 *,* *ùë°* *ùëõ* *< ùë°* *ùëö*


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China


As the second part of our third contribution, we propose a **tem-**
**poral convolution module** using a depth-wise separable convolution [ 3 ], sifting local temporal features from the time-series
representations. As shown in Fig. 1d, this module starts with a layer
normalization, followed by a 1-D depth-wise convolution and a
point-wise convolution layer, with batch normalization and swish
activation after the depth-wise convolution. Integrating convolution and attention allows TimelyGPT to extract global-local feature
interactions [ 10, 49 ]. By stacking multiple decoder layers, each with
a convolution module, TimelyGPT discerns multi-scale features
that characterize patterns across varying time scales [42].
##### **3.4 Computational complexity**

TimelyGPT with its efficient Retention mechanism achieves *ùëÇ* ( *ùëÅ* )
training complexity and *ùëÇ* ( 1 ) inference complexity. In contrast,
BERT and GPT incur *ùëÇ* ( *ùëÅ* [2] ) training complexity and *ùëÇ* ( *ùëÅ* ) inference complexity [ 14 ]. The vanilla attention mechanism in the


Transformer, Attention( *ùëã* ) = Softmax( *[ùëÑ]* *[ùêæ]* *[ùëá]*


*ùëë* [)] *[ùëâ]* [, introduces a train-]

##### **4.2 PopHR database**

The Population Health Record (PopHR) database hosts a massive
amount of longitudinal claim data from the provincial government
health insurer in Quebec, Canada (R√©gie de l‚Äôassurance maladie
du Qu√©bec, RAMQ) on health service use [ 32, 51 ]. In total, there
are approximately 1.3 million participants in the PopHR database,
which represents a randomly sampled 25% of the population in
the metropolitan area of Montreal between 1998 and 2014. Cohort
memberships are maintained dynamically by removing deceased
residents and actively enrolling newborns and immigrants. We
extracted irregularly-sampled time series from the patient clinical
records in the PopHR database. Specifically, we converted ICD-9
diagnostic codes to phenotype codes (PheCodes) using the expert[defined PheWAS catalog [](https://phewascatalog.org/phecodes) 5, 6 ]. we selected 315 unique PheCodes
each with over 50,000 token counts and excluded patients who had
fewer than 50 PheCode tokens. This resulted in a dataset of 489,000
patients, averaging 112 diagnosis records each.
##### **5 EXPERIMENTS**

We first validated the scaling pattern of TimelyGPT, determining
the optimal number of model parameters for different dataset sizes
(Section 6.1). We then explored TimelyGPT‚Äôs extrapolation capabilities for long-term forecasting up to 6,000 timesteps in Sleep-EDF‚Äôs
biosignal data, and analyzed extrapolation‚Äôs underlying mechanism through visualization (Section 6.2). Our evaluation extended
forecasting to irregularly-sampled time series (Section 6.3). Furthermore, we conducted ablation studies to evaluate the contributions
of various components (Section 6.4).
##### **5.1 Pre-training and fine-tuning**

During pre-training, TimelyGPT utilizes a next-token prediction
task to learn general temporal representations from unlabeled data

[ 29 ]. Given a sequence with a [SOS] token, TimelyGPT predicts the
subsequent tokens by shifting the sequence to the right. At the last
layer, each token‚Äôs output representation is fed into a linear layer for
next-token prediction. The pre-training loss is Mean Squared Error
(MSE) for continuous signals (e.g., biosignal) and cross-entropy for
discrete signals (e.g., diagnosis codes).
Among other Transformer baselines, PatchTST adopted a maskingbased approach, masking 40% of its patches as zeros [ 21 ]. CRT utilized a dropping-based pre-training, discarding up to 70% of patches

[ 56 ]. For the Transformer models without established pre-training
methods, we used a masking-based method by randomly masking
40% of timesteps [ 54 ].For downstream forecasting tasks, we employ end-to-end fine-tuning on the entire model. The final linear
layer is utilized for making the forecasts. All Transformer models
performed 20 epochs of pre-training with MSE loss, followed by 5
epochs of end-to-end fine-tuning.
##### **5.2 Jointly forecasting multivariate biosignals** **from Sleep-EDF dataset**

We utilized all seven features from the Sleep-EDF dataset for a
multivariate forecasting task, applying standardization as preprocessing. The Sleep-EDF dataset was split into training (80%), validation (10%), and test (10%) sets. All models were pre-trained on
the entire training set and fine-tuned on a randomly chosen 20%


‚àö


ing complexity of *ùëÇ* ( *ùëÅ* [2] *ùëë* ) . This quadratic computational bottleneck prevents standard Transformer models from modeling long
sequences (i.e., *ùëÅ* *>> ùëë* ).
TimelyGPT achieves linear training complexity by following
research in linear transformers [ 14 ]. In the Retention mechanism,
Ret( *ùëã* *ùëõ* ) = *ùëÑ* *ùëõ* *ùëÜ* *ùëõ* *,ùëÜ* *ùëõ* = *ùêæ* *ùëõ* *[ùëá]* *ùëâ* *ùëõ* + *ùõæùëÜ* *ùëõ* ‚àí1, both *ùëÑ* *ùëõ* *ùëÜ* *ùëõ* and *ùêæ* *ùëõ* *[ùëá]* *ùëâ* *ùëõ* have
*ùëÇ* ( *ùëë* [2] ) complexity. By recursively updating over *ùëÅ* timesteps, the
total complexity becomes *ùëÇ* ( *ùëÅùëë* [2] ). For inference, TimelyGPT proposes time-specific and trajectory-based methods. The trajectorybased inference recursively generates sequences with equally-spaced
time intervals like the GPT model, incurring *ùëÇ* ( *ùëÅ* ) inference complexity. In contrast, the time-specific inference directly predicts
target time point with *ùëÇ* ( 1 ) complexity. Therefore, TimelyGPT
achieves *ùëÇ* ( *ùëÅ* ) training complexity and *ùëÇ* ( 1 ) inference complexity,
making it computationally efficient and suitable for long sequences.
We provided detailed discussion of computational bottleneck of
Transformer and efficient linear Transformer in Appendix A.1.
##### **4 DATA** **4.1 Sleep-EDF dataset**

The Sleep European Data Format (EDF) database, sourced from
PhysioBank [ 8 ], contains sleep recordings from 153 healthy subjects [ 15 ]. These whole-night polysomnographic sleep recordings
include 7 types of biosignalS: electroencephalogram (EEG) from
Fpz-Cz and Pz-Oz electrode locations, electrooculogram (EOG), submental chin electromyogram (EMG), oro-nasal airflow, rectal body
temperature, and an event marker. Both EEG and EOG signals were
sampled at 100 Hz (i.e., the signals were recorded at a rate of 100
samples per second), while EMG and the other features were sampled at 1 Hz (i.e., 1 sample per second). Sleep patterns (hypnograms)
were manually scored by trained technicians into five sleep stages.
This biosignal dataset comprises a total of 1.2 billion timesteps, segmented into 300,700 sequences of 4,000 timesteps each. It provides
large-scale continuous time-series data for training large models.
In our experiment, we forecast all 7 biosignals.


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


subset of the training data, with time-series data segmented into
non-overlapping sequences. For pre-training, we chose an input
length of 4,000 timesteps. For fine-tuning, we used a look-up window of 2,000 timesteps and varied forecasting windows of 720,
2,000, and 6,000 timesteps. We used MAE as a metric. We evaluated
TimelyGPT against Informer [ 59 ], Autoformer [ 48 ], FEDformer

[ 60 ], PatchTST [ 21 ], TimesNet [ 47 ], TS2Vec [ 52 ], and DLinear [ 53 ].
Based on the scaling law in Section 6.1, we set the model parameters
for all transformers to around 18 million, with specific architectures
and parameters detailed in Table S2.
##### **5.3 Forecasting irregularly-sampled diagnostic** **codes from PopHR dataset**

We assessed long-term forecasting task of the irregularly-sampled
time series extracted from the PopHR database. We divided the
dataset into training (80%), validation (10%), and testing (10%) sets.
We pre-trained on the entire training set and fine-tuned on a 20%
subset of training data. We used cross entropy and top- *ùêæ* recall to
evaluate the pre-training and fine-tuning, respectively. For forecasting, we set the look-up window to be 50 timestamps and the
rest as the forecasting window, containing up to more than 100
timestamps (i.e., diagnosis codes).
For our TimelyGPT, we separately evaluated the performance
of trajectory-based and time-specific inferences (Section 3.2). We
compared with several transformer baselines, including Informer,
Fedformer, AutoFormer, and PatchTST as well as the models designed for irregularly-sampled time series, namely mTAND [ 35 ]
and SeFT [ 11 ]. Given that diagnoses are discrete values, there was
no need to utilize the convolution-subsampling tokenizer for TimelyGPT. Furthermore, we specified a patch size of 2 for PatchTST,
indicating that every two adjacent timestamps are projected into a
single patch. Based on the scaling law in Section 6.1, we set model
parameters for all transformers to about 7.5 million, with specific
architectures and parameters detailed in Table S2.
##### **5.4 Model parameters**

For all benchmark experiments, we tailored the architecture and
parameters of TimelyGPT based on the scaling-law analysis (Section 6.1; Fig. 3). Specifically, for the Sleep-EDF dataset, TimelyGPT
was configured with 18 million parameters, and for the PopHR
dataset, it was configured with 7.5 million parameters. While different Transformer models may have unique optimal hyperparameters,
optimizing each model‚Äôs setup is computationally prohibitive with
our current compute resources. For fairness of comparison, we
compared TimelyGPT against all transformer baselines at the same
model size (Table S2).
##### **6 RESULTS** **6.1 Scalability of TimelyGPT**

We evaluated the scalability of TimelyGPT on the large-scale SleepEDF dataset to determine the optimal model parameters with respect to different dataset sizes [ 15 ]. We selected subsets of the
Sleep-EDF dataset with timesteps ranging from 10 [5] to 10 [9], splitting each dataset into training (80%), validation (10%), and testing
(10%) sets. Both look-up and forecasting windows were set to 256


timesteps for this experiment. TimelyGPT‚Äôs performance improves
as parameter and dataset size increase (Fig. 3), which is attributed
to its capacity to handle more data, known as the scaling law for
Transformer [ 13 ]. We provide further discussion of scaling patterns
of the existing Transformer models in Appendix A.3.
##### **6.2 Forecasting multivariate Sleep-EDF** **biosignals**

TimelyGPT achieved the best performance in forecasting biosignals
for all windows in terms of MAE, except for window 720 (Table
1; Fig. 4a). PatchTST achieved the best MAE at 0.456, whereas
TimelyGPT conferred comparable performance. DLinear was also
effective for the 720-timestep forecasting window. However, as the
forecasting window increased to 2,000 and 6,000 timesteps, both
PatchTST and DLinear suffered performance drops due to their
reliance on the linear layers and inability to extrapolate beyond the
training length. In contrast, pre-trained on 4,000 timesteps, TimelyGPT consistently maintained superior performance up to 6,000
timesteps given a short look-up window (i.e., prompt) containing
only 2,000 timesteps. Additionally, TimelyGPT consistently outperformed other baselines across all three forecasting windows in
terms of cross-correlation performance (Fig. 4b; Table 1). TimesNet

**Figure 3: Test MAE of forecasting Sleep-EDF biosignals as a**
**function of dataset sizes and parameter sizes. Both look-up**
**and forecasting windows were set to 256 timesteps. Time-**
**lyGPT with more parameters tends to exhibit better perfor-**
**mance when trained on larger datasets.**

**Table 1: Comparison of TimelyGPT as well as 7 baselines**
**for long-term forecasting experiment on the large-scale**
**SleepEDF dataset. Bold and underlined numbers indicate**
**the best and second best results for each metric and window.**

|Window Size|MAE|Col3|Col4|Cross-Correlation|Col6|Col7|
|---|---|---|---|---|---|---|
|Metrics|720|2000|6000|720|2000|6000|
|TimelyGPT Informer Autoformer Fedformer PatchTST DLinear TS2Vec TimesNet|0.542 0.675 0.532 0.515 0.456 0.521 0.602 0.471|0.567 1.013 0.908 0.865 0.768 0.840 1.231 0.742|0.575 1.256 1.026 0.912 0.824 0.929 1.204 0.865|0.644 0.352 0.452 0.386 0.569 0.452 0.415 0.602|0.628 0.256 0.401 0.307 0.512 0.369 0.301 0.573|0.607 0.221 0.279 0.314 0.370 0.189 0.223 0.403|


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China

a. MAE across window sizes b. Cross correlation across window sizes


1.2

1.0

0.8

0.6


0.6

0.5

0.4

0.3

0.2


720 2000 6000

Forecasting window (timesteps)


720 2000 6000
Forecasting window (timesteps)


**Figure 4: SleepEDF biosignal forecasting performances of TimelyGPT and seven state-of-the-art methods over various forecast-**
**ing windows. a. MAE for 8 methods evaluated over 3 forecasting windows (720, 2000, and 6000 timesteps). b. Cross-correlation**
**scores for the same methods and forecasting windows. The detailed numerical results are summarized in Table 1.**

#### Look-up window

#### Forecasting window



**Figure 5: Predicted sequence of SleepEDF biosignals of 6,000 timesteps. Given a 2,000 look-up window, we applied TimelyGPT**
**(blue solid line) and 4 state-of-the-art methods (dashed lines) to predict the biosignals for the next 6,000 timesteps. The**
**groundtruth biosignals are displayed as red solid line. The two vertical lines demarcate the look-up window and the length of**
**pre-training sequences, respectively.**


was the second best performer for these windows, but declined as
window size gets larger due to the extrapolation issue. These results
underscore TimelyGPT‚Äôs extrapolation capabilities in long-term
forecasting, aligning with the findings in the NLP domain [41].
We visualized the predicted biosignals by TimelyGPT against the
leading baselines (PatchTST and DLinear) and the ablated methods
(GPT-2 and GPT-2 with RoPE), focusing on sleep stage transitions
(Fig. 5). We utilized a 2,000-timestep look-up window and a 6,000timestep forecasting window. Forecasting beyond 2,000 timesteps
is marked as extrapolation, as it exceeds the training length. In the
rectal temperature (i.e., trend signal), TimelyGPT‚Äôs forecast aligned
well with the groundtruth, effectively capturing distinct trend patterns. Notably, the small bump in the prompt before the 1000-th
timestep is a typical indicator for temperature drop. Most models
were able to capture it except for DLinear, showing the benefits
of pre-training. Beyond the training length of 4000, TimelyGPT
demonstrated more advantages in accurately extrapolating the rise
of the rectal temperature around 7000-th timestep while PatchTST
and GPT fell behind. The superior extrapolation capabilities of
TimelyGPT is attributable to its ability to capture the long-term


trends with xPos embedding. In contrast, both PatchTST and vanilla
GPT experienced a performance decline, likely due to the dependency on linear mapping as discussed in previous research [ 17 ].
Additionally, TimelyGPT exhibits superior extrapolation capabilities over the ablated baseline GPT+RoPE, highlighting its effective
trend pattern modeling for extrapolation. We also visualized EEG
periodic biosignal forecast and found a similar conclusion (Fig. S3).
##### **6.3 Forecasting patient diagnosis trajectory**

We then applied TimelyGPT and the baseline methods to forecast
315 PheCodes for 489K patients from PopHR (Section 4.2). We
evaluated the performance using the average top K recall at each
forecast window. TimelyGPT with time-specific inference outperformed the baselines reaching the highest recall rates of 58.65%
and 70.83% at *ùêæ* = 5 and *ùêæ* = 10, respectively (Table 2). At *ùêæ* = 15,
TimelyGPT ranked the second-highest recall with 82.69%. In addition, the time-specific inference outperformed the trajectory-based
inference, highlighting the advantage of time decay mechanism.


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


**Table 2: Forecasting results of TimelyGPT and 6 baselines on**
**PopHR‚Äôs irregular-sampled time series dataset. TimelyGPT**
**with time-specific inference achieved the highest recall at**
*ùêæ* = 5 **and** *ùêæ* = 10 **, and the second highest at** *ùêæ* = 15 **, demon-**
**strating its superior performance in long-term forecasting**
**of irregularly-sampled time series.**

**Recall @** *ùêæ* **(%)**
**Metrics**
*ùêæ* = 5 *ùêæ* = 10 *ùêæ* = 15

**TimelyGPT (trajectory-based)** 52.30 64.35 77.12
**Timel** **y** **GPT** **(** **time-s** **p** **ecific** **)** **58.65** **70.83** 82.69
Informer 46.37 60.14 71.24

Autoformer 42.87 57.43 68.59

Fedformer 43.31 58.34 69.60

PatchTST 48.17 65.55 73.31

MTand 52.59 70.21 **83.73**

SeFT 49.26 68.10 79.39


We then examined the distributions of the top-5 recall rates at
3 forecast windows, comparing two inference methods of TimelyGPT with the best transformer baseline PatchTST and the leading irregular time series algorithm MTand (Fig. 6). TimelyGPT‚Äôs
time-specific inference consistently outperformed trajectory-based
inference as the forecasting window size increases. While both
inference methods exhibited similar performance for predicting
the first 50 timesteps, time-specific TimelyGPT demonstrated significantly better results beyond 50 timesteps. This improvement is
likely due to time-specific inference taking into account the evolving states and the query timestep in the time decay mechanism,
enhancing its ability to predict the temporal evolution of healthcare

Forecasting Windows

**Figure 6: The distribution of top-5 recall performance for**
**TimelyGPT with two inference methods (Time-specific and**
**Trajectory-based), compared to PatchTST and MTand across**
**three forecasting window sizes.**


trajectories over irregular intervals. As expected, all models experienced a performance decline in predicting farther future because of
the increasing uncertainties. Despite this, TimelyGPT maintained
higher and more stable performance within the first 100 steps compared to PatchTST and MTand. Although MTand closely followed to
time-specific TimelyGPT for the first 50 timesteps, its performance
drastically declines as the forecasting window increases, reflecting
its difficulty with extrapolation. These findings highlight the utility
of the proposed time-specific inference in leveraging time-decay
mechanism to handle irregularly-sampled time series for long-term
forecasting.
We visualized the observed and predicted trajectory of a patient
with neoplasm and genitourinary diseases (Fig. 7). TimelyGPT with
time-specific inference produced a high top-5 recall rate of 85.7%
on this patient. Indeed, most of the observed codes were among the
top 5 predicted codes by the time-specific TimelyGPT. Zooming
into the forecast window (Fig. 7b), TimelyGPT accurately predicted
Phecodes 590.0 (Pyelonephritis) three times around the age of 61.
TimelyGPT predicted PheCode 740.9 at age 61 with high probability,
which appeared twice at ages 52 and 53 in the look-up window.
Therefore, TimelyGPT demonstrated a promising direction to forecast patient health state despite the challenges inherent in modeling
irregularly-sampled longitudinal EHR data.
##### **6.4 Ablation study**

To assess the contributions of various components in TimelyGPT,
we conducted ablation studies by omitting the key components, including convolution subsampling tokenizer, temporal convolution
module, exponential decay, and RoPE relative position embedding.
Notably, removing all components results in a vanilla GPT-2. Since
exponential decay in xPos depends on RoPE, we cannot assess the
impact of exponential decay independently by removing the RoPE
component. Additionally, we also ablated the pre-training strategy
by training TimelyGPT from scratch on the forecasting tasks. The
ablation studies focused on downstream forecasting experiments
using the Sleep-EDF and PopHR datasets, corresponding to continuous biosignals and irregularly-sampled time series, respectively. We
conducted the ablation on long-term forecasting of 6000 timesteps
in the Sleep-EDF dataset and evaluated the top-5 recall scores in
the PopHR dataset.

**Table 3: Ablation results of TimelyGPT w/o specific compo-**
**nents, showing forecasting performance for a 6,000-timestep**
**window in the Sleep-EDF dataset and top-15 recall rate in the**
**PopHR dataset.**

|TimelyGPT (with Pre-training) 0.575 58.65 w/o Convolution Subsampling 0.587 ‚Äî w/o Temporal Convolution 0.581 57.69 w/o Exponential Decay 0.715 52.50 w/o RoPE (GPT-2) 1.072 50.18|Col2|
|---|---|
|TimelyGPT (w/o Pre-training)|0.641 56.42|


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China

**Figure 7: Visualization of a cancer patient‚Äôs medical trajectory from the PopHR dataset. a. Look-up and forecast windows.**
**Matched predictions (solid circles) were identified when the top 5 predicted PheCodes contain the groundtruth. b. The top 5**
**predicted PheCodes for the final 5 timesteps of the subject.**


As shown in Table 3, for the Sleep-EDF forecasting task, removing the RoPE component led to the most significant performance
degradation (a MAE of 0.357). The removal of exponential decay
also led to increase MAE of 0.134, demonstrating its benefits of encoding trend patterns for long-term forecasting. Together, the two
ablation experiments show the importance of xPos as our first main
contribution (Section 3.1). The integration of convolution modules
helps TimelyGPT capture local features, although the benefits were
smaller compared with other components.
In the forecasting of irregularly-sampled time series, the exponential decay and RoPE components improved performance by
6.15% and 2.32%, respectively. The time decay mechanism encodes
trend patterns into the modeling of patients‚Äô health trajectories,
making it a promising approach for forecasting irregular clinical
diagnoses. Pre-training decreased MAE by 0.066 for forecasting
continuous biosignals in Sleep-EDF and increased top K recall rate
by 2.21% for forecasting irregularly sampled diagnostic codes.
##### **7 CONCLUSION AND FUTURE WORK**

TimelyGPT effectively forecasts long sequences of time-series, utilizing xPos embedding, recurrent attention, and convolution modules. For continuously monitored biosignals such as Sleep-EDF,
TimelyGPT can accurately extrapolate up to 6,000 timesteps given
only a 2000-timestep prompt. Moreover, TimelyGPT also effectively
forecasts irregularly-sampled time series by conditioning the recurrent Retention on the time. In our future work, we will perform
comprehensive and in-depth analysis on the trajectory inference
of the EHR data, as it may have a profound impact on the future of
patient care and early intervention. TimelyGPT is a causal model
with unidirectional attention [ 27 ]. This may limit its expressiveness in terms of time-series representation learning, which may be
improved via a bidirectional architecture. To enhance transfer learning, we will adapt TimelyGPT for out-of-distribution biosignals,
further enhancing its utility in healthcare time-series.
##### **REFERENCES**

[1] Yuri Ahuja, Yuesong Zou, Aman Verma, David Buckeridge, and Yue Li. 2022.
MixEHR-Guided: A guided multi-modal topic modeling approach for large-scale


automatic phenotyping using the electronic health record. *Journal of biomedical*
*informatics* 134 (2022), 104190.

[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
[arXiv:2005.14165 [cs.CL]](https://arxiv.org/abs/2005.14165)

[3] Fran√ßois Chollet. 2017. Xception: Deep Learning with Depthwise Separable
[Convolutions. arXiv:1610.02357 [cs.CV]](https://arxiv.org/abs/1610.02357)

[4] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan
Salakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a
[Fixed-Length Context. arXiv:1901.02860 [cs.LG]](https://arxiv.org/abs/1901.02860)

[5] Joshua Denny, Lisa Bastarache, Marylyn Ritchie, Robert Carroll, Raquel Zink,
Jonathan Mosley, Julie Field, Jill Pulley, Andrea Ramirez, Erica Bowton, Melissa
Basford, David Carrell, Peggy Peissig, Abel Kho, Jennifer Pacheco, Luke Rasmussen, David Crosslin, Paul Crane, Jyotishman Pathak, and Dan Roden. 2013.
Systematic comparison of phenome-wide association study of electronic medical
record data and genome-wide association study data. *Nature biotechnology* 31
[(11 2013). https://doi.org/10.1038/nbt.2749](https://doi.org/10.1038/nbt.2749)

[6] Joshua C. Denny, Marylyn D. Ritchie, Melissa A. Basford, Jill M. Pulley, Lisa
Bastarache, Kristin Brown-Gentry, Deede Wang, Dan R. Masys, Dan M. Roden,
and Dana C. Crawford. 2010. PheWAS: demonstrating the feasibility of a phenomewide scan to discover gene‚Äìdisease associations. *Bioinformatics* 26, 9 (03 2010),
[1205‚Äì1210. https://doi.org/10.1093/bioinformatics/btq126](https://doi.org/10.1093/bioinformatics/btq126)

[7] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong
Kwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-Series Representation Learning via
Temporal and Contextual Contrasting. In *Proceedings of the Thirtieth International*
*Joint Conference on Artificial Intelligence, IJCAI-21* . 2352‚Äì2359.

[8] A.L. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P.C. Ivanov, R. Mark, J.E. Mietus,
G.B. Moody, C.K. Peng, and H.E. Stanley. 2000. PhysioBank, PhysioToolkit, and
PhysioNet: components of a new research resource for complex physiologic
signals. *Circulation* [101, 23 (2000), e215‚Äìe220. https://doi.org/0.1161/01.cir.101.](https://doi.org/0.1161/01.cir.101.23.e215)
[23.e215](https://doi.org/0.1161/01.cir.101.23.e215)

[9] Albert Gu, Karan Goel, and Christopher R√©. 2022. Efficiently Modeling Long
[Sequences with Structured State Spaces. arXiv:2111.00396 [cs.LG]](https://arxiv.org/abs/2111.00396)

[10] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui
Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.
2020. Conformer: Convolution-augmented Transformer for Speech Recognition.
[5036‚Äì5040. https://doi.org/10.21437/Interspeech.2020-3015](https://doi.org/10.21437/Interspeech.2020-3015)

[11] Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt.
[2020. Set Functions for Time Series. arXiv:1909.12064 [cs.LG]](https://arxiv.org/abs/1909.12064)

[12] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.
PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for
[Traffic Flow Prediction. arXiv:2301.07945 [cs.LG]](https://arxiv.org/abs/2301.07945)

[13] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,
Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
[Scaling Laws for Neural Language Models. arXiv:2001.08361 [cs.LG]](https://arxiv.org/abs/2001.08361)

[14] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret.
2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


[Attention. arXiv:2006.16236 [cs.LG]](https://arxiv.org/abs/2006.16236)

[15] B. Kemp, A.H. Zwinderman, B. Tuk, H.A.C. Kamphuisen, and J.J.L. Oberye. 2000.
Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG. *IEEE Transactions on Biomedical Engineering* 47, 9 (2000),
[1185‚Äì1194. https://doi.org/10.1109/10.867928](https://doi.org/10.1109/10.867928)

[16] Yann LeCun and Yoshua Bengio. 1998. *Convolutional Networks for Images, Speech,*
*and Time Series* . MIT Press, Cambridge, MA, USA, 255‚Äì258.

[17] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. 2023. Revisiting Long-term Time Series
[Forecasting: An Investigation on Linear Mapping. arXiv:2305.10721 [cs.LG]](https://arxiv.org/abs/2305.10721)

[18] Andy T. Liu, Shang-Wen Li, and Hung yi Lee. 2021. TERA: Self-Supervised
Learning of Transformer Encoder Representation for Speech. *IEEE/ACM Trans-*
*actions on Audio, Speech, and Language Processing* [29 (2021), 2351‚Äì2366. https:](https://doi.org/10.1109/taslp.2021.3095662)
[//doi.org/10.1109/taslp.2021.3095662](https://doi.org/10.1109/taslp.2021.3095662)

[19] Andy T. Liu, Shu wen Yang, Po-Han Chi, Po chun Hsu, and Hung yi Lee.
2020. Mockingjay: Unsupervised Speech Representation Learning with Deep
Bidirectional Transformer Encoders. In *ICASSP 2020 - 2020 IEEE International*
*Conference on Acoustics, Speech and Signal Processing (ICASSP)* . IEEE. [https:](https://doi.org/10.1109/icassp40776.2020.9054458)
[//doi.org/10.1109/icassp40776.2020.9054458](https://doi.org/10.1109/icassp40776.2020.9054458)

[20] Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong
Yu, and James T. Kwok. 2023. A Survey on Time-Series Pre-Trained Models.
[arXiv:2305.10716 [cs.LG]](https://arxiv.org/abs/2305.10716)

[21] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.
A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.
[arXiv:2211.14730 [cs.LG]](https://arxiv.org/abs/2211.14730)

[22] Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre
Dognin, Jerret Ross, Ravi Nair, and Erik Altman. 2021. Tabular Transformers for
[Modeling Multivariate Time Series. arXiv:2011.01843 [cs.LG]](https://arxiv.org/abs/2011.01843)

[23] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming
[Curated Corpora with Web Data, and Web Data Only. arXiv:2306.01116 [cs.CL]](https://arxiv.org/abs/2306.01116)

[24] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,
Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong,
Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom,
Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu,
and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era.
[arXiv:2305.13048 [cs.CL]](https://arxiv.org/abs/2305.13048)

[25] Huy Phan, Oliver Y. Chen, Minh C. Tran, Philipp Koch, Alfred Mertins, and
Maarten De Vos. 2021. XSleepNet: Multi-View Sequential Model for Automatic
Sleep Staging. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
[(2021), 1‚Äì1. https://doi.org/10.1109/tpami.2021.3070057](https://doi.org/10.1109/tpami.2021.3070057)

[26] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R√©. 2023. Hyena Hierarchy:
[Towards Larger Convolutional Language Models. arXiv:2302.10866 [cs.LG]](https://arxiv.org/abs/2302.10866)

[27] Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention
[with Linear Biases Enables Input Length Extrapolation. arXiv:2108.12409 [cs.CL]](https://arxiv.org/abs/2108.12409)

[28] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey,
and Ilya Sutskever. 2022. Robust Speech Recognition via Large-Scale Weak
[Supervision. arXiv:2212.04356 [eess.AS]](https://arxiv.org/abs/2212.04356)

[29] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
[Sutskever. 2019. Language Models are Unsupervised Multitask Learners. https:](https://api.semanticscholar.org/CorpusID:160025533)
[//api.semanticscholar.org/CorpusID:160025533](https://api.semanticscholar.org/CorpusID:160025533)

[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
[arXiv:1910.10683 [cs.LG]](https://arxiv.org/abs/1910.10683)

[31] Attila Reiss, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. 2019. Deep
PPG: Large-Scale Heart Rate Estimation with Convolutional Neural Networks.
*Sensors* [19, 14 (2019). https://doi.org/10.3390/s19143079](https://doi.org/10.3390/s19143079)

[32] Arash Shaban-Nejad, Maxime Lavigne, Anya Okhmatovskaia, and David Buckeridge. 2016. PopHR: a knowledge-based platform to support integration,
analysis, and visualization of population health data: The Population Health
Record (PopHR). *Annals of the New York Academy of Sciences* 1387 (10 2016).
[https://doi.org/10.1111/nyas.13271](https://doi.org/10.1111/nyas.13271)

[33] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training Enhanced
Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting.
In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and*
*Data Mining* [. ACM. https://doi.org/10.1145/3534678.3539396](https://doi.org/10.1145/3534678.3539396)

[34] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In *Proceedings of the 2018 Conference of the*
*North American Chapter of the Association for Computational Linguistics: Human*
*Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018,*
*Volume 2 (Short Papers)*, Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.).
[Association for Computational Linguistics, 464‚Äì468. https://doi.org/10.18653/](https://doi.org/10.18653/V1/N18-2074)
[V1/N18-2074](https://doi.org/10.18653/V1/N18-2074)



[35] Satya Narayan Shukla and Benjamin M. Marlin. 2021. Multi-Time Attention
[Networks for Irregularly Sampled Time Series. arXiv:2101.10318 [cs.LG]](https://arxiv.org/abs/2101.10318)

[36] Yonghao Song, Qingqing Zheng, Bingchuan Liu, and Xiaorong Gao. 2023. EEG
Conformer: Convolutional Transformer for EEG Decoding and Visualization.
*IEEE Transactions on Neural Systems and Rehabilitation Engineering* 31 (2023),
[710‚Äì719. https://doi.org/10.1109/TNSRE.2022.3230250](https://doi.org/10.1109/TNSRE.2022.3230250)

[37] Ziyang Song, Yuanyi Hu, Aman Verma, David L. Buckeridge, and Yue Li. 2022.
Automatic Phenotyping by a Seed-guided Topic Model. In *Proceedings of the 28th*
*ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (Washington
DC, USA) *(KDD ‚Äô22)* . Association for Computing Machinery, New York, NY, USA,
[4713‚Äì4723. https://doi.org/10.1145/3534678.3542675](https://doi.org/10.1145/3534678.3542675)

[38] Rachel Stirling, Mark Cook, David Grayden, and Pip Karoly. 2020. Seizure
forecasting and cyclic control of seizures. *Epilepsia* 62 Suppl 1 (07 2020).
[https://doi.org/10.1111/epi.16541](https://doi.org/10.1111/epi.16541)

[39] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng
Liu. 2022. RoFormer: Enhanced Transformer with Rotary Position Embedding.
[arXiv:2104.09864 [cs.CL]](https://arxiv.org/abs/2104.09864)

[40] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive Network: A Successor to Transformer
[for Large Language Models. arXiv:2307.08621 [cs.CL]](https://arxiv.org/abs/2307.08621)

[41] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A Length-Extrapolatable
[Transformer. arXiv:2212.10554 [cs.CL]](https://arxiv.org/abs/2212.10554)

[42] Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Michael Blumenstein, and Jing
Jiang. 2021. Omni-Scale CNNs: a simple and effective kernel size configuration for
time series classification. In *International Conference on Learning Representations* .

[43] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. 2021. MLP-Mixer: An all-MLP Archi[tecture for Vision. arXiv:2105.01601 [cs.CV]](https://arxiv.org/abs/2105.01601)

[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro,
Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.
[arXiv:2302.13971 [cs.CL]](https://arxiv.org/abs/2302.13971)

[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All
[You Need. arXiv:1706.03762 [cs.CL]](https://arxiv.org/abs/1706.03762)

[46] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.
ETSformer: Exponential Smoothing Transformers for Time-series Forecasting.
[arXiv:2202.01381 [cs.LG]](https://arxiv.org/abs/2202.01381)

[47] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series
Analysis. In *International Conference on Learning Representations* .

[48] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2022. Autoformer:
Decomposition Transformers with Auto-Correlation for Long-Term Series Fore[casting. arXiv:2106.13008 [cs.LG]](https://arxiv.org/abs/2106.13008)

[49] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. Lite Trans[former with Long-Short Range Attention. arXiv:2004.11886 [cs.CL]](https://arxiv.org/abs/2004.11886)

[50] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
and Quoc V. Le. 2020. XLNet: Generalized Autoregressive Pretraining for Lan[guage Understanding. arXiv:1906.08237 [cs.CL]](https://arxiv.org/abs/1906.08237)

[51] Mengru Yuan, Guido Powell, Maxime Lavigne, Anya Okhmatovskaia, and David
Buckeridge. 2018. Initial Usability Evaluation of a Knowledge-Based Population
Health Information System: The Population Health Record (PopHR). *AMIA ...*
*Annual Symposium proceedings. AMIA Symposium* 2017 (04 2018), 1878‚Äì1884.

[52] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,
Yunhai Tong, and Bixiong Xu. 2022. TS2Vec: Towards Universal Representation
of Time Series. *Proceedings of the AAAI Conference on Artificial Intelligence* 36
[(Jun. 2022), 8980‚Äì8987. https://doi.org/10.1609/aaai.v36i8.20881](https://doi.org/10.1609/aaai.v36i8.20881)

[53] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2022. Are Transformers
[Effective for Time Series Forecasting? arXiv:2205.13504 [cs.AI]](https://arxiv.org/abs/2205.13504)

[54] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,
and Carsten Eickhoff. 2020. A Transformer-based Framework for Multivariate
[Time Series Representation Learning. arXiv:2010.02803 [cs.LG]](https://arxiv.org/abs/2010.02803)

[55] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022. Scaling
Vision Transformers. In *Proceedings of the IEEE/CVF Conference on Computer*
*Vision and Pattern Recognition (CVPR)* . 12104‚Äì12113.

[56] Wenrui Zhang, Ling Yang, Shijia Geng, and Shenda Hong. 2023. Self-Supervised
Time Series Representation Learning via Cross Reconstruction Transformer.
[arXiv:2205.09928 [cs.LG]](https://arxiv.org/abs/2205.09928)

[57] Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik.
2022. Graph-Guided Network for Irregularly Sampled Multivariate Time Se[ries. arXiv:2110.05357 [cs.LG]](https://arxiv.org/abs/2110.05357)

[58] Liang Zhao, Min Gao, and Zongwei Wang. 2022. ST-GSP: Spatial-Temporal Global
Semantic Representation Learning for Urban Flow Prediction. In *Proceedings of the*
*Fifteenth ACM International Conference on Web Search and Data Mining* (Virtual
Event, AZ, USA) *(WSDM ‚Äô22)* . Association for Computing Machinery, New York,


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China

[NY, USA, 1443‚Äì1451. https://doi.org/10.1145/3488560.3498444](https://doi.org/10.1145/3488560.3498444)

[59] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long
[Sequence Time-Series Forecasting. arXiv:2012.07436 [cs.LG]](https://arxiv.org/abs/2012.07436)

[60] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.
FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series
[Forecasting. arXiv:2201.12740 [cs.LG]](https://arxiv.org/abs/2201.12740)


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China
##### **A REVISITING TRANSFORMERS** **A.1 Efficient attention in Transformer**

Transformer models have found extensive applications in both the Natural Language Processing and Computer Vision domains [ 45 ]. In
the vanilla self-attention mechanism, the query, key, value matrices are denoted as *ùë∏, ùë≤, ùëΩ* ‚àà R *[ùëÅ]* [√ó] *[ùëë]* . The output embedding for the *ùëõ* -th

token is represented as *ùë∂* *ùëõ* = ÔøΩ ÔøΩ *ùëö* *ùëÅùëöùëÅ* [sim][sim] [(] *[ùë∏]* [(] *[ùë∏]* *[ùëõ]* *[,]* *[ùëõ]* *[ùë≤][,]* *[ùëö]* *[ùë≤]* *[ùëö]* [)] *[ùëΩ]* [)] *[ùëö]* [, where the the similarity function represents the softmax of inner-product] [ sim][(] *[ùë∏]* *[ùëõ]* *[,][ ùë≤]* *[ùëö]* [)][ =]

exp( *ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [/] ~~‚àö~~ *ùëë* ) . The self-attention mechanism, also known as token-mixer, aims to integrate information from every token and thus capture

global-range interaction. However, computing the dot product *ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [before the softmax operation introduces computational complexity of]
*ùëÇ* ( *ùëÅ* [2] *ùëë* ). As sequence length increases, this quadratic complexity becomes bottleneck, making it challenging to train for longer sequences.
Many studies have been proposed to address the quadratic issue in self-attention mechanism. The linear attention replaces the softmax term
sim( *ùë∏* *ùëö* *, ùë≤* *ùëö* ) with *ùúô* ( *ùë∏* *ùëõ* ) *ùúô* ( *ùë≤* *ùëö* [‚ä§] [)][ for a nonlinear kernel function] *[ ùúô]* [(¬∑)][ [14], avoiding quadratic computation.]
Recent research has explored alternatives to the token-mixer attention mechanism including Multi-Layer Perceptron (MLP) [ 43 ], convolution [ 26 ], and RNN [ 24, 40 ]. Particularly, RNN-variant models like RWKV and RetNet have successfully scaled up to more than 14
billion parameters, yielding comparable performance to conventional transformers. A fascinating connection between linear attention
and RNNs has been identified [ 14 ], making RNN-based token mixer as efficient as linear attention. The output embedding from linear

attention can be recast as an RNN: *ùë∂* *ùëõ* = *[ùúô]* *ùúô* [(] *[ùë∏]* ( *ùë∏* *[ùëõ]* [)] *ùëõ* [ÔøΩ] ) [ÔøΩ] *ùëö* *[ùëÅ]* *ùëö* *[ùëÅ]* *[ùúô]* *[ùúô]* [(] *[ùë≤]* [(] *ùëö* *[ùë≤]* [‚ä§] *ùëö* [‚ä§] [)] *[ùëΩ]* [)] *[ùëö]* = *ùúô* *[ùúô]* ( [(] *ùë∏* *[ùë∏]* *ùëõ* *[ùëõ]* ) [)] *ùíÅ* *[ùë∫]* *[ùëõ]* *ùëõ* [, where] *[ ùë∫]* *[ùëõ]* [=][ ÔøΩ] *[ùëÅùëö]* *[ùúô]* [(] *[ùë≤]* *ùëö* [‚ä§] [)] *[ùëΩ]* *[ùëö]* *[,][ ùíÅ]* *[ùëõ]* [=][ ÔøΩ] *ùëö* *[ùëÅ]* *[ùúô]* [(] *[ùë≤]* *ùëö* [‚ä§] [)] [. Thus, the output]

embedding *ùë∂* *ùëõ* depends on both *ùë∫* *ùëõ* and *ùíÅ* *ùëõ*, which are incrementally updated through cumulative sums. Thus, the RNN-based token-mixer
not only competes in performance, but also offers linear training and consistent inference complexities. By employing exponential decay
mechanism, it diminishes the influence of distant positions, transitioning from ‚Äútoken-mixing" to ‚Äútime-mixing". Considering RNN‚Äôs historical
effectiveness in time-series and audio domains, it stands out as an excellent choice for temporal modeling.
##### **A.2 Time-series Transformer**

Transformers are increasingly applied in LTSF tasks, attributed to their capabilities in capturing long-term temporal dependencies [ 21, 46, 48,
59, 60 ]. Researchers have modified transformers by incorporating custom attention modules to address complex temporal dependencies

[ 48, 59, 60 ]. Studies like [ 46, 48, 60 ] have introduced time-decomposition techniques into attention mechanisms to bolster modeling capability.
The majority of studies focus on the encoder-decoder architecture, coupled with a one-forward prediction framework [ 59 ]. In this design,
the decoder takes a concatenated input of the context (or prompt) and placeholder forecasting windows, directly generating the resulting
embedding without autoregressive decoding. As a result, these models aim to avoid error accumulation seen in autoregressive frameworks,
but aligning its performance closely with linear models [ 53 ]. Encoder-only models, like patchTST, use the encoded embedding for forecasting
with the help of a linear layer [ 21 ]. Additionally, self-supervised representation learning techniques in time series, such as TS2Vec and
TimesNet, offer valuable representation learning capabilities for forecasting tasks [47, 52].
##### **A.3 Transformer scaling law in time-series**

Despite the broad applications of transformer-based models in time-series data such as speech [ 10, 28 ], biosignals [ 36 ], and traffic flow [ 12, 33 ],
their effectiveness in capturing temporal dependencies in LTSF task has been limited and often underperforms compared to linear models

[ 53 ]. As Table S1 indicates, time-series transformer models often have much more parameters than the dataset size (timestep) with only two
exceptions, namely large-size Conformer and CRT. Such disparities imply that many transformers may be over-parameterized, leading to
highly variable performance. In Section 6.1, our study validates the Transformer scaling law in time-series domain (i.e., scaling up both
model parameters and dataset size to improve performance) [ 13, 55 ]. For all benchmark experiments, our proposed TimelyGPT effectively
pre-trains on large-scale data with model parameters aligned to this scaling law.

**Table S1: The model parameters and utilized datasets of time-series transformers and comparison methods. These setups**
**are sourced from papers and default implementation. Over-parameterization indicates model parameters** *>>* **dataset size**
**(timestep).**

**Method** Application Dimension Layer Model Parameter Dataset Size (Timestep) Param versus Data

Informer Forecasting 512 3 11.3M 69.7K Over-param
Autoformer Forecasting 512 3 10.5M 69.7K Over-param
Fedformer (F/W) Forecasting 512 3 16.3/114.3M 69.7K Over-param
PatchTST Forecasting 128 3 1.2M 69.7K Over-param
DLinear Forecasting    - 1 70K 69.7K Adequate
Conformer (L) Classification 512 18 118.8M 55.9B Adequate
CRT Pre-training 128 18 8.8 M 109.2M Adequate


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


xPOS


Trend Seasonal


Relative position

**Figure S1: The xPos embedding diminishes distant temporal information according to the relative distance, enabling decompo-**
**sition to capture both trend and periodic dynamics in time-series data.**
##### **B DETAILS ABOUT TIMELYGPT** **B.1 From absolute to relative position embedding**

Unlike RNNs or CNNs, the inclusion of positional embedding is essential for the Transformer model. Since the permutation-invariant
self-attention mechanism cannot capture input order, making it challenging to differentiate tokens in various positions. The solution fall into
two categories: (1) incorporate position information into the inputs, i.e., absolute position embedding; (2) modify the attention matrix to
distinguish tokens at different positions, referring to relative position embedding.
In absolute position embedding, the token representation for a given token *ùëõ* consists of a word embedding *ùëø* *ùëõ* and a position embedding
*ùë∑* *ùëõ* . The self-attention mechanism is expressed as:

*ùë∏* *ùëõ* = ( *ùëø* *ùëõ* + *ùë∑* *ùëõ* ) *ùëæ* *ùëÑ* *,* *ùë≤* *ùëõ* = ( *ùëø* *ùëõ* + *ùë∑* *ùëõ* ) *ùëæ* *ùêæ* *,* *ùëΩ* *ùëõ* = ( *ùëø* *ùëõ* + *ùë∑* *ùëõ* ) *ùëæ* *ùëâ*


*ùë®* *ùëõ,ùëö* = softmax( *ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [)] *[,]* *ùë∂* *ùëö* = ‚àëÔ∏Å *ùë®* *ùëõ,ùëö* *ùëΩ* *ùëö* (6)

*ùëö*


where *ùë®* *ùëõ,ùëö* is an attention score between token *ùëõ* and *ùëö* without scaling. The inner-dot product *ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [and output embedding] *[ ùë∂]* *[ùëö]* [can be]
expanded as follows:

*ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [=][ (] *[ùëø]* *[ùëõ]* [+] *[ ùë∑]* *[ùëõ]* [)] *[ùëæ]* *ùëÑ* [((] *[ùëø]* *[ùëö]* [+] *[ ùë∑]* *[ùëö]* [)] *[ùëæ]* *ùêæ* [)] [‚ä§]

= ( *ùëø* *ùëõ* + *ùë∑* *ùëõ* ) *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] [(] *[ùëø]* *[ùëö]* [+] *[ ùë∑]* *[ùëö]* [)] [‚ä§]


= *ùëø* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùëø]* *ùëö* [‚ä§] + *ùëø* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùë∑]* *ùëö* [‚ä§]

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ


+ *ùë∑* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùëø]* *ùëö* [‚ä§]

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

position-token


+ *ùë∑* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùë∑]* *ùëö* [‚ä§]

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

position-position


(7)


token-token


token-position


*ùë∂* *ùëõ* = ‚àëÔ∏Å softmax [ÔøΩ] ( *ùëø* *ùëõ* *ùëæ* *ùëÑ* + *ùë∑* *ùëõ* *ùëæ* *ùëÑ* )( *ùëæ* *ùêæ* [‚ä§] *[ùëø]* *ùëö* [‚ä§] [+] *[ ùëæ]* *ùêæ* [‚ä§] *[ùë∑]* *ùëö* [‚ä§] [)][ÔøΩ] [(] *[ùëø]* *[ùëõ]* [+] *[ ùë∑]* *[ùëö]* [)] *[ùëæ]* *ùëâ* (8)

*ùëö*


where attention arises from four types of interactions: (1) token-token interaction; (2) token-position interaction; (3) position-token interaction;
(4) position-position interaction. However, absolute position embedding only incorporates fixed position information, neglecting the relative
positional difference between the token *ùëõ* and *ùëö* .
In the realm of audio processing, prevalent transformers like Conformer [ 10 ] incorporate relative positional information through the T5
position embedding [ 30 ]. Notably, the T5 model suggests a minimal interaction between tokens and positions, resulting in the exclusion of
token-position and position-token terms from the attention matrix:

*ùë∏* *ùëõ* *ùë≤* *ùëö* [‚ä§] [=] *[ ùëø]* *[ùëõ]* *[ùëæ]* *ùëÑ* *[ùëæ]* *ùêæ* [‚ä§] *[ùëø]* *ùëö* [‚ä§] [+] *[ ùõΩ]* *[ùëõ,ùëö]* (9)

where the position-position interaction term, *ùë∑* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùë∑]* *ùëö* [‚ä§] [, is replaced with a trainable bias related to the position] *[ ùëõ]* [and] *[ ùëö]* [. The T5 position]
embedding follows Transformer-XL, omitting the position term *ùë∑* *ùëö* *ùëæ* *ùëâ* in the attentive aggregation computation [ 4, 50 ]. As a result, the
relative position embedding is only added to the dot product *ùë∏ùë≤* [‚ä§] :


*ùë∂* *ùëõ* = ‚àëÔ∏Å softmax( *ùëø* *ùëõ* *ùëæ* *ùëÑ* *ùëæ* *ùêæ* [‚ä§] *[ùëø]* *ùëö* [‚ä§] [+] *[ ùõΩ]* *[ùëõ,ùëö]* [)] *[ùëø]* *[ùëö]* *[ùëæ]* *ùëâ* (10)

*ùëö*


The RoPE technique leverages the property of rotation matrix to model positional information [ 39 ]. To incorporate this relative position
information into the queries *ùë∏* and keys *ùë≤*, the method aims to identify functions *ùëì* *ùë∏* ( *ùë∏,* - ) and *ùëì* *ùêæ* ( *ùë≤,* - ) that satisfies this invariant criteria


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China

about relative distance:

‚ü® *ùë∏* *ùëõ* *, ùë≤* *ùëö* ‚ü© = ÔøΩ *ùëì* *ùë∏* ( *ùë∏,ùëõ* ) *, ùëì* *ùêæ* ( *ùë≤,ùëö* )ÔøΩ = *ùëî* ( *ùë∏, ùë≤,ùëö* ‚àí *ùëõ* ) *,* (11)

where *ùëî* is a function that depends only on the relative distance *ùëö* ‚àí *ùëõ* and *ùë∏* = *ùëøùëæ* *ùëÑ* and *ùë≤* = *ùëøùëæ* *ùêæ* stand for token embedding for queries
and keys matrices, respectively. RoPE defines the function *ùëì* involving a *ùëë* -dimensional rotation matrix *ùëπ* :

*ùëì* *ùë∏* ( *ùë∏,ùëõ* ) = *ùëπ* *[ùëë]* Œò *,ùëõ* [(] *[ùëø]* *[ùëõ]* *[ùëæ]* *[ùëÑ]* [)] *[,]* *ùëì* *ùë≤* ( *ùë≤,ùëö* ) = *ùëπ* *[ùëë]* Œò *,ùëö* [(] *[ùëø]* *[ùëö]* *[ùëæ]* *[ùêæ]* [)] (12)

With a given hidden size *ùëë*, a block diagonal matrix *ùëπ* *[ùëë]* Œò *,ùëõ* [contains multiple rotation matrices][ (] *[ùëπ]* *ùëõ,ùúÉ* [(][1][)] 1 *[, . . .,][ ùëπ]* *ùëõ,ùúÉ* [(] *[ùëë]* [/] *ùëë* [2] / [)] 2 [)][ on its diagonal:]


*ùëπ* [(][1][)]
*ùëõ,ùúÉ* 1

*...*


*ùëπ* [(] *[ùëë]* [/][2][)]
*ùëõ,ùúÉ* *ùëë* /2


*ùëπ* *[ùëë]*
Œò *,ùëõ* [=]


Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞


*,* *ùëπ* *ùëõ,ùúÉ* [(] *[ùëñ]* [)] *ùëñ* [=] ÔøΩcossin *ùëõùúÉ ùëõùúÉ* *ùëñùëñ* ‚àícossin *ùëõùúÉ ùëõùúÉ* *ùëñùëñ*

Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª


(13)
ÔøΩ


where the rotation hyperparameter *ùúÉ* *ùëñ* = 10000 [‚àí][2][(] *[ùëñ]* [‚àí][1][)/] *[ùëë]* . In RoPE, any even-dimension representation can be built by placing multiple
2-dimensional rotation matrices diagonally within the *ùëπ* *[ùëë]* Œò *,ùëõ* [matrix, expanding hidden size from 2-dimension to] *[ ùëë]* [-dimension. As] *[ ùëπ]* *[ùëë]* Œò *,ùëö* ‚àí *ùëõ* [=]

( *ùëπ* *[ùëë]* Œò *,ùëõ* [)] [‚ä§] *[ùëπ]* *[ùëë]* Œò *,ùëö* [, RoPE satisfies the property outlined in Eq 11:]


‚ü® *ùë∏* *ùëõ* *, ùë≤* *ùëö* ‚ü© =


*ùëë* /2
‚àëÔ∏Å ‚ü® *ùë∏* *ùëõ* [2 *ùëñ* ‚àí 1 : 2 *ùëñ* ] *, ùë≤* *ùëö* [2 *ùëñ* ‚àí 1 : 2 *ùëñ* ]‚ü©

*ùëñ* =1


=


*ùëë* /2
‚àëÔ∏Å *ùëπ* *ùúÉ* *[ùëë]* *ùëñ* *,ùëö* ‚àí *ùëõ* ÔøΩ( *ùëø* *ùëõ* *ùëæ* *ùëÑ* )[2 *ùëñ* ‚àí 1 : 2 *ùëñ* ] *,* ( *ùëø* *ùëö* *ùëæ* *ùêæ* )[2 *ùëñ* ‚àí 1 : 2 *ùëñ* ]ÔøΩ (14)

*ùëñ* =1


In RoPE, relative position information is added to the inner product *ùë∏ùë≤* [‚ä§] by rotating the angles of queries and keys matrices. Recently, [ 41 ]
argues that the sinusoids used in the rotation matrices do not change monotonically. Instead, they oscillate dramatically as the relative
distance increases. This limitation hinders RoPE‚Äôs ability to sequences of extended lengths. To address it, [ 41 ] proposes xPos that preserves
the advantage of ROPE and behaves stably at long-term dependency by measuring position monotonicity [41].
##### **B.2 Equivalence of three forward-pass Retention**

According to Section 3.2, the parallel forward-pass is equivalent to the recurrent forward-pass. With the initial state variable *ùë∫* 0 = 0, the
recurrent forward-pass can be expressed as follows:


**Recurrent:** *ùë∫* *ùëõ* = *ùë≤* *ùëõ* [‚ä§] *[ùëΩ]* *[ùëõ]*

ÔøΩÔøΩÔøΩÔøΩ

Single-token


+ *ùõæùë∫* *ùëõ* ‚àí1 *,* Ret( *ùëø* *ùëõ* ) = *ùë∏* *ùëõ* *ùë∫* *ùëõ*


*ùëõ*
‚àëÔ∏Å *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* *ùë≤* *ùëö* [‚ä§] *[ùëΩ]* *[ùëö]* (15)

*ùëö*


=‚áí *ùë∫* *ùëõ* =


*ùëõ*
‚àëÔ∏Å *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* *ùë≤* *ùëö* [‚ä§] *[ùëΩ]* *[ùëö]* *[,]* Ret( *ùëø* *ùëõ* ) = *ùë∏* *ùëõ*

*ùëö*


where Ret( *ùëø* *ùëõ* ) calculates the Retention at single-time *ùëõ* by considering timestep *ùëñ* up to the current time. It corresponds to the *ùëõ* -th timestep
(row) of parallel forward-pass of Retention.


**Recurrent:** Ret( *ùëø* *ùëõ* ) = *ùë∏* *ùëõ*


*ùëõ*
‚àëÔ∏Å *ùõæ* *[ùëõ]* [‚àí] *[ùëö]* *ùë≤* *ùëö* [‚ä§] *[ùëΩ]* *[ùëö]*

*ùëö*


=‚áí **Parallel:** Ret( *ùëø* *ùëõ* ) = *ùë∏* *ùëõ*

ÔøΩÔøΩÔøΩÔøΩ

1√ó *ùëë* *ùëûùëò*


*ùë≤* *ùëö* [‚ä§] ‚â§ *ùëõ*

ÔøΩÔøΩÔøΩÔøΩ

*ùëë* *ùëûùëò* √ó *ùëõ*


‚äô *ùë´* *ùëö* ‚â§ *ùëõ*

ÔøΩ **ÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩ** ÔøΩ

*ùëõ* √ó *ùëõ*


*ùëΩ* *ùëö* ‚â§ *ùëõ*

ÔøΩÔøΩÔøΩÔøΩ

*ùëõ* √ó *ùëë* *ùë£*


(16)


When the recurrent forward-pass traverses all timesteps, the parallel and recurrent forward-passes of Retention become identical. With the
parallel and recurrent forward-passes of Retention, we aim to show the equivalence between the chunk-wise forward-pass and the parallel
and recurrent forward-passes. The computation of chunk-wise Retention involves both parallel intra-chunk and recurrent inter-chunk
computation as follows.


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.


**Chunk-wise:** Ret( *ùëø* [ *ùëñ* ] ) = ( *ùë∏* [ *ùëñ* ] *ùë≤* [ [‚ä§] *ùëñ* ] [‚äô] *[ùë´]* [)] *[ùëΩ]* [[] *[ùëñ]* []]

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

Intra-chunk


+ ( *ùë∏* [ *ùëñ* ] *ùë∫* [ *ùëñ* ‚àí1] ) ‚äô *ùúÅ*

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

Inter-chunk


*ùë∫* [ *ùëñ* ] = *ùë≤* [ [‚ä§] *ùëñ* ] [(] *[ùëΩ]* [[] *[ùëñ]* []] [ ‚äô] *[ùë´]* *[ùêµ]* [)]

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

Current chunk


+ *ùõæ* *[ùêµ]* *ùë∫* [ *ùëñ* ‚àí1] *, ùúÅ* *ùëó* = *ùõæ* *[ùëó]* (17)

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

Past chunk


where *ùúÅ* = [ *ùõæ* [1] *,ùõæ* [2] *, . . .,ùõæ* *[ùêµ]* ] [‚ä§] is a column-vector of time-decay scaling factor for inter-chunk attention between the current chunk [ *ùëñ* ] and the
previous chunk [ *ùëñ* ‚àí 1 ] . Specifically, *ùõæ* *[ùëó]* is the scaling factor for the *ùëó* *[ùë°‚Ñé]* row of chunk [ *ùëñ* ] from the last row of chunk [ *ùëñ* ‚àí 1 ] such that the
bigger the *ùëó* index the smaller the *ùõæ* *[ùëó]* value. Therefore, Retention recursively aggregates information from the *ùëñ* -th chunk (i.e., intra-chunk
embedding) and the previous chunk (i.e., inter-chunk embedding).
For the per-chunk state variable *ùë∫* [ *ùëñ* ], it computes current-chunk information as well as past-chunk information. The current-chunk
information *ùë≤* [‚ä§]

[ *ùëñ* ] *[ùëΩ]* [[] *[ùëñ]* []] [ decays by] *[ ùë´]* *[ùêµ]* [, which is the last row of decay matrix] *[ ùë´]* [. The past chunk information] *[ ùë∫]* [[] *[ùëñ]* [‚àí][1][]] [ is decayed with respect to]
the chunk size *ùêµ* . The initial state variable *ùë∫* [ *ùëñ* =0] = 0 is computed recurrently given the chunk size *ùêµ* :


*ùë∫* [ *ùëñ* ] = *ùë≤* [ [‚ä§] *ùëñ* ] [(] *[ùëΩ]* [[] *[ùëñ]* []] [ ‚äô] *[ùë´]* *[ùêµ]* [) +] *[ ùõæ]* *[ùêµ]* *[ùë∫]* [[] *[ùëñ]* [‚àí][1][]] [ =]


*ùêµ*
‚àëÔ∏Å *ùõæ* *[ùêµ]* [‚àí] *[ùëö]* *ùë≤* *ùëö* [‚ä§] *[ùëΩ]* *[ùëö]* [+] *[ ùõæ]* *[ùêµ]* *[ùë∫]* [ *ùëñ* ‚àí1] (18)

*ùëö* =1


Moreover, the update of state variable *ùë∫* [ *ùëñ* ] can be reformulated in parallel. The first term represents the information of current chunk, and
the second term represented the past-chunk information decayed by the chunk size *ùë©* . Consequently, *ùë∫* [ *ùëñ* ‚àí1] represents the state information
from the beginning to the ( *ùëñ* ‚àí 1)-th chunk, and we represent the inter-chunk information in chunk-wise Retention:


*ùë∫* [ *ùëñ* ‚àí1] =


*ùêµ* ‚àó *ùëñ*
‚àëÔ∏Å *ùõæ* *[ùêµ]* [‚àó] *[ùëñ]* [‚àí] *[ùëö]* *ùë≤* *ùëö* [‚ä§] *[ùëΩ]* *[ùëö]* [=] *[ ùë≤]* 1: [‚ä§] ( *ùêµ* ‚àó *ùëñ* ) [‚äô] *[ùë´]* [1:][(] *[ùêµ]* [‚àó] *[ùëñ]* [)] *[ùëΩ]* [1:][(] *[ùêµ]* [‚àó] *[ùëñ]* [)]

*ùëö* =1


( *ùë∏* [ *ùëñ* ] *ùë∫* [ *ùëñ* ‚àí1] ) ‚äô *ùúÅ*

ÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩÔøΩ **ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ** ÔøΩ

Inter-chunk


= ( *ùë∏* ( *ùêµ* ‚àó *ùëñ* ):( *ùêµ* ‚àó( *ùëñ* +1)) *ùë≤* 1: [‚ä§] ( *ùêµ* ‚àó *ùëñ* ) [‚äô] *[ùë´]* [1:][(] *[ùêµ]* [‚àó] *[ùëñ]* [)] *[ùëΩ]* [1:][(] *[ùêµ]* [‚àó] *[ùëñ]* [)] [) ‚äô] *[ùúÅ]*


= *ùë∏* ( *ùêµ* ‚àó *ùëñ* ):( *ùêµ* ‚àó( *ùëñ* +1)) *ùë≤* 1: [‚ä§] *ùêµ* ‚àó *ùëñ* [‚äô] *[ùë´]* [(] *[ùêµ]* [‚àó] *[ùëñ]* [)][:][(] *[ùêµ]* [‚àó(] *[ùëñ]* [+][1][))] *[ùëΩ]* [1:][(] *[ùêµ]* [‚àó] *[ùëñ]* [)] (19)

where the intra-chunk computation updates each row of the lower triangular matrix (highlighted as green in Fig. 1.c). Together, the recurrent
intra-chunk computation with the parallel intra-chunk computation (highlighted as purple Fig. 1c) completes the chunk-wise forward-pass
of Retention.

**Figure S2: Schematic of the TimelyGPT Pre-Training Process**


-----

TimelyGPT ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China
##### **B.3 TimelyGPT pre-training overflow**

For the TimelyGPT pre-training, we illustrate the full processes of input processing, model training, and next-token prediction in Fig. S2.
For a time-series input with *ùëá* timesteps and *ùëâ* variates, it is tokenized via a convolution-subsampling module. This tokenizer, typically
comprising two 1-D convolution layers with a kernel size of 3 and stride of 2. It produces a sequence of tokens of the shape *ùëÅ* √ó *ùëâ*, effectively
reducing the sequence length to 1/4, i.e., *ùëÅ* = 1 / 4 *ùëá* . The sequence of tokens is projected into an input embedding of the shape *ùêø* √ó *ùëë* with an
linear projection layer. As a result, the input embedding is passed through *ùêø* generative decoder layers, where the Retention mechanism
takes segmented mulitple-chunk input embedding. Finally, the output embedding of the shape *ùëÅ* √ó *ùëë* is passed through an output projection
layer, which generate a sequence of tokens with the shape of *ùêø* √ó *ùëâ* for next-token prediction.
##### **C EXPERIMENT SUMMARY**

**Table S2: Configurations of TimelyGPT, transformer baselines, and recurrent models across different datasets**

**Sleep-EDF** **PopHR**

Data Size (timesteps) 1.2B 54.9M
Model Parameters 18M 7.5M

**Timel** **y** **GPT**
Decoder Layers 12 8
Heads 8 4

Dim ( *ùë∏*, *ùë≤*, *ùëΩ*, FF ) 320,320,640,640 200,200,400,400
**Transformer baselines includin** **g** **Encoder-decoder and Encoder-onl** **y** **models**
Enc-Dec Layers 6 & 6 4 & 4
Encoder Layers 12 8
Decoder Layers 12 8
Heads 8 4

Dim ( *ùë∏*, *ùë≤*, *ùëΩ*, FF ) 384,384,384,1536 200,200,200,400
**Recurrent Models**

Layers 12 8

Dim 384 200

We summarize the setup of model architecture for TimelyGPT and other baselines for the experiments in Table S2. Additionally, we also
provide the visualization of forecasting experiment on the period signal (EEG Pz-Oz) in Fig. S3.


-----

ACM BCB ‚Äô24, Nov. 22-25, 2024, Shenzhen, Guangdong, PR China Song et al.

**Figure S3: Example of forecasting experiments on the period signal (EEG Pz-Oz). a. the groundtruth of EEG Pz-Oz singal.**
**Forecasting results are shown between 520 and 720 timesteps (b), 1800 and 2000 timesteps (c), and 5800 and 6000 timesteps (d).**
**TimelyGPT is able to forecast the periodic signals up to 6000 timesteps owing to the extrapolation capabilities.**


-----

