IEEE Journal of Biomedical and Health Informatics 1
## Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text- Guided Customization

*Abderrachid Hamrani* *[ [1]]* **, Daniela Leizaola* *[ [2]]* *, Renato Sousa* *[ [3]]* *, Jose P. Ponce* *[ [3]]* *, Stanley Mathis* *[[3][4]]* *,*
*David G. Armstrong* *[[5]]* *, Anuradha Godavarty* *[ [2]]* ***


***Abstract*** **—Diabetic foot ulcers (DFUs) pose a significant**
**challenge in healthcare, requiring precise and efficient**
**wound assessment to enhance patient outcomes. This study**
**introduces the Attention Diffusion Zero-shot Unsupervised**
**System (ADZUS), a novel text-guided diffusion model that**
**performs wound segmentation without relying on labeled**
**training data. Unlike conventional deep learning models,**
**which require extensive annotation, ADZUS leverages zero-**
**shot learning to dynamically adapt segmentation based on**
**descriptive prompts, offering enhanced flexibility and**
**adaptability** **in** **clinical** **applications.** **Experimental**
**evaluations demonstrate that ADZUS surpasses traditional**
**and state-of-the-art segmentation models, achieving an IoU**
**of 86.68% and the highest precision of 94.69% on the chronic**
**wound dataset, outperforming supervised approaches such**
**as FUSegNet. Further validation on a custom-curated DFU**
**dataset reinforces its robustness, with ADZUS achieving a**
**median DSC of 75%, significantly surpassing FUSegNet’s**
**45%. The model’s text-guided segmentation capability**
**enables real-time customization of segmentation outputs,**
**allowing targeted analysis of wound characteristics based**
**on** **clinical** **descriptions.** **Despite** **its** **competitive**
**performance, the computational cost of diffusion-based**
**inference and the need for potential fine-tuning remain areas**
**for future improvement. ADZUS represents a transformative**
**step in wound segmentation, providing a scalable, efficient,**
**and adaptable AI-driven solution for medical imaging.**

***Index Terms*** **— Diabetic foot ulcers, medical image**
**segmentation,** **self-attention** **mechanisms,** **diffusion**
**models, zero-shot learning, unsupervised learning.**

This study is partially supported by National Institutes of Health, National
Institute of Biomedical Engineering and Bioengineering Award Number
5R01EB033413, National Institute of Diabetes and Digestive and Kidney
Diseases Award Number 1R01124789, and by National Science
Foundation (NSF) Center to Stream Healthcare in Place (#C2SHiP)
CNS Award Number 2052578.
Abderrachid Hamrani is with Department of Mechanical and Materials
Engineering, Florida International University, Miami, FL 33174, USA (email: ahamrani@fiu.edu) .
Daniela Leizaola is with Optical Imaging Laboratory, Department of
Biomedical Engineering, Florida International University, Miami, FL
33174, USA (e-mail: dleiz001@fiu.edu ).
Renato Sousa is with White Memorial Medical Group, Los Angeles, CA
90033, USA (e-mail: sousadpm@gmail.com ).


I. I NTRODUCTION

iabetic foot ulcers (DFUs) are a prevalent and severe
complication among individuals with diabetes,
# D significantly impacting patient morbidity and

healthcare costs[1], [2]. As a critical manifestation of diabetic
neuropathy and peripheral vascular disease, DFUs affect
approximately 34% of the diabetic population at some point in
their lives[1], [3], [4]. The risk of lower extremity amputation
increases substantially with the presence of ulcers, making early
and accurate detection vital for effective intervention and

management [5]. However, the standard procedures for
detecting and assessing the severity of DFUs largely depend on
physical examinations conducted by healthcare professionals

[6]. This traditional approach requires significant clinical
expertise and is inherently limited by its subjective nature,
leading to variability in diagnosis and treatment outcomes. The
reliance on manual inspection makes the process not only laborintensive but also inconsistent, with potential disparities in care
depending on the practitioner's experience and the clinical
setting [7]. Visual inspections are challenged by factors such as
poor lighting, diverse skin tones, and the subtle appearance of
early ulcers, which may not be distinctly visible [8], [9]. As a
result, there is a pressing need for more objective, reliable, and
scalable methods for DFU detection and monitoring that can
support clinicians [10]–[12].
Deep learning approaches, particularly convolutional networks,
have achieved significant success in various visual recognition

Jose P. Ponce is with White Memorial Medical Group, Los Angeles, CA
90033, USA (e-mail: josepponce8@gmail.com ).
Stanley Mathis is with White Memorial Medical Group, Los Angeles, CA
90033, and Clemente Clinical Research, Los Angeles, CA 90033, USA
(e-mail: stan.mathis@clementeclinical.com) .
David G. Armstrong is with Southwestern Academic Limb Salvage
Alliance, Department of Surgery, Keck School of Medicine of University
of Southern California, Los Angeles, CA 90033 (e-mail:
armstrong.dg@gmail.com)
Anuradha Godavarty is with Optical Imaging Laboratory, Department of
Biomedical Engineering, Florida International University, Miami, FL
33174, USA (e-mail: godavart@fiu.edu ).

Color versions of one or more of the figures in this article are available
online at http://ieeexplore.ieee.org


-----

2

tasks, including biomedical image processing [13], [14]. evolution from traditional machine learning approaches to
Convolutional networks have evolved from basic architectures advanced deep learning models, showcasing the progression in
to more sophisticated designs, such as the U-Net, which was segmentation accuracy across different datasets. By presenting
tailored for biomedical image segmentation [15], [16]. The U- a range of models, from MobileNet and U-Net to conditional
Net architecture, with its contracting and expanding paths, has GANs, this brief review outlines the state of the art in wound
proven effective in capturing context and enabling precise segmentation, particularly for DFUs and other chronic wounds.
localization. Building on these advancements, diffusion models

TABLE I

introduce a fundamentally different approach to image L ITERATURE REVIEW SUMMARY OF WOUND SEGMENTATION TECHNIQUES
synthesis and analysis, shifting the focus from deterministic ACROSS DIFFERENT DATASETS .
mappings to probabilistic transformations. Diffusion models **Ref** **Methodology** **Dataset**
represent a groundbreaking advancement in the field of [25] WoundSeg using MobileNet and 950 images
generative models, having gained significant attention for their VGG16
ability to produce high-fidelity images that rival those [26] MobileNetv2 with post-processing 810 training, 200
generated by traditional methods [17]. Unlike earlier generative test images

[27] LinkNet and U-Net ensemble FUSeg Challenge

approaches such as generative adversarial networks (GANs) or

2021

variational autoencoders (VAEs) [18], [19], diffusion models [28] FCN32 with VGG backbone DFUC2022 (4000
operate through a unique process that incrementally adds noise images)
to an image and then learns to reverse this process. This iterative [29] OCRNet with ConvNeXt DFUC2022
denoising technique allows the model to gradually refine its [30] SegFormer MiT - B5 DFUC2022
predictions, enhancing its ability to reconstruct complex visual [31] K - means and SVM 767 tissue regions
information from highly corrupted inputs. By systematically [32] K-means and SVM 96 images (64

training, 32

breaking down the image generation process into a series of

validation)

reversible steps, these models gain a nuanced understanding of [33] Attention-embedded encoder- Swift Medical
the data's underlying structure. This characteristic makes decoder Wound Data Set
diffusion models particularly suited for tasks where detail and (467,000 images)
accuracy are paramount, such as medical imaging [20]. [33] CNNs with morphological Medetec database

operations

In the context of medical imaging, the strength of diffusion

[34] Conditional GAN eKare Inc. (100
models lies in their exceptional ability to capture the subtle

4000 images)

nuances and variations that are typical in medical data [21]. For [35] Hybrid deep learning with mix DFUTissue dataset
instance, in imaging modalities like MRI or CT scans, slight Transformer (110 labeled, 600
variations in tissue density or abnormality size can be crucial unlabeled images)
for accurate diagnosis and treatment planning. Diffusion [36] Adaptive-gated MLP for location AZH and Medetec
models' proficiency in handling such details offers a significant and image analysis datasets
advantage over traditional segmentation methods, which often

The table underscores a significant gap in the literature, as most

struggle with the variability and complexity of medical images.

existing methods rely on supervised learning with large labeled

Moreover, a key challenge in applying advanced machine

datasets. In contrast, our approach ADZUS introduces a novel

learning techniques to medical imaging has been the scarcity of

text-guided diffusion model that achieves competitive

high-quality, annotated datasets [22]. Annotating medical

segmentation results without requiring labeled data.

images requires expert knowledge and is time-consuming and
costly. Diffusion models offer a promising solution to this *B. Motivation*
bottleneck by reducing the reliance on extensive labeled

Producing high-quality segmentation masks for medical

datasets. Their ability to learn from and generate nuanced

images remains a fundamental challenge in biomedical image

images in an unsupervised manner potentially alters how

analysis. Recent research has explored large-scale supervised

medical image segmentation tasks are approached [23], [24].

training to enable zero-shot transfer segmentation and

The objective of this study is to harness the capabilities of self
unsupervised methods to reduce reliance on dense annotations.

attention mechanisms within diffusion models to address the

However, constructing a model capable of segmenting diverse

pressing need for effective, zero-shot image segmentation of

medical images in a zero-shot manner without any annotations

diabetic foot ulcers. By doing so, this research aims to

remains difficult. Our motivation stems from the observation

circumvent the limitations of data-dependency and improve the

that stable diffusion models, initially designed for image

generalizability of segmentation models across varied clinical

generation, can produce highly realistic and detailed images

scenarios, thereby contributing to more accurate, efficient, and

(including medical ones) based solely on text prompts.

accessible diabetic foot ulcer management.


evolution from traditional machine learning approaches to
advanced deep learning models, showcasing the progression in
segmentation accuracy across different datasets. By presenting
a range of models, from MobileNet and U-Net to conditional
GANs, this brief review outlines the state of the art in wound
segmentation, particularly for DFUs and other chronic wounds.


TABLE I

L ITERATURE REVIEW SUMMARY OF WOUND SEGMENTATION TECHNIQUES

ACROSS DIFFERENT DATASETS .


**Ref** **Methodology** **Dataset**

[25] WoundSeg using MobileNet and 950 images
VGG16



[26] MobileNetv2 with post-processing 810 training, 200
test images

[27] LinkNet and U-Net ensemble FUSeg Challenge
2021

[28] FCN32 with VGG backbone DFUC2022 (4000
images)

[29] OCRNet with ConvNeXt DFUC2022

[30] SegFormer MiT - B5 DFUC2022

[31] K - means and SVM 767 tissue regions

[32] K-means and SVM 96 images (64
training, 32
validation)

[33] Attention-embedded encoder- Swift Medical
decoder Wound Data Set
(467,000 images)

[33] CNNs with morphological Medetec database
operations



[34] Conditional GAN eKare Inc. (1004000 images)

[35] Hybrid deep learning with mix DFUTissue dataset
Transformer (110 labeled, 600
unlabeled images)

[36] Adaptive-gated MLP for location AZH and Medetec
and image analysis datasets

The table underscores a significant gap in the literature, as most
existing methods rely on supervised learning with large labeled
datasets. In contrast, our approach ADZUS introduces a novel
text-guided diffusion model that achieves competitive
segmentation results without requiring labeled data.


*B. Motivation*


*A. Literature review*

The table below provides a summary of the methodologies
used in various wound segmentation studies, emphasizing their
datasets and performance metrics. The table emphasizes the


Producing high-quality segmentation masks for medical
images remains a fundamental challenge in biomedical image
analysis. Recent research has explored large-scale supervised
training to enable zero-shot transfer segmentation and
unsupervised methods to reduce reliance on dense annotations.
However, constructing a model capable of segmenting diverse
medical images in a zero-shot manner without any annotations
remains difficult. Our motivation stems from the observation

that stable diffusion models, initially designed for image
generation, can produce highly realistic and detailed images
(including medical ones) based solely on text prompts.
Figure 1 illustrates this potential by comparing wounds
generated using stable diffusion models with real clinical cases.
The top row (a, b, c) displays diabetic foot ulcers generated
using the following text prompt:


-----

3

*"A highly detailed, realistic close-up of a diabetic foot ulcer.* *A. Overview of the stable diffusion model*
*The open wound has irregular edges, with inflamed, reddish* The Stable Diffusion Model [38], a well-known variant
*skin surrounding granulation tissue, necrotic (blackened)* within the diffusion model family [39], [40], is a generative
*areas, and yellowish slough. The surrounding skin is dry,* model that operates through both forward and reverse passes.
*cracked, and discolored with mild swelling, indicating poor* During the forward pass, Gaussian noise is incrementally added
*circulation. Captured in clinical lighting, the image highlights* at each time step until the image becomes completely isotropic
*high-resolution textures of the skin and wound."* Gaussian noise. Conversely, in the reverse pass, the model is
The generated images exhibit notable resemblance to real trained to progressively eliminate this Gaussian noise, thereby
clinical cases shown in the bottom row (d, e, f), capturing details reconstructing the original clean image. The stable diffusion
such as granulation tissue, necrosis, and skin discoloration. This model [38] incorporates an encoder-decoder and U-Net
high similarity motivates the hypothesis that the self-attention architecture with attention layers [39] (Figure 2).
mechanisms within stable diffusion models inherently capture Initially, an image *x*   *H W*   3 is compressed into a latent
medical imaging concepts, making them suitable for zero-shot *h w c* 

space with reduced spatial dimensions *z*   using an

segmentation tasks. By leveraging these properties, our
###### approach aims to bypass the need for annotated datasets while encoder z  E x  . This latent space can then be decompressed

maintaining high segmentation accuracy, thus offering a 
###### scalable solution for diverse medical imaging applications. back into the image x  D z  through a decoder. All diffusion


*A. Overview of the stable diffusion model*


Fig. 1. Comparison of Stable Diffusion-generated (a, b, c) and real
diabetic foot ulcer images (d, e, f).
Given this capability, an important question arises: has Stable
Diffusion been trained on medical images (e.g., wound-related
data), in particular the datasets used in this study? To address
this, we conducted a data similarity verification analysis using
the publicly available LAION-5B search tool [37], which forms
the primary training dataset of Stable Diffusion. The images
used in this study, including those in Figure 1 (d, e, f) and those
in the results section, were tested to determine if they were
present in Stable Diffusion's training corpus. Our analysis
revealed no identical or closely matching images within the
LAION-5B database. These findings confirm that our dataset
was not included in the pretraining data of Stable Diffusion,
ensuring that our segmentation experiments are conducted

without prior model exposure to the images used in this study .

II. M ETHODOLOGY

A pre-trained stable diffusion model is leveraged by ADZUS,
with utilization of its self-attention layers to generate highquality segmentation masks. In subsection II.A, a concise
overview of the stable diffusion model architecture will be

provided, followed by a detailed introduction to ADZUS in
subsection II.B.


The Stable Diffusion Model [38], a well-known variant
within the diffusion model family [39], [40], is a generative
model that operates through both forward and reverse passes.
During the forward pass, Gaussian noise is incrementally added
at each time step until the image becomes completely isotropic
Gaussian noise. Conversely, in the reverse pass, the model is
trained to progressively eliminate this Gaussian noise, thereby
reconstructing the original clean image. The stable diffusion
model [38] incorporates an encoder-decoder and U-Net
architecture with attention layers [39] (Figure 2).

*H W*   3
Initially, an image *x*   is compressed into a latent

*h w c* 
space with reduced spatial dimensions *z*   using an
###### encoder z  E x  . This latent space can then be decompressed


###### back into the image x  D z  through a decoder. All diffusion

processes occur within this latent space via the U-Net
architecture, which is the primary focus of this paper's
investigation. The U-Net consists of modular blocks, including
16 specific blocks composed of ResNet layers and Transformer
layers. The Transformer layer uses two attention mechanisms:
self-attention to learn global attention across the image and
cross-attention to learn attention between the image and
optional text input.

Fig. 2. Schematic of the stable diffusion configuration used in ADZUS
model, consisting of 16 blocks, each containing transformer layers that
produce a 4d self-attention tensor at various resolutions.

The component of interest for our investigation is the selfattention layer in the Transformer layer. Specifically, there are
16 self-attention layers distributed across the 16 composite
blocks, resulting in 16 self-attention tensors. Each attention

tensor A *k*   *h* *k*  *w* *k*  *h* *k*  *w* *k* is 4-dimensional. Inspired by

DiffuMask [41], which demonstrates object grouping in the
cross-attention layer, it is hypothesized that the unconditional
self-attention also contains inherent object grouping
information, which can be used to produce segmentation masks
without text inputs.

For each spatial location ( *I, J* ) in the attention tensor, the
###### corresponding 2D attention map A k  [I J],, :, :    h k  w k

captures the semantic correlation between all locations and the
location ( *I, J* ). Each location ( *I, J* ) corresponds to a region in the
original image pixel space, the size of which depends on the
receptive field of the tensor.


-----

4

Two important observations motivate the method proposed in
###### the next section: A  k  Bilinear-upsample A  k    h k  w k  64 64  (1)



- **Intra-Attention Similarity:** Within a 2D attention map
A *k*  *[I J]*,, :, : , locations tend to have strong responses if

they correspond to the same object group as ( *I, J* ) in the
original image space.

- **Inter-Attention Similarity:** Between two 2D attention
maps, e.g., A *k*  *[I J]*,, :, :  and A *k*  *[I]*  1, *J*  1, :, : , they tend

to share similar activations if ( *I, J* ) and ( *I + 1, J + 1* ) belong
to the same object group in the original image space.

The resolution of the attention map dictates the size of its
receptive field concerning the original image. Lower resolution
maps (e.g., 8×8) provide better grouping of large objects, while
higher resolution maps (e.g., 16×16) offer more fine-grained
grouping of components within larger objects, potentially
identifying smaller objects more effectively. The current stable
diffusion model has attention maps in four resolutions: 8×8,
16×16, 32×32, and 64×64. Building on these observations, a
simple heuristic is proposed to aggregate weights from different
resolutions and an iterative method to merge all attention maps
into valid segmentation masks. In our experiments, the stable
diffusion pre-trained models from “Huggingface” are used [42].
Typically, these prompt-conditioned diffusion models run for
50 or more diffusion steps to generate new images. However, to
efficiently extract attention maps for an existing clean image
without conditional prompts, we use only the unconditioned
latent and run the diffusion process once. The unconditional
latent is calculated using an unconditioned text embedding. We
set the time-step variable *t* to a large value (e.g., *t = 300* ) so that
real images are viewed as primarily denoised generated images
from the diffusion model's perspective.

*B. ADZUS model*

Since the self-attention layers capture inherent object
grouping information in spatial attention (probability) maps, we
propose ADZUS, a simple post-processing method, to
aggregate and merge attention tensors into a valid segmentation
mask. The pipeline consists of three components: attention
aggregation, iterative attention merging, and non-maximum
suppression. ADZUS is built on pre-trained stable diffusion
models. For our implementation, we use stable diffusion V1.4

[38].

*1) Attention aggregation*

Given an input image passing through the encoder and U-Net,
the stable diffusion model generates 16 attention tensors.
Specifically, there are 5 tensors for each of the dimensions: (64
× 64 × 64 × 64), (32 × 32 × 32 × 32), (16 × 16 × 16 × 16), and
(8 × 8 × 8 × 8). The goal is to aggregate attention tensors of
different resolutions into the highest resolution tensor. To
achieve this, the last 2 dimensions of all attention maps are upsampled (bilinear interpolation) to 64 × 64, their highest

resolution. Formally, for A *k*   *h* *k*  *w* *k*  *h* *k*  *w* *k* :


The first 2 dimensions indicate the locations to which attention

maps are referenced. Therefore, we aggregate attention maps
accordingly. For example, the attention map in the (0, 0)

location in A *k*   8 8  is first upsampled and then repeatedly

aggregated pixel-wise with the 4 attention maps (0, 0), (0, 1),
(1, 0), (1, 1) in A *z*   16 16  . Formally, the final aggregated

attention tensor A   64 64  is:
*f*


### A f  I J,, :, :    A k  I  k, J  k, :, :   R k (2)

*k*   1, ,16 
#### where  k 64 w k and  k [R] k [ ] 1 . The aggregated attention

map is normalized to ensure it is a valid distribution. The
weights *R* are important hyper-parameters and are proportional
to the resolution *w* *k* [. ]

*2) Iterative attention merging*

In this step, the algorithm computes an attention tensor
A *f*   64 64  . The goal is to merge the 64×64 attention maps

in the tensor A *f* to a stack of object proposals where each

proposal likely contains the activation of a single object or
category. Instead of using a K-means algorithm, which requires
specifying the number of clusters, we generate a sampling grid
from which the algorithm can iteratively merge attention maps.
A set of *M*  *M* evenly spaced anchor points is generated. We
then sample the corresponding attention maps from the tensor
A *f* . This operation yields a list of *M* 2 2D attention maps as

anchors:
##### L a   f  im jm,, :, :   64 64   im jm,   M  (3)

To measure similarity between attention maps, we use KL
divergence:


  *f*  *i j*, ,  *f*  *y z*,  


 *D*    *i j*    *y z*   


2  *D*   *f*  *i j*, ,  *f*  *y z*,


*D*   *f*  *i j*, ,  *f*  *y z*,


*f*,, *f*


(4)
######   i j    y z    KL    y z    i j   

######  KL   f  i j,   f  y z,    KL   f  y z,   f  i j,   

###### KL   f  i j,   f  y z,    KL   f  y z,   f ,

###### i j,   f  y z,    KL   f  y z,   f  i j,


*f*, *f* *y z*, *f* *y z*, *f*


We start with N iterations of the merging process, where we
compute the pair-wise distance between each element in the
anchor list and all attention maps, averaging all attention maps
with a distance smaller than a threshold  . This process is


-----

5


Fig. 3. Workflow of text-to-image diffusion integration in ADZUS for wound segmentation.


repeated in subsequent iterations, reducing the number of
proposals by merging maps with distances smaller than  .

*3) Non-maximum suppression*

The iterative attention merging step yields a list
*L* *p*  *N* *p*  64 64  of *N* object proposals in the form of attention *p*

maps. To convert the list into a valid segmentation mask, we
use non-maximum suppression (NMS). Each element is a
probability distribution map, and the final segmentation mask
*S*  512 512  is obtained by upsampling all elements in *L* to *p*

the original resolution and taking the index of the largest
probability at each spatial location across all maps. This
methodology, combining attention aggregation, iterative
merging, and non-maximum suppression, forms the core of the
ADZUS approach for producing high-quality segmentation
masks.
The iterative attention merging step yields a list of object
proposals in the form of attention maps. To convert this list into
a valid segmentation mask, we apply NMS, ensuring the
selection of the most relevant segmented regions. Rather than
directly identifying a specific anatomical structure, ADZUS
generates a comprehensive segmentation mask that delineates
multiple regions within the image, leveraging its self-attention
mechanisms to outline the boundaries of all distinguishable
structures. The model performs guided segmentation,
generating a set of segmented regions without inherently
classifying or labeling a specific area. Instead, it provides a
structured segmentation output, allowing clinicians or users to
interactively select the relevant region of interest based on the
specific medical application.

*4) Integration of text-to-image diffusion in ADZUS*

The ADZUS framework utilizes the text-to-image capabilities
inherent in stable diffusion models to enhance the segmentation
process of diabetic wounds. This methodology enables the
integration of descriptive textual inputs, thereby enriching the
feature extraction process and improving the accuracy and
robustness of the segmentation results.
A core novelty of the ADZUS model lies in its ability to
incorporate detailed medical captions to guide the segmentation


process. This integration enables the model to process latent
embeddings of input images alongside descriptive textual
prompts. The incorporation of textual guidance facilitates the
model in capturing nuanced semantic features, particularly in
scenarios where visual data alone may not sufficiently
differentiate overlapping or ambiguous wound characteristics.
This approach enhances segmentation precision by aligning
generated attention maps with clinically relevant attributes.
The integration of text-to-image diffusion into the ADZUS
framework follows a structured pipeline (Figure 3) that begins
with preprocessing input images and encoding them into a
compact latent representation using a Variational Autoencoder
(VAE), which serves as the foundation for subsequent analysis.
These latent embeddings are paired with descriptive captions to
provide clinical context during the diffusion process, aligning
segmentation outputs with detailed wound morphology features
such as necrotic tissue, granulation areas, and fibrin deposits.
Multi-resolution attention maps are then generated through selfattention mechanisms, prioritizing regions based on textual
input. The segmentation workflow leverages these outputs
through three key steps: generating text-guided weighted
attention maps, iteratively refining and merging attention maps
using Kullback-Leibler divergence and creating final
segmentation masks via Non-Maximum Suppression (NMS) to
resolve overlaps and produce coherent, clinically aligned
outputs.

III. R ESULTS

The results presented in this study aim to evaluate the
performance of ADZUS on a custom-curated dataset of wound
images, focusing on its ability to accurately segment wound
regions. The analysis compares ADZUS against conventional
methods, including CNN and K-means, as well as state-of-theart segmentation models such as Swin-Unet and
DeepLabV3Plus [43], [44]. Additionally, the concept of textguided image segmentation, demonstrating how descriptive
prompts can dynamically influence the segmentation process,
is introduced. This study evaluated ADZUS's effectiveness in
wound tissue segmentation using established metrics, including
dice similarity coefficient (DSC), intersection-over-union
(IoU), precision, and recall. A detailed description of these


-----

6

metrics is provided in the Appendix A. visualization and focus on the wound areas. Another qualitative


*A. Comparison ADZUS with state-of-the-art methods*

The first results provide a comparative analysis of ADZUS
against state-of-the-art segmentation models using the publicly
available chronic wound dataset [26]. This dataset consists of
1,010 labeled images of diabetic foot ulcers, with 810 images
designated for training and 200 for inference. It provides a
standardized platform for segmentation performance
assessment within a supervised learning framework. The stateof-the-art models includes LinkNet-EffB1+UNet-EffB2 [27],

[43], DeepLabV3Plus [44], [45], Swin-Unet [43], DDRNet

[43], SegFormer-b5 [46], and FUSegNet [43], which were used
for the comparison.
Having ADZUS as an unsupervised learning model presents a
significant challenge in this evaluation since supervised models
leverage labeled wound tracings to optimize their performance
during training. The experiments detailed in this section aim to
contextualize ADZUS’s performance within the broader
landscape of advanced segmentation techniques, emphasizing
its potential to deliver accurate and adaptable results despite its
independence from labeled training data. In this evaluation, the
prompt text used for ADZUS was: " *a detailed medical*
*photograph of a diabetic foot ulcer with necrotic tissue, slough*
*formation, granulation areas, and wound exudate surrounding*
*tissue damage.* " This descriptive input guided the segmentation
process, allowing ADZUS to generate meaningful delineations
of wound structures without relying on labeled supervision.
As illustrated in Table II, ADZUS demonstrates competitive
performance (using only the test dataset of 200 images) when
compared with state-of-the-art segmentation models. Notably,
ADZUS outperforms the previously best-performing
FUSegNet model by achieving the highest IoU of 86.68% and
the highest precision of 94.69%, compared to FUSegNet's IoU
of 86.40% and precision of 94.40%. In terms of recall score,
ADZUS achieves an impressive 92.46%, exceeding most
models, including DeepLabV3Plus and LinkNet-EffB1 +
UNet-EffB2, while remaining slightly behind in DSC compared
to FUSegNet, which achieved the highest DSC of 92.70%.
These findings collectively highlight the strength of ADZUS in
delivering accurate and reliable segmentation results without
the need for labeled training data, a significant advantage over
supervised models such as FUSegNet and SegFormer-b5.

TABLE II

P ERFORMANCE COMPARISON OF ADZUS AND STATE - OF - THE - ART MODELS

BASED ON EVALUATION METRICS .







DDRNet 57.64 80.86 66.75 73.13

Swin-Unet 79.30 89.94 87.02 88.46
SegFormer-b5 83.58 92.21 89.94 91.06
DeepLabV3Plus 85.19 92.75 91.27 92.00
LinkNet-EffB1 + UNet- 85.51 92.68 91.80 92.07

EffB2



FUSegNet 86.40 94.40 91.07 **92.70**
ADZUS (our model) **86.68** **94.69** **92.46** 91.98

The qualitative results for the chronic wound dataset are
illustrated in Figure 4. The segmentation outputs demonstrate
the performance of ADZUS in accurately delineating wound
regions, with original boundaries depicted in red and predicted
boundaries in green. The images are cropped to enhance


visualization and focus on the wound areas. Another qualitative
results for the chronic wound dataset are presented in Figure 5,
offering a comparative analysis of segmentation outputs from
ADZUS and benchmark models. Original boundaries, shown in
red, and predicted boundaries, depicted in green, are overlaid
on cropped images to enhance clarity and focus on the wound
regions. The figure highlights ADZUS's ability to achieve
precise segmentation, closely aligning with the original
boundaries and minimizing deviations. With a DSC of 93.56,
ADZUS demonstrates competitive performance, surpassing
supervised models like FUSegNet and SegFormer, while
performing slightly below DeepLabV3+. This analysis
underscores the effectiveness of ADZUS's text-guided
diffusion mechanism and self-attention features in accurately
delineating wound boundaries, even in complex and
challenging scenarios.

*B. Comparison ADZUS with FUSegNet using a custom-*
*curated dataset*

Building upon the promising results from the
comparison with state-of-the-art segmentation models, this
section evaluates ADZUS against FUSegNet model using a
custom-curated dataset of 40 white-light (WL) images of
diabetic foot ulcers (DFUs) with ground truth tracings of the
wounds (by the clinical team). The WL images, were captured
using a smartphone-based near-infrared optical imaging device

[47], [48] from 15 participants over 1-4 weeks (in an IRB
approved study), were manually traced by clinicians to
delineate wound regions. This curated dataset ensures a
consistent basis for comparing the performance of both
methods under identical conditions. Unlike ADZUS, which
requires no labeled training data, FUSegNet [43] is a supervised
model trained on the FUSeg dataset, which consists of 1,210
foot ulcer images, including 1,010 images for training and 200
images for evaluation. The previously used prompt text for
ADZUS has been maintained for this comparison.
Quantitative analysis (Figure 6) demonstrated ADZUS's
superior performance over FuSegNet, with a median DSC score
of approximately 75% and an IoU score of around 68%. In
contrast, FuSegNet exhibited lower segmentation accuracy,
with median DSC and IoU scores of approximately 45% and
50%, respectively. Figure 6 highlights these differences,
showing that ADZUS consistently achieved higher
segmentation agreement with ground truth across varying
conditions.
Qualitative comparisons further reinforce these findings. Figure
7 shows that ADZUS effectively segmented wound boundaries,
avoiding over-segmentation and peri-wound misclassification,
which were common issues with FUSegNet. The superior
performance of ADZUS is attributed to its text-guided diffusion
mechanism and self-attention capabilities, which enabled
precise identification of granulation zones while excluding
irrelevant features.


-----

7

Fig. 4. Sample qualitative segmentation on the chronic wound dataset [26]: original boundaries (red) and predicted boundaries (green) displayed on
cropped images for enhanced visualization.

Fig. 5. Sample comparison of segmentation results: ADZUS vs. benchmark models on chronic wound dataset [26], showing original boundaries (red) and
predicted boundaries (green) on cropped images.


-----

8

Fig. 6. Comparison of ADZUS with FUSegNet model based on (a) IoU and (b) DSC scores.

Fig. 7. Sample comparison of wound segmentation results using ADZUS and FUSegNet for one sample DFU case from our custom-curated dataset. Red
outlines represent clinical ground truth tracings, while blue outlines denote segmentations produced by the respective methods.


C. Demonstration of text-guided segmentation

The ability of ADZUS to perform text-guided segmentation
is demonstrated in Figure 8, where different descriptive
prompts were used to influence the segmentation results (using
an image from our custom-curated dataset of 40 white-light
(WL) images of DFUs). The leftmost panel presents the original
wound image, while the middle and right panels illustrate
segmentation outputs generated based on two distinct textual
prompts. The segmentation maps incorporate confidence
scores, visualized through a color scale, with higher confidence
regions indicated by red and lower confidence by blue. In the
first scenario, the descriptive prompt "a detailed medical
photograph highlighting infected wound" leads to a
segmentation that focuses on broader wound structures,
identifying general ulcer boundaries with relatively uniform
confidence across the segmented regions. In contrast, a second
prompt, "a wound with inflamed red tissue, swollen areas, and
early signs of infection", enhances the segmentation


granularity, highlighting additional pathological features such
as peri-wound inflammation and early-stage infection
indicators. This differentiation underscores ADZUS’s
adaptability in generating hierarchical segmentations based on
varying levels of textual detail. The first segmentation output
primarily captures the overall wound region, while the second
introduces finer delineations of tissue abnormalities. The
accuracy of this text-guided segmentation, correlating the text
prompts provided and regions it segmented, is part of our
ongoing efforts.
By integrating a text-guided diffusion mechanism, ADZUS
enables customized segmentation that aligns with specific
clinical descriptions, providing a flexible and interactive tool
for medical image analysis.


-----

9


Fig. 8. Demonstration of text-guided segmentation in ADZUS: (a) original wound image, (b) segmentation result using Prompt 1, (c)
confidence map corresponding to Prompt 1, (d) segmentation result using Prompt 2, and (e) confidence map corresponding to Prompt 2.


IV. D ISCUSSION

The findings of this study demonstrate the effectiveness of
ADZUS in wound segmentation, highlighting its ability to
perform competitively against state-of-the-art supervised
models while operating in a zero-shot learning mode. Unlike
traditional approaches that rely on extensive labeled datasets,
ADZUS segments wound regions without prior training,
leveraging its text-guided diffusion mechanism for precise
boundary delineation. The comparison with established
segmentation models, including FUSegNet, DeepLabV3Plus,
and Swin-Unet, underscores its robustness. On the publicly
available chronic wound dataset, ADZUS achieved the highest
IoU (86.68%) and precision (94.69%) among all evaluated
models, surpassing FUSegNet, which had previously been the
best-performing approach. While ADZUS exhibited a slightly
lower DSC (91.98%) than FUSegNet (92.70%), its superior
recall indicates a strong ability to capture wound regions
effectively, reducing under-segmentation risks and ensuring
comprehensive tissue identification.
Further evaluation on a custom-curated dataset of white-light
images of diabetic foot ulcers reaffirmed ADZUS’s superiority
over FUSegNet, achieving a median DSC of 75% and an IoU
of 68%, significantly outperforming FUSegNet, which reached
DSC and IoU scores of around 45% and 50%, respectively.
Qualitative analysis reinforced these findings, with ADZUS
demonstrating greater consistency in delineating wound
boundaries while mitigating peri-wound misclassification.
One of the key contributions of this study is the demonstration
of ADZUS’s text-guided segmentation capability, allowing
dynamic refinement of segmentation outputs based on
descriptive textual prompts. By adjusting segmentation
boundaries according to specified wound characteristics,
ADZUS introduces a level of flexibility not present in
traditional models. The ability to incorporate user-defined
textual input enables more precise segmentation tailored to
specific clinical needs, as demonstrated by the differentiation of
necrotic tissue, granulation zones, and exudate accumulation
based on distinct textual descriptions. This feature allows


ADZUS to refine segmentation results based on promptspecific details, where different textual descriptions led to the
segmentation of distinct wound zones, providing an
interpretable and adaptable segmentation approach that aligns
closely with expert clinical assessment.
Despite these promising results, certain limitations must be
addressed to enhance the model’s practical applicability. The
diffusion-based inference process introduces high
computational costs and prolonged processing times, which
could limit scalability in real-time clinical environments. Future
work should focus on optimizing ADZUS’s computational
efficiency through reduced-step diffusion processes or
hardware acceleration strategies. Additionally, while the model
performs well across wound segmentation tasks, expanding its
evaluation to broader medical imaging applications, such as
retinal vessel segmentation or tumor boundary detection, could
further validate its generalizability. The integration of domain
adaptation techniques and hybrid self-supervised learning
approaches may enhance performance in challenging
segmentation scenarios. Furthermore, leveraging its textguided segmentation capabilities in multimodal clinical
applications, such as incorporating electronic health records or
histopathological data, could unlock new possibilities for AIdriven medical diagnostics.

V. C ONCLUSION

In this study, we introduced the attention diffusion zero-shot
unsupervised system (ADZUS) as a novel approach for wound
segmentation, specifically targeting DFUs. The results
demonstrated the model's ability to achieve precise
segmentation without the reliance on labeled training data,
distinguishing it from conventional and state-of-the-art
supervised methods. Through a series of comparative analyses,
ADZUS consistently achieved competitive performance across
key evaluation metrics such as IoU, precision, recall, and DSC.
Notably, the model surpassed several benchmark models,
emphasizing its potential to deliver accurate wound boundary
delineations with minimal data dependency.


-----

10

The key innovation of ADZUS lies in its text-guided diffusion A CKNOWLEDGMENT
mechanism, which enables dynamic segmentation outputs

We would like to thank the residents, coordinators, and

tailored to specific descriptive prompts. This feature showcases medical staff at Clemente Clinical Research and White
the model's adaptability, allowing for customized wound

Memorial Medical Group in LA for assisting us during clinical

analysis based on clinical descriptions. However, the study also

imaging studies.

highlighted certain limitations. The computational time
required for text-guided segmentation remains a challenge.

R EFERENCES

Furthermore, while the zero-shot approach offers flexibility,
fine-tuning the model to accommodate diverse clinical datasets [1] D. G. Armstrong, T. W. Tan, A. J. M. Boulton, and S.
and contexts is necessary to enhance its robustness across A. Bus, “Diabetic Foot Ulcers: A Review,” *JAMA*,
various medical applications. vol. 330, no. 1, pp. 62–75, Jul. 2023, doi:
Looking ahead, future research should focus on optimizing the 10.1001/JAMA.2023.10578.
computational efficiency of ADZUS to facilitate its deployment [2] D. G. Armstrong, A. J. M. Boulton, and S. A. Bus,
in real-world clinical environments. Additionally, integrating “Diabetic Foot Ulcers and Their Recurrence,” *N. Engl.*
multimodal data sources, such as electronic health records and *J. Med.*, vol. 376, no. 24, pp. 2367–2375, Jun. 2017,

doi:

advanced imaging modalities, could further enrich the model's

10.1056/NEJMRA1615439/SUPPL_FILE/NEJMRA1

predictive capabilities and broaden its utility in medical

615439_DISCLOSURES.PDF.

diagnostics. Expanding the model's validation across larger,
more diverse datasets will be essential to ensure its [3] A. K. Madeshiya *et al.*, *Nutrition and diabetic wound*

*healing* . Elsevier, 2020. doi: 10.1016/B978-0-12
generalizability and reliability in different healthcare contexts.

816413-6.00020-4.

Future research will focus on expanding ADZUS's capabilities

[4] A. Godavarty, K. Leiva, N. Amadi, D. C. Klonoff, and

to additional medical imaging domains, exploring integration

D. G. Armstrong, “Diabetic Foot Ulcer Imaging: An

with real-time diagnostic systems, and advancing toward a Overview and Future Directions,” *J. Diabetes Sci.*
more autonomous segmentation framework. This next phase *Technol.*, vol. 17, no. 6, pp. 1662–1675, Nov. 2023,
will enable ADZUS to recognize, label and segment specific doi:
zones of interest requested by clinicians, such as infections or 10.1177/19322968231187660/ASSET/IMAGES/LAR
abnormal regions, leveraging its learned attention patterns. The GE/10.1177_19322968231187660-FIG1.JPEG.
promising results of this study suggest that ADZUS could [5] M. Goyal, N. D. Reeves, S. Rajbhandari, N. Ahmad,
evolve into an intelligent, semi-supervised system capable of C. Wang, and M. H. Yap, “Recognition of ischaemia
automatic region identification and preliminary labeling, and infection in diabetic foot ulcers: Dataset and
thereby enhancing its adaptability for clinical applications techniques,” *Comput. Biol. Med.*, vol. 117, p. 103616,
while maintaining interpretability and expert oversight. Feb. 2020, doi:


A CKNOWLEDGMENT


We would like to thank the residents, coordinators, and
medical staff at Clemente Clinical Research and White

Memorial Medical Group in LA for assisting us during clinical
imaging studies.


R EFERENCES


A PPENDIX

*Evaluation metrics*

In this study, the evaluation of segmentation performance
was critical to understanding the effectiveness of the ADZUS
model for medical image segmentation. Established metrics
(expressed as percentages), including dice similarity coefficient
(DSC), intersection-over-union (IoU), precision, and recall,
were adopted.

*D SC*   2 *T P*  2 *F PT P*  *F N*   100, (A1)

*IoU*   *T P*   100, (A2)
 *T P*  *F P*  *F N* 

*precision*   *T P*   100, (A3)
 *T P*  *F P* 

*recall*   *TP*   100, (A4)
 *T P*  *F N* 

Where TP, FP, and FN represent true positives, false positives,
and false negatives, respectively.



[1] D. G. Armstrong, T. W. Tan, A. J. M. Boulton, and S.
A. Bus, “Diabetic Foot Ulcers: A Review,” *JAMA*,
vol. 330, no. 1, pp. 62–75, Jul. 2023, doi:
10.1001/JAMA.2023.10578.

[2] D. G. Armstrong, A. J. M. Boulton, and S. A. Bus,
“Diabetic Foot Ulcers and Their Recurrence,” *N. Engl.*
*J. Med.*, vol. 376, no. 24, pp. 2367–2375, Jun. 2017,
doi:
10.1056/NEJMRA1615439/SUPPL_FILE/NEJMRA1
615439_DISCLOSURES.PDF.

[3] A. K. Madeshiya *et al.*, *Nutrition and diabetic wound*
*healing* . Elsevier, 2020. doi: 10.1016/B978-0-12816413-6.00020-4.

[4] A. Godavarty, K. Leiva, N. Amadi, D. C. Klonoff, and
D. G. Armstrong, “Diabetic Foot Ulcer Imaging: An
Overview and Future Directions,” *J. Diabetes Sci.*
*Technol.*, vol. 17, no. 6, pp. 1662–1675, Nov. 2023,
doi:

10.1177/19322968231187660/ASSET/IMAGES/LAR
GE/10.1177_19322968231187660-FIG1.JPEG.

[5] M. Goyal, N. D. Reeves, S. Rajbhandari, N. Ahmad,
C. Wang, and M. H. Yap, “Recognition of ischaemia
and infection in diabetic foot ulcers: Dataset and
techniques,” *Comput. Biol. Med.*, vol. 117, p. 103616,
Feb. 2020, doi:
10.1016/J.COMPBIOMED.2020.103616.

[6] J. L. Ramirez-GarciaLuna *et al.*, “Is my wound
infected? A study on the use of hyperspectral imaging
to assess wound infection,” *Front. Med.*, vol. 10, p.
1165281, Aug. 2023, doi:
10.3389/FMED.2023.1165281/BIBTEX.

[7] J. Z. M. Lim, N. S. L. Ng, and C. Thomas,
“Prevention and treatment of diabetic foot ulcers,”
*https://doi.org/10.1177/0141076816688346*, vol. 110,
no. 3, pp. 104–109, Jan. 2017, doi:
10.1177/0141076816688346.

[8] A. Hamrani *et al.*, “AI Dermatochroma Analytica
(AIDA): Smart Technology for Robust Skin Color
Classification and Segmentation,” *Cosmet. 2024, Vol.*
*11, Page 218*, vol. 11, no. 6, p. 218, Dec. 2024, doi:
10.3390/COSMETICS11060218.

[9] N. Amin and J. Doupis, “Diabetic foot disease: From
the evaluation of the ‘foot at risk’ to the novel diabetic

ulcer treatment modalities,” *World J. Diabetes*, vol. 7,
no. 7, p. 153, 2016, doi: 10.4239/WJD.V7.I7.153.

[10] W. J. Jeffcoate, L. Vileikyte, E. J. Boyko, D. G.
Armstrong, and A. J. M. Boulton, “Current Challenges
and Opportunities in the Prevention and Management
of Diabetic Foot Ulcers,” *Diabetes Care*, vol. 41, no.
4, pp. 645–652, Apr. 2018, doi: 10.2337/DC17-1836.

[11] D. G. Armstrong, L. A. Lavery, and L. B. Harkless,
“WHO IS AT RISK FOR DIABETIC FOOT
ULCERATION?,” *Clin. Podiatr. Med. Surg.*, vol. 15,


-----

no. 1, pp. 11–19, Jan. 1998, doi: 10.1016/S08918422(23)01025-X.

[12] G. A. Murphy, R. P. Singh-Moon, A. Mazhar, D. J.
Cuccia, V. L. Rowe, and D. G. Armstrong,
“Quantifying dermal microcirculatory changes of
neuropathic and neuroischemic diabetic foot ulcers
using spatial frequency domain imaging: a shade of
things to come?,” *BMJ Open Diabetes Res. Care*, vol.
8, no. 2, p. e001815, Nov. 2020, doi:
10.1136/BMJDRC-2020-001815.

[13] M. H. Yap *et al.*, “Deep learning in diabetic foot
ulcers detection: A comprehensive evaluation,”
*Comput. Biol. Med.*, vol. 135, p. 104596, Aug. 2021,
doi: 10.1016/J.COMPBIOMED.2021.104596.

[14] T. Weatherall *et al.*, “The impact of machine learning
on the prediction of diabetic foot ulcers – A systematic
review,” *J. Tissue Viability*, vol. 33, no. 4, pp. 853–
863, Nov. 2024, doi: 10.1016/J.JTV.2024.07.004.

[15] S. V. N. Murthy, K. N. Bhargavi, S. Isaac, and E.
N.Ganesh, “Automated Detection of Infection in
Diabetic Foot Ulcer Using Pre-trained Fast
Convolutional Neural Network with U++net,” *SN*
*Comput. Sci.*, vol. 5, no. 6, pp. 1–15, Aug. 2024, doi:
10.1007/S42979-024-02981-4/FIGURES/12.

[16] S. Lucho, R. Naemi, B. Castaneda, and S. Treuillet,
“Can deep learning wound segmentation algorithms
developed for a dataset be effective for another
dataset? A specific focus on diabetic foot ulcers,”
*IEEE Access*, 2024, doi:
10.1109/ACCESS.2024.3502467.

[17] R. Po *et al.*, “State of the Art on Diffusion Models for
Visual Computing,” *Comput. Graph. Forum*, vol. 43,
no. 2, p. e15063, May 2024, doi: 10.1111/CGF.15063.

[18] M. Megahed and A. Mohammed, “A comprehensive
review of generative adversarial networks:
Fundamentals, applications, and challenges,” *Wiley*
*Interdiscip. Rev. Comput. Stat.*, vol. 16, no. 1, p.
e1629, Jan. 2024, doi: 10.1002/WICS.1629.

[19] R. Wei and A. Mahmood, “Recent Advances in
Variational Autoencoders with Representation
Learning for Biomedical Informatics: A Survey,”
*IEEE Access*, vol. 9, pp. 4939–4956, 2021, doi:
10.1109/ACCESS.2020.3048309.

[20] J. Wu *et al.*, “MedSegDiff: Medical Image
Segmentation with Diffusion Probabilistic Model.”
PMLR, pp. 1623–1639, Jan. 23, 2024. Accessed: Jan.
05, 2025. [Online]. Available:
https://proceedings.mlr.press/v227/wu24a.html

[21] Z. Dong, G. Yuan, Z. Hua, and J. Li, “Diffusion
model-based text-guided enhancement network for
medical image segmentation,” *Expert Syst. Appl.*, vol.
249, p. 123549, Sep. 2024, doi:
10.1016/J.ESWA.2024.123549.

[22] N. Rania, H. Douzi, L. Yves, and T. Sylvie, “Semantic
segmentation of diabetic foot ulcer images: Dealing
with small dataset in dl approaches,” *Lect. Notes*
*Comput. Sci. (including Subser. Lect. Notes Artif.*
*Intell. Lect. Notes Bioinformatics)*, vol. 12119 LNCS,
pp. 162–169, 2020, doi: 10.1007/978-3-030-519353_17/TABLES/1.

[23] C. Wang, Z. Yu, Z. Long, H. Zhao, and Z. Wang, “A


11

few-shot diabetes foot ulcer image classification
method based on deep ResNet and transfer learning,”
*Sci. Reports 2024 141*, vol. 14, no. 1, pp. 1–9, Dec.
2024, doi: 10.1038/s41598-024-80691-w.

[24] S.-J. Kuo, P.-H. Huang, C.-C. Lin, J.-L. Li, and M.-C.
Chang, “Improving Limited Supervised Foot Ulcer
Segmentation Using Cross-Domain Augmentation,”
Jan. 2024, Accessed: Jan. 05, 2025. [Online].
Available: https://arxiv.org/abs/2401.08422v1

[25] X. Liu, C. Wang, F. Li, X. Zhao, E. Zhu, and Y. Peng,
“A framework of wound segmentation based on deep
convolutional networks,” *Proc. - 2017 10th Int.*
*Congr. Image Signal Process. Biomed. Eng.*
*Informatics, CISP-BMEI 2017*, vol. 2018-January, pp.
1–7, Jul. 2017, doi: 10.1109/CISPBMEI.2017.8302184.

[26] C. Wang *et al.*, “Fully automatic wound segmentation
with deep convolutional neural networks,” *Sci.*
*Reports 2020 101*, vol. 10, no. 1, pp. 1–9, Dec. 2020,
doi: 10.1038/s41598-020-78799-w.

[27] A. Mahbod, G. Schaefer, R. Ecker, and I. Ellinger,
“Automatic Foot Ulcer Segmentation Using an
Ensemble of Convolutional Neural Networks,” *2022*
*26th Int. Conf. Pattern Recognit.*, vol. 2022-August,
pp. 4358–4364, Aug. 2022, doi:
10.1109/ICPR56361.2022.9956253.

[28] C. Kendrick *et al.*, “Translating Clinical Delineation
of Diabetic Foot Ulcers into Machine Interpretable
Segmentation,” *arXiv*, Apr. 2022, Accessed: Jan. 07,
2025. [Online]. Available:
https://arxiv.org/abs/2204.11618v2

[29] H. Yi *et al.*, “OCRNet for Diabetic Foot Ulcer
Segmentation Combined with Edge Loss,” *Lect. Notes*
*Comput. Sci. (including Subser. Lect. Notes Artif.*
*Intell. Lect. Notes Bioinformatics)*, vol. 13797 LNCS,
pp. 31–39, 2023, doi: 10.1007/978-3-031-263545_3/TABLES/3.

[30] M. Hassib, M. Ali, A. Mohamed, M. Torki, and M.
Hussein, “Diabetic Foot Ulcer Segmentation Using
Convolutional and Transformer-Based Models,” *Lect.*
*Notes Comput. Sci. (including Subser. Lect. Notes*
*Artif. Intell. Lect. Notes Bioinformatics)*, vol. 13797
LNCS, pp. 83–91, 2023, doi: 10.1007/978-3-03126354-5_7/FIGURES/6.

[31] R. Mukherjee, D. D. Manohar, D. K. Das, A. Achar,
A. Mitra, and C. Chakraborty, “Automated Tissue
Classification Framework for Reproducible Chronic
Wound Assessment,” *Biomed Res. Int.*, vol. 2014, no.
1, p. 851582, Jan. 2014, doi: 10.1155/2014/851582.

[32] D. Li, C. Mathews, C. Zamarripa, F. Zhang, and Q.
Xiao, “Wound tissue segmentation by computerised
image analysis of clinical pressure injury photographs:
a pilot study,”
*https://doi.org/10.12968/jowc.2022.31.8.710*, vol. 31,
no. 8, pp. 710–719, Aug. 2022, doi:
10.12968/JOWC.2022.31.8.710.

[33] D. Ramachandram, J. L. Ramirez-GarciaLuna, R. D. J.
Fraser, M. A. Martínez-Jiménez, J. E. ArriagaCaballero, and J. Allport, “Fully Automated Wound
Tissue Segmentation Using Deep Learning on Mobile
Devices: Cohort Study,” *JMIR mHealth uHealth*, vol.


-----

10, no. 4, p. e36977, Apr. 2022, doi: 10.2196/36977.

[34] S. Sarp, M. Kuzlu, M. Pipattanasomporn, and O.
Guler, “Simultaneous wound border segmentation and
tissue classification using a conditional generative
adversarial network,” *J. Eng.*, vol. 2021, no. 3, pp.
125–134, Mar. 2021, doi: 10.1049/TJE2.12016.

[35] M. K. Dhar *et al.*, “Wound Tissue Segmentation in
Diabetic Foot Ulcer Images Using Deep Learning: A
Pilot Study,” Jun. 2024, Accessed: Jan. 07, 2025.

[Online]. Available:
https://arxiv.org/abs/2406.16012v1

[36] Y. Patel *et al.*, “Integrated image and location analysis
for wound classification: a deep learning approach,”
*Sci. Reports 2024 141*, vol. 14, no. 1, pp. 1–20, Mar.
2024, doi: 10.1038/s41598-024-56626-w.

[37] “Have I been Trained?” Accessed: Feb. 20, 2025.

[Online]. Available: https://haveibeentrained.com/

[38] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and
B. Ommer, “High-Resolution Image Synthesis With
Latent Diffusion Models,” in *IEEE/CVF Conference*
*on Computer Vision and Pattern Recognition (CVPR)*,
2022, pp. 10684–10695. Accessed: Jun. 04, 2024.

[Online]. Available:
https://github.com/CompVis/latent-diffusion

[39] C. Saharia *et al.*, “Photorealistic Text-to-Image
Diffusion Models with Deep Language
Understanding,” *Adv. Neural Inf. Process. Syst.*, vol.
35, pp. 36479–36494, Dec. 2022.

[40] J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion
Probabilistic Models,” *Adv. Neural Inf. Process. Syst.*,
vol. 33, pp. 6840–6851, 2020.

[41] W. Wu, Y. Zhao, M. Z. Shou, H. Zhou, and C. Shen,
“DiffuMask: Synthesizing Images with Pixel-level
Annotations for Semantic Segmentation Using
Diffusion Models,” pp. 1206–1217, 2023, doi:
10.1109/ICCV51070.2023.00117.

[42] J. Tian, L. Aggarwal, A. Colaco, Z. Kira, and M.
Gonzalez-Franco, “Diffuse Attend and Segment:
Unsupervised Zero-Shot Segmentation using Stable
Diffusion,” in *Proceedings of the IEEE/CVF*
*Conference on Computer Vision and Pattern*
*Recognition (CVPR)*, 2024, pp. 3554–3563.

[43] M. K. Dhar, T. Zhang, Y. Patel, S. Gopalakrishnan,
and Z. Yu, “FUSegNet: A deep convolutional neural
network for foot ulcer segmentation,” *Biomed. Signal*
*Process. Control*, vol. 92, p. 106057, Jun. 2024, doi:
10.1016/J.BSPC.2024.106057.

[44] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy,
and A. L. Yuille, “DeepLab: Semantic Image
Segmentation with Deep Convolutional Nets, Atrous
Convolution, and Fully Connected CRFs,” *IEEE*
*Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 4, pp.
834–848, Apr. 2018, doi:
10.1109/TPAMI.2017.2699184.

[45] L. C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H.
Adam, “Encoder-Decoder with Atrous Separable
Convolution for Semantic Image Segmentation,” *Lect.*
*Notes Comput. Sci. (including Subser. Lect. Notes*
*Artif. Intell. Lect. Notes Bioinformatics)*, vol. 11211
LNCS, pp. 833–851, Feb. 2018, doi: 10.1007/978-3030-01234-2_49.


12

[46] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M.
Alvarez, and P. Luo, “SegFormer: Simple and
Efficient Design for Semantic Segmentation with
Transformers,” *Adv. Neural Inf. Process. Syst.*, vol.
15, pp. 12077–12090, May 2021, Accessed: Jan. 05,
2025. [Online]. Available:
https://arxiv.org/abs/2105.15203v3

[47] K. Kaile, C. Fernandez, and A. Godavarty,
“Development of a Smartphone-Based Optical Device
to Measure Hemoglobin Concentration Changes for
Remote Monitoring of Wounds,” *Biosens. 2021, Vol.*
*11, Page 165*, vol. 11, no. 6, p. 165, May 2021, doi:
10.3390/BIOS11060165.

[48] K. Kaile and A. Godavarty, “Development and
Validation of a Smartphone-Based Near-Infrared
Optical Imaging Device to Measure Physiological
Changes In-Vivo,” *Micromachines 2019, Vol. 10,*
*Page 180*, vol. 10, no. 3, p. 180, Mar. 2019, doi:
10.3390/MI10030180.


-----

