1
# Embedding Empirical Distributions for Computing Optimal Transport Maps
##### Mingchen Jiang [2] [∗] Peng Xu [3] [,] [4] [∗†] Xichen Ye [1] Xiaohui Chen [4] Yun Yang [5] Yifan Chen [1] [†] 1 Hong Kong Baptist University 2 Institute of Science Tokyo 3 University of Illinois Urbana-Champaign 4 University of Southern California 5 University of Maryland, College Park ∗ Equal contribution. This work was performed while the first author was interning at Hong Kong Baptist University. † Correspondence to: Peng Xu 〈 pengxu1@illinois.edu 〉, Yifan Chen 〈 yifanc@hkbu.edu.hk 〉 .

**Abstract**

Distributional data have become increasingly prominent in modern signal processing, highlighting the necessity of computing
optimal transport (OT) maps across *multiple* probability distributions. Nevertheless, recent studies on neural OT methods predominantly focused on the efficient computation of a *single* map between two distributions. To address this challenge, we introduce
a novel approach to learning transport maps for new empirical distributions. Specifically, we employ the transformer architecture
to produce embeddings from distributional data of varying length; these embeddings are then fed into a hypernetwork to generate
neural OT maps. Various numerical experiments were conducted to validate the embeddings and the generated OT maps. The
[model implementation and the code are provided on https://github.com/jiangmingchen/HOTET.](https://github.com/jiangmingchen/HOTET)

I. I NTRODUCTION

Optimal transport (OT) theory [1] is a mathematical framework for finding the most efficient way (in the sense of minimizing
a given cost function) to transport one probability distribution to another. When the quadratic cost is used, OT theory induces a
metric space for probability measures, and the distance thereof is referred to as the 2-Wasserstein metric [2]. This notion provides
a geometric view of distributions, and therefore makes OT an invaluable tool in information theory [3]–[6]. Furthermore, OT
has already been used in many applications, such as flow-based diffusion models [7], [8], GANs [9], [10], style transfer [11],
data embedding [12], [13], multilingual alignment [14], [15], domain adaptation [16], [17], and model compression [18]–[20].
**Challenges.** In many applications, transport maps between *n* source distributions and a single target distribution are desired.
For instance, in the Wasserstein embedding [12] scheme, input distributions are represented by the OT maps that link them
to a reference distribution. Another example is the color transfer technique, where the color histogram of a reference image
is transformed to match that of other images. To obtain OT maps in these settings, conventional approaches require the use
of 2 *n* neural networks to model the dual variables in the OT problem. If a new OT map is needed between a different source
measure and the target measure, it must be computed from scratch. Computation of a single OT map is already challenging,
and the difficulty increases considerably when seeking transport maps between numerous new source distributions and a single
target distribution.
**Overview.** To make the computation of multiple OT maps more efficient and generalizable, we propose a new paradigm
(illustrated in the right panel of Figure 1) to learn the OT maps between multiple source distributions *µ* 1 *, . . ., µ* *n* and a single
target distribution *ν* . In short, samples from the source distributions are passed to a transformer-based module *E* to produce
embeddings in R *[d]* . These embeddings are then feed into hypernetworks *F* and *G* to generate parameters for the potential
networks, whose gradients approximate the OT maps. During training, samples from *ν* are passed to the potential networks
for loss computation, which propagates *ν* ’s information into *F* and *G* . Notably, no explicit embedding of *ν* is required after
the training is complete.
We refer to the whole framework as the Hypernetworks for Optimal Transport with Embedding Transformers ( **HOTET** ).
One notable strength of HOTET is that the embeddings of the input distributions in R *[d]* are directly obtained, which can then
be used in downstream learning tasks.

*A. Related Works*

There are some works that use transformers to deal with OT problems, such as embedding empirical distributions into a latent
space wherein Euclidean distances approximate OT distances [21], regularizing the training process [22], and neural network
fusion [23]. And our focus is on generating transport maps without engaging in direct computation by using transformers.
Generally speaking, the idea of generating transport maps without direct computation is not new, as it is closely related to
distributional regression [24], [25]. Recent computational developments along this line include C OND OT [26], Meta OT [27],
and GeONet [28].

*∗* Equal contribution.


-----

2


#### …













Fig. 1: The network architectures for direct computation of OT maps (left), and for our proposed method of generating OT
maps through hypernetworks (right).

In more detail, C OND OT [26] proposed to estimate a family of OT maps conditioned on a context variable, which can
then be generalized given new context; in Meta OT [27], amortized optimization was used to predict OT maps from the input
measure. Unlike the previous two, GeONet [28] learned neural operators to generate the *Wasserstein geodesics* connecting the
pair of input measures, which can be regarded as an extension to the aforementioned methods in the context of dynamic OT
problem. However, this method falls out of the scope of neural OT learning, the focus of our project.
We defer a more comprehensive comparison with existing methods to Appendix B-E.

*B. Our Contributions*

We summarize the contributions of this work as follows:

*•* We proposed a new paradigm for learning neural OT maps and distribution embeddings for multiple distributions.

*•* We employed existing benchmarks [29] and conducted various tasks to demonstrate the effectiveness of our design.

II. P RELIMINARIES

We start the review with the necessary notations for OT in Section II-A. We then introduce ICNN, hypernetwork, and
Transformer, in Section II-B. Moreover, a more comprehensive review, and the connection between attention and kernel
estimators, are provided in Appendix B; building on these we propose to embed an empirical distribution (observation points
with arbitrary sample weights) through a transformer in Section III-B, so as to generate embeddings for hypernetworks.
Notations in this work are collected in Appendix C-A.


-----

3

*A. Optimal Transport*

Optimal transport, as the name suggests, is an optimization problem. We will review its properties in this subsection.
**Monge’s problem.** Let *P* 2 (R *[d]* ) denote the set of probability measures over R *[d]* with finite second moments. Given *µ, ν ∈*
*P* 2 (R *[d]* ), Monge seeks a map *T* that push forwards *µ* to *ν* while minimizing the transportation cost. With cost *c* ( *x, y* ) = *∥x−y∥* 2 [2] [,]
a metric *W* 2 on the space *P* 2 (Ω) is induced as


*W* 2 [2] [(] *[µ, ν]* [) :=] inf
*T* # *µ* = *ν*


2 [d] *[µ]* [(] *[x]* [)] *[,]* (MP)

� R *[d]* *[∥][T]* [(] *[x]* [)] *[ −]* *[x][∥]* [2]


where *T* # *µ* = *ν* means that *ν* ( *A* ) = *µ* � *T* *[−]* [1] ( *A* )� for every Borel-measurable set *A ⊂* R *[d]* .
**Kantorovich’s relaxation.** The constraint in (MP) is highly nonlinear, which makes the solution difficult to obtain. Thus,
a linear programming relaxation of (MP), introduced by Kantorovich, is more commonly used in computation:


*W* 2 [2] [(] *[µ, ν]* [) :=] inf
*γ∈* Π( *µ,ν* )


2 [d] *[γ]* [(] *[x, y]* [)] *[.]* (KR)

� R *[d]* *×* R *[d]* *[∥][x][ −]* *[y][∥]* [2]


The constraint set Π( *µ, ν* ) in (KR) is the set of couplings between *µ* and *ν*, i.e., probability measures *γ ∈P* (R *[d]* *×* R *[d]* ) that
satisfy ( *π* *x* ) # *γ* = *µ* and ( *π* *y* ) # *γ* = *ν*, where *π* *x* ( *x, y* ) = *x*, *π* *y* ( *x, y* ) = *y* for all ( *x, y* ) *∈* R *[d]* *×* R *[d]* .
**Dual Formulation.** The relaxation (KR) admits a dual problem [1, Thm. 1.3], which reads


R *[d]* *[ ψ]* [d] *[ν]* �


1
2 [(] *[µ, ν]* [) = sup]
2 *[W]* [ 2]
*φ,ψ*


��


R *[d]* *[ φ]* [ d] *[µ]* [ +] �


s.t. ( *φ, ψ* ) *∈* *L* [1] *µ* *[×][ L]* *ν* [1]


(DP)


*φ* ( *x* ) + *ψ* ( *y* ) *≤* [1] 2

2 *[∥][x][ −]* *[y][∥]* [2]

The optimal solution for the dual exists [30, Prop. 1.11] and is known as *Kantorovich potentials* .
The formulation (DP) and its extensions are adopted in multiple computational methods for their convenience. The conjugacy
in the dual variables is typically enforced through regularization, cf. [29] for a comprehensive review. Once the optimal
dual potentials *φ* *[∗]* *, ψ* *[∗]* are obtained, the OT map (whenever exists) can be recovered as *T* *[∗]* ( *x* ) = *∇φ* ˜( *x* ), where ˜ *φ* ( *x* ) :=
*∥x∥* 2 [2] *[/]* [2] *[ −]* *[φ]* *[∗]* [(] *[x]* [)][ is a convex function. It follows that]

( *T* *[∗]* ) *[−]* [1] ( *y* ) = *y −∇ψ* *[∗]* ( *y* ) *.*

In other words, the OT map and its inverse can be obtained by differentiating the solutions of (DP).

*B. Neural Architectures*

We then introduce the neural components in learning.
**Input Convex Neural Networks** [31, ICNN] are utilized for modeling the convex potential function ˜ *φ* ( *·* ) in many neural
OT implementations [29]. More details are deferred to Appendix B-A.
**Hypernetwork** [32] is another component in our paradigm. Broadly speaking, hypernetwork refers to the neural network
architecture designed to generate parameters for another neural network. Given a target network *f* *θ*, instead of directly learning
parameters *θ* from data as traditional learning, hypernetworks generate *θ* as an output mapped from a certain context variable.
This allows for more flexible and efficient learning, particularly in tasks with complex or varying structures. Hypernetworks
have been used in tasks such as neural architecture search, meta-learning, and conditional generation [33].
**Transformers** [34], equipped with the attention modules, are primarily used in natural language processing (NLP) tasks. The
attention modules in transformers follow the spirit of nonparametric methods and can address sequences of indefinite length.
After removing the positional encoding for addressing sequences, transformers are proven universal approximators for set-to-set
maps [35, Theorem 2], thus appropriate to handle empirical distributional data (a set of samples). A complete introduction to
transformers are deferred to Appendix B-B.
While the complexity of attention is quadratic, efficient GPU implementations, such as FlashAttention [36], [37], have
dramatically accelerated the computation while maintaining a linear space complexity. Our proposed method has incorporated
the open-shelf efficient implementations as well.

III. H YPERNETWORKS FOR O PTIMAL T RANSPORT WITH E MBEDDING T RANSFORMERS

In this section, we discuss several crucial aspects of HOTET, our proposed training paradigm. Due to space limit, implementation details are deferred to Appendix A-A.


-----

4

TABLE I: Performance of the constructed forward (fwd) and inverse (inv) transport maps ( *L* [2] -UVP (%) as the metric). Lower
implies the fitted map approximates the true OT map better. The standard deviation is calculated over 10 runs.

|DIM|HOTET (fwd) MetaOT (fwd) MM-B (fwd)|HOTET (inv) MetaOT (inv) MM-B (inv)|
|---|---|---|
|2 4 8 16 32 64|5.03 11.71 0.54 ±0.33 ±0.49 ±0.06 10.06 11.63 1.91 ±0.39 ±0.46 ±0.10 11.38 23.18 5.51 ±0.41 ±0.42 ±0.15 16.91 47.61 13.17 ±0.58 ±1.03 ±0.08 20.87 59.99 25.87 ±0.56 ±0.67 ±0.59 37.62 74.02 23.63 ±0.84 ±0.80 ±0.25|0.66 15.52 0.81 ±0.03 ±0.56 ±0.03 2.01 9.27 0.48 ±0.04 ±0.42 ±0.07 3.80 20.50 2.22 ±0.23 ±0.87 ±0.05 4.64 34.16 6.27 ±0.18 ±0.47 ±0.32 17.03 44.60 10.20 ±0.47 ±0.78 ±0.26 15.07 30.88 13.95 ±0.37 ±0.89 ±0.52|



*A. Base OT solvers in HOTET*

Training the hypernetworks in HOTET requires a base OT solver to learn to generate neural networks that approximate true
OT maps (though in principle the HOTET framework is agnostic to the choice of the base solver, provided it delivers accurate
approximations). Mainstream OT solvers are mainly maximization-minimization-based, which are subject to divergence in
training; for the numerical experiments in this work, we selected the base solver in use from two instances for numerical
stability, referred to as MM-B and MMv2 in [29]. Both solvers utilize the dual OT formulation and ICNNs to approach the
convex potentials *φ* and *ψ*, whose gradients serve as approximations of the OT maps. Usage of the solvers is discussed in
Section IV-A, and detailed formulations / implementations for these solvers are provided in Appendix A-B.

*B. Embedding empirical distributions*

The key step in our training paradigm is to generate context embeddings from the input **empirical distributions** . We employ
transformers in this task for several reasons:

1) With the positional encoding removed, transformers are universal approximators for set-to-set maps [35, Thm. 2] and
permutation invariant functions [38, Prop. 2], which match the characteristics of an empirical distribution.
2) Transformers are suitable for inputs with variable sizes, which is not well handled by previous methods [26], [27].
3) The architecture of the transformer allows it to take the weights from the empirical distribution into consideration. This
aspect is explored in Appendices B-C and B-D.

A simplified process for HOTET to embed empirical distributions can be described as follows: a design matrix ***X*** (the input
empirical distribution) is passed to the *L* blocks in our transformer embedding module; the first *L −* 1 blocks follow a regular
design in [34], which keeps the input sample dimension *hd* ( *h* is the number of heads, see Appendix B-B); the MLP sub-layer
in the last block lifts the hidden dimension to the one of the context vector. A mean-pooling of the output matrix produces
the context vector, which is then fed into the hypernetworks to generate model parameters.

*C. HOTET for computing individual OT maps.*

To demonstrate the work flow of HOTET, we first discuss the computation of individual maps between two distributions
with HOTET (though HOTET is **suboptimal** in this case).
HOTET consists of three modules: the embedding network *E* and two hypernetworks *F, G* . Given a pair of empirical
distributions *µ, ν*, the embedding network *E* is applied to generate context vectors *E* ( *µ* ) *, E* ( *ν* ) as specified in Section III-B.
The two hypernetworks *F, G* will then respectively take the context *E* ( *µ* ) *, E* ( *ν* ) as inputs, and produce parameters

*θ* = *F ◦E* ( *µ* ) *,* *ω* = *G ◦E* ( *ν* ) *,*

for the two ICNNs *f* *θ* ( *·* ) and *g* *ω* ( *·* ) approximating the convex potentials. Afterwards, the potential networks are provided to the
base OT solver, which concludes the entire forward pass. The resulting gradients of the forward pass are then used to update
the three modules ( *E* and *F, G* ) with backpropagation.
As the parameter space for *θ, ω* in HOTET is strictly smaller than in the original solver, there will be natural concerns about
the representation power of HOTET. Shortly in Section IV-B, we compare HOTET with the MMv2 solver as a sanity check,
under the same setting taken by [29]; we note the proposed framework still achieves a comparable performance in unfavorable
settings. The details are provided in Appendix A-C.

*D. HOTET for computing multiple OT maps*

We then illustrate the desired scenario for HOTET where the source measures *µ* 1 *, . . ., µ* *n* (training set) and the reference
measure *ν* are already given and one aims to efficiently obtain the corresponding OT maps between a new *µ* and the reference *ν* .
In training, instead of computing OT maps individually for the *n* distribution pairs, HOTET generates the 2 *n* sets of parameters
for ICNNs together via *F* and *G* . To train the hypernetworks, loss from the base OT solver is computed for each ( *µ* *i* *, ν* ) and
subsequently aggregated together.


-----

5

TABLE II: Cosine similarity between *T* [ˆ] and *T* *[∗]* . The standard deviation is calculated over 10 runs. Best viewed zoom in.

|DIM|HOTET (fwd) MetaOT (fwd) MM-B (fwd)|HOTET (inv) MetaOT (inv) MM-B (inv)|
|---|---|---|
|2 4 8 16 32 64|0.93 0.81 0.99 ±0.01 ±0.01 ±0.01 0.87 0.84 0.96 ±0.01 ±0.01 ±0.01 0.89 0.80 0.94 ±0.01 ±0.01 ±0.01 0.90 0.74 0.92 ±0.01 ±0.01 ±0.01 0.91 0.76 0.90 ±0.01 ±0.01 ±0.01 0.84 0.71 0.90 ±0.01 ±0.01 ±0.01|0.97 0.74 0.99 ±0.01 ±0.01 ±0.01 0.96 0.88 0.97 ±0.01 ±0.01 ±0.01 0.97 0.82 0.97 ±0.01 ±0.01 ±0.01 0.97 0.78 0.96 ±0.01 ±0.01 ±0.01 0.91 0.80 0.97 ±0.01 ±0.01 ±0.01 0.91 0.87 0.94 ±0.01 ±0.01 ±0.01|



TABLE III: Performance of the constructed transport forward (fwd) and inverse (inv) maps in predicting OT maps ( *L* [2] -UVP (%)
as the metric). The standard deviation is calculated over 10 runs.

|DIM|Train|Col3|Predict|Col5|
|---|---|---|---|---|
||HOTET (fwd) MetaOT (fwd)|HOTET (inv) MetaOT (inv)|HOTET (fwd) MetaOT (fwd)|HOTET (inv) MetaOT (inv)|
|2 4 8 16 32 64|3.25 ±0.56 2.66 ±0.41 3.40 ±0.29 20.27 ±2.12 6.59 ±0.28 50.34 ±0.81 10.72 ±0.26 73.76 ±0.82 18.00 ±0.47 86.93 ±0.37 29.18 ±0.46 93.13 ±0.25|3.01 ±0.49 2.71 ±0.43 3.44 ±0.30 23.37 ±2.43 6.48 ±0.27 54.26 ±1.45 11.19 ±0.27 74.02 ±1.15 18.96 ±0.54 88.27 ±0.35 26.44 ±0.36 94.13 ±0.40|3.13 ±0.47 2.63 ±0.45 3.37 ±0.27 20.27 ±2.11 6.54 ±0.31 50.35 ±0.82 10.59 ±0.25 73.70 ±0.82 18.03 ±0.44 86.92 ±0.37 28.29 ±0.83 93.14 ±0.25|3.07 ±0.45 2.69 ±0.47 3.43 ±0.29 23.48 ±2.46 6.45 ±0.28 54.29 ±1.48 11.05 ±0.25 73.98 ±1.16 19.00 ±0.53 88.27 ±0.33 26.49 ±0.35 94.11 ±0.41|



Since the reference measure *ν* is fixed in the training and testing process, we take *µ* *i* as input to model **both the forward map**
**and the inverse map** between *µ* *i* and *ν* . That is, the parameters for the ICNNs *f* *θ* *i* (resp. *g* *ω* *i* ) are generated as *θ* *i* = *F ◦E* ( *µ* *i* )
(resp. *ω* *i* = *G ◦E* ( *µ* *i* ))

IV. N UMERICAL E XPERIMENTS

We conducted various experiments to evaluate the performance of HOTET. The detailed experimental setups are discussed in
Section IV-A. In particular, we perform a sanity check (Section IV-B) based on the benchmarks from [29] to exhibit the capability
of our paradigm to generate quality transport map. This particular setting will be referred to as **W2B** hereinafter. Furthermore,
we evaluated the prediction performance of our proposed training paradigm in Section IV-C, where we demonstrated that
HOTET is capable of generating transport maps for unseen distributions after being trained on similar sample distributions
(c.f. Section III-D). Lastly, we tested our model on images data through various applications in Section IV-D, and performed
an ablation study on the embedding module in Section IV-E.

*A. Experiment Settings*

*1) Choice of base OT solvers:* While the theory foundation for OT is solid, the choice of a better solver for a specific
task is an engineering problem. It is typical that in some cases the solvers will be hard to optimize: for example, the MMv2
solver tends to produce forward and inverse maps with large performance discrepancy, due to its asymmetric nature (detailed
in Appendix A-B). When applicable, we compare the performance of the MM-B and MMv2 solver (see Appendix A-F), and
report the results of the more stable one in the main text.
*2) W2B benchmark:* The performance of numerous OT solvers are evaluated in [29] on Gaussian mixture distributions.
The performance of an estimated transport map *T* [ˆ] with respect to the ground truth *T* *[∗]* is evaluated with the *L* [2] -Unexplained
Variance Percentage (UVP) and Cosine Similarity (CS):

*∥T* [ˆ] *−* *T* *[∗]* *∥* [2] *L* [2] ( *µ* )
*L* [2] -UVP( *T* [ˆ] ) := 100 *·* % *,*
Var( *ν* )

*⟨* *T* [ˆ] *−* id *, ∇ψ* *[∗]* *−* id *⟩* *L* 2 ( *µ* )
CS( *T* [ˆ] ) := *.*
*∥T* [ˆ] *−* id *∥* [2] *L* [2] ( *µ* ) *[· ∥][T]* *[ ∗]* *[−]* [id] *[∥]* [2] *L* [2] ( *µ* )

We inherited these settings for our own comparisons.
*3) Predicting OT maps with HOTET:* In this experiment, multiple Gaussian mixture distributions are generated with random
means and covariance matrices. One distribution is designated as the reference, while the rest are divided into training and test
sets. Distributions in the test set are used to assess the quality of the OT maps predicted by HOTET.
*4) Color transfer:* To further explore the capabilities of HOTET, we conducted color transfer experiments on paintings from
[WikiArt. The experiments include transferring colors from one image to another, as well as transferring colors from multiple](https://www.wikiart.org/)
images to a single target image.


-----

6


2 1 0 1 2 3

2 1 0 1 2 3


2

1

0

1

2

2

1

0

1

2


2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5


2 0 2

2 0 2


2 0 2


2 1 0 1 2 3


2

1

0

1

2

2

1

0

1

2


2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

(b) MM-B

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5


(a) Ground Truth

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5


2 0 2


2 1 0 1 2 3

(c) HOTET


(d) Meta OT


Fig. 2: Visualization of the transported data mapped via various methods. The ground truth data are shown in the top-left
corner. For the other figures, the left (resp. right) subplots are generated by applying the forward maps *T* ˆ *[−]* [1] ) to the corresponding ground truth data. *T* [ˆ] (resp. inverse maps

*B. W2B Benchmark*

We trained HOTET with the MM-B solver using the high-dimensional distribution dataset from W2B For the estimated
forward and inverse transport maps, we examine the divergence between *ν* and *T* [ˆ] # *µ*, as well as the one between *µ* and, *T* [ˆ] # *[−]* [1] *[ν]* [,]
which are visualized in Figure 2. To make the comparison fair, the architectures of the ICNNs used in all the methods are
identical.

The results are reported in Tables I and II, indicating that the transport maps generated by HOTET are comparable to those
trained directly. This evidence validates that HOTET is able to generate quality transport maps.

*C. Predict OT Maps with HOTET*

We then evaluated the prediction performance of HOTET. In this experiment, Gaussian mixtures with three components,
across multiple dimensions are generated, and each with different mean and covariance. One mixture *ν* was chosen as the
reference, while 500 others formed the training set. After fitting 500 OT maps under HOTET, we then predicted the OT maps
between *ν* and 100 new Gaussian mixtures (we found that MMv2 performed better in this setting than MM-B). For comparison,
Meta OT was also trained in the setting above.
The results are summarized in Table III. The prediction performance of HOTET quickly exceeds baselines as the dimension
increases, exhibiting the capability of HOTET to capture distribution embeddings. Due to space limit, evaluation on time
efficiency is deferred to Appendix A-D.
**Key note: removal of the pretraining stage.** [29] suggested a pre-training stage to turn the ICNN into an identity map,
before the regular training procedure. This process is time-consuming and unnecessary given the recent advance in transfer
learning; we followed [39] and initialized the weights *W* *h* *∼N* (0 *,* 0 *.* 1) in hypernetworks with a small variance. This way, we
utilized the residual connection within ICNNs (with the tiny initial weights the ICNN already approaches an identity map) and
thus can skip the pre-training stage.

*D. Color Transfer*

In this experiment, we used the OT map to transform the color histogram of an image to match that of another. The
transport maps were constructed by interpreting the input RGB images as samples from their respective color distributions
over the support [0 *,* 1] [3] . As before, we considered both one-to-one and many-to-one settings, employing the MM-B solver.
**One-to-one color transfer.** Given two color histograms, *x* and *y*, we trained HOTET to generate OT maps *T,* [ˆ] *T* [ˆ] *[−]* [1] . To
obtain new images, we replaced the colors of individual pixels so that the resulting color histograms became *x* trans = *T* [ˆ] # *x* and
*y* trans = *T* [ˆ] # *[−]* [1] *[y]* [, respectively. The results are shown in Figure][ 3][.]
**Multiple-to-one color transfer.** Given a collection of color histograms *X* = *{x* 1 *, x* 2 *, . . ., x* *n* *}* and a reference histogram *y*, we
trained HOTET to generate simultaneously the forward transport maps *{T* [ˆ] 1 *,* *T* [ˆ] 2 *, . . .,* *T* [ˆ] *n* *}* and inverse maps *{T* [ˆ] 1 *[−]* [1] *,* *T* [ˆ] 2 *[−]* [1] *, . . .,* *T* [ˆ] *n* *[−]* [1] *[}]* [.]


-----

7

Fig. 3: One-to-one color transfer using HOTET.

Fig. 4: Multiple-to-one color transfer using HOTET. Showing both the training and in-context learning stages.

The images with new color histograms, shown in the left panel of Figure 4, were generated in the same way as in the one-to-one
setting.
**In-context learning.** To further evaluate the generalization capacity of a trained HOTET, we followed the in-context learning
setting in [27] and reused the trained HOTET model from the multiple-to-one setting to generate OT maps for previously unseen
color histograms *x* new . Instead of a full training run (5000 iterations), we fine-tuned the HOTET on new images through only
50 warm-up steps. The results are presented in the right panel of Figure 4, demonstrating that the OT maps produced by
short-term fine-tuning are visually comparable to those generated through complete training.

*E. Ablation Studies on the Distribution Embedding Module*

In this experiment, we **removed** the embedding module to investigate whether the overall performance would be impacted.
We followed the setting in Section IV-C and the dimension is set at 64; the results are provided in Table IV. As expected,


-----

8

TABLE IV: *L* [2] -UVP (%) results of the two different methods for obtaining the forward and inverse transport maps.

|Method|Train|Predict|
|---|---|---|
||Fowrad Map Inverse Map|Fowrad Map Inverse Map|
|HOTET No Emb.|29.18 26.44 ±0.46 ±0.36 82.75 69.42 ±0.55 ±0.47|28.29 26.49 ±0.83 ±0.35 83.71 69.54 ±0.51 ±0.64|



HOTET significantly outperforms the version without embedding, highlighting the importance of the embedding module in
effectively capturing signals from empirical distributions.

V. C ONCLUSIONS AND L IMITATIONS

In this paper, we recognize the increasing needs for computation of OT maps in modern signal processing, and propose a
training paradigm HOTET to learn the OT maps between an unseen empirical distribution and a reference measure. In HOTET,
information from the input distributions is extracted by a transformer, and then passed to a hypernetwork to generate the desired
OT maps. Extensive experiments were conducted to demonstrate the efficacy of our new paradigm, showing that it is capable
of producing quality OT maps.
*Limitations:* Despite the efficacy of HOTET, it faces challenges in the many-to-many setting, where data are multiple
i.i.d. distribution pairs ( *µ* *i* *, ν* *i* ). Further exploration is needed to address this complex scenario.

VI. A CKNOWLEDGMENTS

Mingchen Jiang was supported by the “R&D Hub Aimed at Ensuring Transparency and Reliability of Generative AI Models”
project of the Ministry of Education, Culture, Sports, Science and Technology.


-----

9

R EFERENCES

[1] C. Villani, *Topics in Optimal Transportation*, ser. Graduate Studies in Mathematics. American Mathematical Society, 2003, vol. 58.

[2] V. M. Panaretos and Y. Zemel, *An invitation to statistics in Wasserstein space* . Springer Nature, 2020.

[3] Y. Polyanskiy and Y. Wu, “Wasserstein continuity of entropy and outer bounds for interference channels,” *IEEE Transactions on Information Theory*,
vol. 62, no. 7, pp. 3992–4002, 2016.

[4] Y. Cai and L.-H. Lim, “Distances between probability distributions of different dimensions,” *IEEE Transactions on Information Theory*, vol. 68, no. 6,
pp. 4020–4031, 2022.

[5] X. Cheng, J. Lu, Y. Tan, and Y. Xie, “Convergence of flow-based generative models via proximal gradient descent in wasserstein space,” *IEEE Transactions*
*on Information Theory*, vol. 70, no. 11, pp. 8087–8106, 2024.

[6] J. Fan and H.-G. Müller, “Conditional wasserstein barycenters and interpolation/extrapolation of distributions,” *IEEE Transactions on Information Theory*,
pp. 1–1, 2024.

[7] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, “Flow matching for generative modeling,” in *The Eleventh International Conference*
*on Learning Representations*, 2023.

[8] X. Liu, C. Gong, and qiang liu, “Flow straight and fast: Learning to generate and transfer data with rectified flow,” in *The Eleventh International*
*Conference on Learning Representations*, 2023.

[9] T. Salimans, H. Zhang, A. Radford, and D. Metaxas, “Improving gans using optimal transport,” *arXiv preprint arXiv:1803.05573*, 2018.

[10] T. Xu, L. K. Wenliang, M. Munn, and B. Acciaio, “Cot-gan: Generating sequential data via causal optimal transport,” *Advances in neural information*
*processing systems*, vol. 33, pp. 8798–8809, 2020.

[11] N. Kolkin, J. Salavon, and G. Shakhnarovich, “Style transfer by relaxed optimal transport and self-similarity,” in *Proceedings of the IEEE/CVF Conference*
*on Computer Vision and Pattern Recognition*, 2019, pp. 10 051–10 060.

[12] S. Kolouri, N. Naderializadeh, G. K. Rohde, and H. Hoffmann, “Wasserstein embedding for graph learning,” in *International Conference on Learning*
*Representations*, 2021.

[13] C. Moosmüller and A. Cloninger, “Linear optimal transport embedding: provable wasserstein classification for certain rigid transformations and
perturbations,” *Information and Inference: A Journal of the IMA*, vol. 12, no. 1, pp. 363–389, 09 2022.

[14] X. Lian, K. Jain, J. Truszkowski, P. Poupart, and Y. Yu, “Unsupervised multilingual alignment using wasserstein barycenter,” in *Proceedings of the*
*Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence*, 2021, pp. 3702–3708.

[15] S. Alqahtani, G. Lalwani, Y. Zhang, S. Romeo, and S. Mansour, “Using optimal transport as alignment objective for fine-tuning multilingual contextualized
embeddings,” *arXiv preprint arXiv:2110.02887*, 2021.

[16] N. Courty, R. Flamary, and D. Tuia, “Domain adaptation with regularized optimal transport,” in *Machine Learning and Knowledge Discovery in Databases:*
*European Conference, ECML PKDD 2014*, 2014.

[17] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy, “Joint distribution optimal transportation for domain adaptation,” *Advances in neural*
*information processing systems*, vol. 30, 2017.

[18] S. P. Singh and M. Jaggi, “Model fusion via optimal transport,” *Advances in Neural Information Processing Systems*, vol. 33, pp. 22 045–22 055, 2020.

[19] T. Wei, Z. Guo, Y. Chen, and J. He, “Ntk-approximating mlp fusion for efficient language model fine-tuning,” in *International Conference on Machine*
*Learning* . PMLR, 2023, pp. 36 821–36 838.

[20] M. Ai, T. Wei, Y. Chen, Z. Zeng, R. Zhao, G. Varatkar, B. D. Rouhani, X. Tang, H. Tong, and J. He, “Resmoe: Space-efficient compression of mixture
of experts llms via residual restoration,” in *31st SIGKDD Conference on Knowledge Discovery and Data Mining - Research Track*, 2025.

[21] D. Haviv, R. Z. Kunes, T. Dougherty, C. Burdziak, T. Nawy, A. Gilbert, and D. Pe’Er, “Wasserstein wormhole: Scalable optimal transport distance with
transformers,” *ArXiv*, pp. arXiv–2404, 2024.

[22] K. Kan, X. Li, and S. Osher, “Ot-transformer: A continuous-time transformer architecture with optimal transport regularization,” *arXiv preprint*
*arXiv:2501.18793*, 2025.

[23] M. Imfeld, J. Graldi, M. Giordano, T. Hofmann, S. Anagnostidis, and S. P. Singh, “Transformer fusion with optimal transport,” 2024. [Online].
[Available: https://arxiv.org/abs/2310.05719](https://arxiv.org/abs/2310.05719)

[24] Z. L. Yaqing Chen and H.-G. Müller, “Wasserstein regression,” *Journal of the American Statistical Association*, vol. 118, no. 542, pp. 869–882, 2023.

[25] N. Bonneel, G. Peyré, and M. Cuturi, “Wasserstein Barycentric Coordinates: Histogram Regression Using Optimal Transport,” *ACM Transactions on*
*Graphics*, vol. 35, no. 4, pp. 71:1–71:10, 2016.

[26] C. Bunne, A. Krause, and M. Cuturi, “Supervised training of conditional monge maps,” *Advances in Neural Information Processing Systems*, vol. 35,
pp. 6859–6872, 2022.

[27] B. Amos, S. Cohen, G. Luise, and I. Redko, “Meta optimal transport,” *arXiv preprint arXiv:2206.05262*, 2022.

[28] A. Gracyk and X. Chen, “GeONet: a neural operator for learning the Wasserstein geodesic,” *The Conference on Uncertainty in Artificial Intelligence*
*(UAI)*, 2024.

[29] A. Korotin, L. Li, A. Genevay, J. M. Solomon, A. Filippov, and E. Burnaev, “Do neural optimal transport solvers work? a continuous wasserstein-2
benchmark,” *Advances in Neural Information Processing Systems*, vol. 34, 2021.

[30] F. Santambrogio, *Optimal Transport for Applied Mathematicians* . Birkhäuser Cham, 2015.

[31] B. Amos, L. Xu, and J. Z. Kolter, “Input convex neural networks,” in *International Conference on Machine Learning* . PMLR, 2017, pp. 146–155.

[32] D. Ha, A. M. Dai, and Q. V. Le, “Hypernetworks,” in *International Conference on Learning Representations*, 2017.

[33] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, “A brief review of hypernetworks in deep learning,” 2023.

[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in*
*Neural Information Processing Systems*, 2017.

[35] C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar, “Are transformers universal approximators of sequence-to-sequence functions?” in
*International Conference on Learning Representations*, 2020.

[36] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré, “FlashAttention: Fast and memory-efficient exact attention with IO-awareness,” in *Advances in Neural*
*Information Processing Systems*, 2022.

[37] T. Dao, “FlashAttention-2: Faster attention with better parallelism and work partitioning,” 2023, technical Report.

[38] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,”
in *International conference on machine learning* . PMLR, 2019, pp. 3744–3753.

[39] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer learning
for nlp,” in *International conference on machine learning* . PMLR, 2019, pp. 2790–2799.

[40] A. Mallasto, J. Frellsen, W. Boomsma, and A. Feragen, “(q, p)-wasserstein gans: Comparing ground metrics for wasserstein gans,” *arXiv preprint*
*arXiv:1902.03642*, 2019.

[41] A. Makkuva, A. Taghvaei, S. Oh, and J. Lee, “Optimal transport mapping via input convex neural networks,” in *Proceedings of the 37th International*
*Conference on Machine Learning* . PMLR, 2020.

[42] A. Korotin, V. Egiazarian, A. Asadulaev, A. Safin, and E. Burnaev, “Wasserstein-2 generative networks,” in *International Conference on Learning*
*Representations*, 2021.

[43] R. T. Rockafellar, *Convex Analysis* . Princeton University Press, 1970.


-----

10

[44] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
pp. 2278–2324, 1998.

[45] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv preprint arXiv:1312.6114*, 2013.

[46] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for generation,” in *Proceedings of the 59th Annual Meeting of the Association for*
*Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)* . Online: Association
for Computational Linguistics, Aug. 2021, pp. 4582–4597.

[47] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J.
Colwell, and A. Weller, “Rethinking attention with performers,” in *International Conference on Machine Learning* . PMLR, 2021.

[48] Y. Chen, Q. Zeng, H. Ji, and Y. Yang, “Skyformer: Remodel self-attention with gaussian kernel and nyström method,” *Advances in Neural Information*
*Processing Systems*, vol. 34, pp. 2122–2135, 2021.

[49] L. Wasserman, *All of Nonparametric Statistics* . Springer New York, 2006.


-----

11
### **Appendix to “Embedding Empirical Distributions for Computing** **Optimal Transport Maps”**

A PPENDIX A

M ORE ON E XPERIMENTS

*A. Implementation details and discussions*

Despite the theoretical results for OT, there are a few numerical issues in implementing the HOTET framework, which

significantly impact the empirical performance without proper practical adaptation.

*1) Asymmetry in computing multiple OT maps w.r.t. one reference measure:* In the setting of Section III-D, the roles of the

two hypernetworks differ: the hypernetwork *G* for inverse maps still depends on *µ* *i* ’s, and is expected to precisely transport

the single reference measure *ν* onto different destinations. Due to the inherent asymmetry in the MMv2 solver, practically the

forward maps *φ* *i* usually show superior performance over the inverse ones.

We take the special characteristics into consideration when devising the implementation of HOTET. Specifically, for the OT

maps required in downstream applications, such as Wasserstein embedding or color transfer, we suggest setting them as the

forward maps, to obtain high-quality neural maps.

*2) Restriction on parameters of ICNN:* Recall from Section II-B that the parameters of certain weight matrices in Equa
tion (B.1) must remain non-negative. To enforce this restriction, we adopt *projected gradient descent* and formally apply and

additional ReLU activation to the selected outputs of the hypernetwork; other alternatives, such as Softplus, are evaluated while

we found ReLU is numerically the most stable choice.

*B. Implementation of the Base OT Solvers*

In this work, we considered two base OT solvers: MM-B [40] MMv2 [41]. Both solvers are proven to perform reasonably

well via benchmarking [29].

*a) MM-B Solver:* The MM-B solver is built upon a reformulation [1, Thm. 2.9] of the dual problem (DP). By dropping

the constant terms, the problem reduces to an equivalent formulation [42, Eqn. 5]:


min
*φ∈C*


*φ* ( *x* ) *dµ* ( *x* ) + *φ* *[†]* ( *y* ) d *ν* ( *y* ) *,* *φ* *[†]* ( *y* ) := sup (Cor)
�� � � *x∈* R *[d]* *[ {⟨][x, y][⟩−]* *[φ]* [(] *[x]* [)] *[}][ .]*


where *φ* *[†]* is the Fenchel conjugate [43] of *φ*, and *C* denotes the class of convex functions. The main challenge in computing

*φ* *[†]* lies in finding *x* ( *y* ) that achieves the maximum for each *y* on the support of *ν* . The MM-B solver simplifies this process

by addressing the inner problem only on minibatches sampled from *µ* and *ν* . Specifically, let *f* *θ* be an ICNN, the MM-B

solver, as implemented by [29], optimize *θ* as follows: given minibatches *X ∼* *µ* and *Y ∼* *ν* of size *B*, the loss function to

be minimized is computed as


*L* ( *θ* ) = [1]

*B*


*B*
� *f* *θ* ( *X* *j* ) *−* *f* *θ* � *X* *i* ( *j* ) � *,* *i* ( *j* ) := arg max *⟨X* *i* *, Y* *j* *⟩−* *f* *θ* ( *X* *i* ) *.*

*j* =1 *i∈* [ *B* ]


Although this approach produces a biased solution, it significantly accelerates computation.

To solve for *φ* and its conjugate simultaneously, the loss function can be symmetrized as


*,* *k* ( *j* ) := arg max *⟨Y* *k* *, X* *j* *⟩−* *g* *ω* ( *Y* *k* ) *.*
*k∈* [ *B* ]



*L* ( *θ, ω* ) = *L* ( *θ* ) +







 *B* [1]


*B*


*B*
� *g* *ω* ( *Y* *j* ) *−* *g* *ω* � *Y* *k* ( *j* ) �

*j* =1 


*B*
�


-----

12


where *g* *ω* is an ICNN that approximates the conjugate of *f* *θ* .

*b) MMv2 Solver:* The MMv2 solver, on the other hand, is based on the reformulation [41, Thm. 3.3]


min
*φ∈C*


� *φ* ( *x* ) *dµ* ( *x* ) + � max *ψ∈C* *[{⟨∇][ψ]* [(] *[y]* [)] *[, y][⟩−]* *[φ][ ◦∇][ψ]* [(] *[y]* [)] *[}]* [ d] *[ν]* [(] *[y]* [)] *[.]*


To compute the OT map, potentials *φ* and *ψ* are approximated by ICNNs *f* *θ* and *g* *ω*, respectively. The parameters *θ* and *ω* are

updated in an alternating fashion. For the inner problem, *ω* is updated through maximizing


*L* ( *ω* ) = [1]

*B*


*B*
� *⟨∇g* *ω* ( *Y* *i* ) *, Y* *i* *⟩−* *f* *θ* *◦∇g* *ω* ( *Y* *i* )

*i* =1


via SGD, with multiple iterations performed to ensure convergence. Afterward, *θ* is updated by minimizing


*L* ( *θ* ) = [1]

*B*


*B*
� *f* *θ* ( *X* *i* ) *−* *f* *θ* *◦∇g* *ω* ( *Y* *i* ) *.*

*i* =1


This procedure is repeated until both *θ* and *ω* converge to near-optimal values.

*C. Pseudocode for Network Training*

Algorithm 1 outlines the training process for HOTET using the MM-B solver. Here, *X* := *{X* *i* *}* *[n]* *i* =0 [represents the set of]

distributions, and *Y* is the reference distribution. In the case where *X* contains only a single distribution, it reduces to a classical

one-to-one OT problem. For scenarios where *n >* 1 (e.g. OT maps prediction), a batch of distributions, with size *B ≤* *n*, is

sampled from *X* . Then, from each sampled distribution and the reference *y*, batches of size *b* are drawn for evaluating the

individual OT losses. The networks *f* *θ* and *g* *ω* are ICNNs that parameterize the convex dual potentials. The hypernetworks *F*

and *G* generate the parameters for these potential networks, while *E* serving as the embedding module. The variable *K* denotes

the total number of training iterations.

TABLE V: Time cost (sec) of training different models. The batch sizes in the 32/64 dimensional settings are 256, and in
other settings are 1024. The GPU is a single Nvidia RTX 4090.

*D. Runtime analysis*

|Model|2 4 8 16|32 64|
|---|---|---|
|MetaOT HOTET-MMB HOTET-MMv2 MM-B MMv2|1161 1183 1214 1248 1228 1239 1339 1405 30972 31762 29106 30541 20695 21848 24099 24615 409284 365529 418475 411523|1405 4198 1223 2324 28150 20120 19618 52633 401511 395583|



We further assessed the time efficiency of HOTET, MetaOT, and repeating MMv2 solver in an 8-dimensional setting. The

results in Table V showed that HOTET performs similarly to MetaOT. However, training networks with MMv2 solver directly

is more time-consuming, as it requires training each distribution pair individually 500 times in this setting. Meanwhile, a direct

MMv2 solver does not have prediction capabilities, as the potentials of the trained networks only represents the transport maps

between the two input distributions.

*E. Additional Experiment Results*

Figures 5 and 6 present OT maps learned in the W2B experiment (Section IV-B) with *d* = 4 and *d* = 8. The results are

projected onto the first 2 principal directions for visualization.


-----

13

**Al** **g** **orithm 1** Trainin g Procedure of HOTET with MM-B Solver

1: **procedure** T RAIN M ODEL ( *X* *, Y, f* *θ* *, g* *ω* *, F, G, E* )
2: Initialize the parameters of each module
3: **for** t = 1 *, . . ., K* **do**

4: Sample batch *X* batch of size B from *X* .
5: *L ←* 0

6: **for** i = 1 *, . . .,* B **do**
7: Sample batches x, y of size b from *X* *i* *, Y*, respectively
8: embedding_x *←E* (x)

9: embedding_y *←E* (y)
10: *L* *xy* *←* C OMPUTE L OSS F ORWARD (x *,* y *, F,* embedding_x)
11: *L* *yx* *←* C OMPUTE L OSS I NVERSE (y *,* x *, G,* embedding_y)
12: *L ←L* +( *L* *xy* + *L* *yx* ) */* (2 *·* B)
13: **end for**

14: Update model *E, F, G* according to *L*

15: **end for**

16: **end procedure**

17:

18: **function** C OMPUTE L OSS F ORWARD (x *,* y *, F,* embedding_x)
19: *θ ←F* (embedding_x)
20: x_push *←∇f* (x *| θ* )

21: xy *←⟨* x *,* y *⟩*
22: idx_y *←* argmax(xy *−* x_push *,* dim = 0)
23: y_push *←* x[idx_y]
24: W_loss_xy *←* mean(x_push *−∇f* (y_push *| θ* ))
25: **end function**

26:

27: **function** C OMPUTE L OSS I NVERSE (y *,* x *, G,* embedding_y)
28: *ω ←G* (embedding_y)
29: y_push *←∇g* (y *| ω* )
30: yx *←⟨* y *,* x *⟩*
31: idx_x *←* argmax(yx *−* y_push *,* dim = 0)
32: x_push *←* y[idx_x]

33: W_loss_yx *←* mean(y_push *−∇g* (x_push *| ω* ))
34: **end function**

TABLE VI: Performance of the constructed forward (fwd) and inverse (inv) transport maps by MM-B and MMv2 solvers in
W2B benchmark ( *L* [2] -UVP (%) as the metric). Lower implies the fitted map approximates the true OT map better.

|DIM|HOTET-MMB (fwd) HOTET-MMv2 (fwd)|HOTET-MMB (inv) HOTET-MMv2 (inv)|
|---|---|---|
|2 4 8 16 32 64|5.03 ± 0.33 13.34 ± 0.46 10.06 ± 0.39 30.74 ± 0.55 11.38 ± 0.41 33.90 ± 0.61 16.91 ± 0.58 30.86 ± 0.56 20.87 ± 0.56 40.72 ± 0.54 37.62 ± 0.84 38.42 ± 0.51|10.49 ± 0.52 5.31 ± 0.58 15.54 ± 0.46 23.84 ± 0.67 18.79 ± 0.45 16.92 ± 0.52 25.62 ± 0.49 28.72 ± 0.39 23.04 ± 0.53 35.71 ± 0.47 22.94 ± 0.87 36.06 ± 0.62|



*F. Choosing the Solvers*

MM-B and MMv2 perform differently in the experiment settings in Section IV-B and Section IV-C, the details are in Table VI

and Table VII. Therefore, we choose the better performed one in our main paper.

*G. Validating the Embedding Module with the MNIST Dataset*

In this experiment, we examined the embeddings produced by the embedding module *E* from a HOTET trained on the

MNIST [44] handwritten digits dataset. We selected 6000 images from the training set (600 per digit), and then trained a


-----

14


3

2

1

0

1

2

3

3

2

1

0

1

2

3

4

2

0

2

4

4

2

0

2

4


6

4

2

0

2

4

6

4 2 0 2 4

(a) Ground Truth

6

4

2

0

2

4

6

4 2 0 2 4

(c) HOTET


4

3

2

1

0

1

2

3


5 0 5

5 0 5


4

3

2

1

0

1

2

3

5.0 2.5 0.0 2.5 5.0

(b) MM-B

4

3

2

1

0

1

2

3

5.0 2.5 0.0 2.5 5.0

(d) Meta OT


4 2 0 2 4

4 2 0 2 4


5.0 2.5 0.0 2.5 5.0


4 2 0 2 4


3

2

1

0

1

2

3

3

2

1

0

1

2

3


(a) Ground Truth

4

3

2

1

0

1

2

3


5.0 2.5 0.0 2.5 5.0

(c) HOTET


4 2 0 2 4


Fig. 5: Samples generated by the forward and inverse maps in *d* = 4, compared with ground truth input distributions.


6

4

2

0

2

4

6

4 2 0 2 4

(b) MM-B

6

4

2

0

2

4

6

4 2 0 2 4

(d) Meta OT


5 0 5

5 0 5


4

2

0

2

4

4

2

0

2

4


Fig. 6: Samples generated by the forward and inverse maps in *d* = 8, compared with ground truth input distributions.

VAE [45] to map them to 6000 3-dimensional latent distributions. Next, we trained a HOTET with embedding dimension

*d* = 128 to learn OT maps between the latent distributions and the reference measures *N* ( **0** *,* ***I*** 3 ). Afterwards, we visualized


-----

15

TABLE VII: Performance of the constructed forward (fwd) and inverse (inv) transport maps by MM-B and MMv2 solvers in
predicting OT maps setting ( *L* [2] -UVP (%) as the metric). Lower implies the fitted map approximates the true OT map better.

|DIM|Train|Predict|
|---|---|---|
||HOTET-MMv2 (fwd) HOTET-MMB (fwd) HOTET-MMv2 (inv) HOTET-MMB (inv)|HOTET-MMv2 (fwd) HOTET-MMB (fwd) HOTET-MMv2 (inv) HOTET-MMB (inv)|
|2 4 8 16 32 64|3.25 ± 0.56 4.23 ± 0.54 3.01 ± 0.49 4.03 ± 0.52 3.40 ± 0.29 4.95 ± 0.61 3.44 ± 0.30 3.73 ± 0.29 6.59 ± 0.28 6.95 ± 0.28 6.48 ± 0.27 7.00 ± 0.28 10.72 ± 0.26 12.60 ± 0.26 11.19 ± 0.27 12.64 ± 0.31 18.00 ± 0.47 20.36 ± 0.26 18.96 ± 0.54 20.35 ± 0.25 29.18 ± 0.46 28.69 ± 0.18 26.44 ± 0.36 28.72 ± 0.18|3.13 ± 0.47 4.12 ± 0.62 3.07 ± 0.45 3.84 ± 0.61 3.37 ± 0.27 3.57 ± 0.33 3.43 ± 0.29 3.66 ± 0.36 6.54 ± 0.31 6.80 ± 0.20 6.45 ± 0.28 6.94 ± 0.22 10.59 ± 0.25 12.66 ± 0.28 11.05 ± 0.25 12.71 ± 0.31 18.03 ± 0.44 20.00 ± 0.22 19.00 ± 0.53 19.93 ± 0.21 28.29 ± 0.83 28.74 ± 0.18 26.49 ± 0.35 28.84 ± 0.17|



the training set embeddings using *t* -SNE to assess whether the embedding module effectively captured the information from

the original images. The result is showed in Figure 7.

*H. Figure Result of Individual OT Maps*

Fig. 7: A *t* -SNE visualization of the 128-dimensional vectors produced by the embedding module for MNIST, with the true
labels used to color the points. The embeddings of the same digit clearly clustered together, indicating that the embedding
module effectively preserved the information from the original images.

A PPENDIX B

U SEFUL F ACTS

*A. ICNN*

The fully connected ICNN is a feed-forward neural network whose intermittent layer *z* *ℓ* is the activation of the linear

transformation of the previous layer, plus the affine transformation of the input *z* 0 := *x* . In other words, for *ℓ* *∈{* 1 *, . . ., L}*,

*z* *ℓ* := *σ* *ℓ* ( *A* *ℓ−* 1 *z* *ℓ−* 1 + *W* *ℓ−* 1 *x* + *b* *ℓ−* 1 ) *,* (B.1)


-----

16

where *A* is a non-negative matrix, *W, b* are regular unrestricted weight matrix and bias vector, and *σ* is a convex and non
decreasing activation function. The final ICNN output is ˜ *φ* ( *x* ) = *z* *L* for some pre-specified *L ≥* 1.

The structure of ICNN is justified by the following facts:

*•* The composition of a convex and non-decreasing function and a convex function is convex.

*•* The composition of a convex function and an affine function is convex.

*•* The non-negative sum of convex functions is also convex.

*B. Transformer*

Transformers [34] are a type of neural network architecture primarily used in natural language processing (NLP) tasks. They

are composed by *L* stacked layers, where each layer comprises of a multi-headed attention and a fully connected feed-forward

network (FFN) sub-layer. The attention sub-layer, assuming *h* heads and dimension size *d* for each head, first maps an input

*X ∈* R *[n][×][hd]* into the query ( *Q* ), key ( *K* ), and value ( *V* ) matrices through the following affine transformations:

*Q/K/V* = *XW* [ *q/k/v* ] + **1** *b* *[⊤]* [ *q/k/v* ] *[,]* (B.2)

where *Q, K, V ∈* R *[n][×][hp]*, *W* *q* *, W* *k* *, W* *v* are *hd × hd* weight matrices, and *b* *q* *, b* *k* *, b* *v* *∈* R *[N]* *[h]* *[p]* are the bias terms [1] . After the

transformation, the three components *Q, K, V* are split into *h* blocks corresponding to different heads. For example, *Q* is

re-written as *Q* = ( *Q* [(1)] *, . . ., Q* [(] *[N]* *[h]* [)] ), where each block *Q* [(] *[h]* [)] = *XW* *q* [(] *[h]* [)] + 1( *b* [(] *q* *[h]* [)] [)] *[T]* [ is an] *[ n][ ×][ p]* [ matrix, and] *[ W]* [ (] *q* *[h]* [)] *, b* [(] *q* *[h]* [)] are the

corresponding parts in *W* *q* *, b* *q* . The attention output for the *h* *[th]* head is then computed as:

***L*** [(] *[h]* [)] ***V*** [(] *[h]* [)] := softmax( ***Q*** [(] *[h]* [)] ( ***K*** [(] *[h]* [)] ) *[T]* */* *[√]* *p* ) ***V*** [(] *[h]* [)] = ( ***D*** [(] *[h]* [)] ) *[−]* [1] ***M*** [(] *[h]* [)] ***V*** [(] *[h]* [)] *,* (B.3)

where *M* [(] *[h]* [)] := exp( *Q* [(] *[h]* [)] ( *K* [(] *[h]* [)] ) *[T]* */* *[√]* *p* ) and *D* [(] *[h]* [)] is a diagonal matrix in which *D* *ii* [(] *[h]* [)] is the sum of the *i* -th row in *M* [(] *[h]* [)],

corresponding to the normalization part in softmax.

After we obtain the outputs in each head, they are concatenated as,

*L* := � *L* [(1)] *V* [(1)] *, . . ., L* [(] *[N]* *[h]* [)] *V* [(] *[N]* *[h]* [)] [�] *,* (B.4)

followed by the overall output,

*LW* *o* + 1 *b* *[T]* *o* *[,]* (B.5)

where *W* *o* and *b* *o* are similarly sized as the other matrices in Equation (B.2).

*C. Attention as Kernel Estimators*

For each head in the attention module, we have given the expression of attention output in Equation B.3. In this subsection,

we will re-write attention as a kernel estimator to show the connection.

In computing the attention output (of a single head), we have an input sequence *{x* *i* *}* *[n]* *i* =1 [(the rows in] *[ X]* [) and accordingly]

we can obtain *N* [2] key vectors *{k* *j* *}* *[N]* *j* =1 *[⊂]* [R] *[p]* [ (from the] *[ key]* [ matrix] ***[ K]*** [) and query vectors] *[ {][q]* *[i]* *[}]* *[n]* *i* =1 *[⊂]* [R] *[p]* [ (from] ***[ Q]*** [).] [3] [ The]

original goal of self-attention is to obtain the representation of each input token *x* *i* : *g* ( *x* *i* ). By denotation exchange: *q* *i* := *x* *i*

and *f* ( *q* *i* ) := *g* ( *x* *i* ), we can also understand the aforementioned self-attention module as returning the representation *f* ( *q* *i* ) of

1 To ease the notations we adopt the setting where *X, Q, K, V* have the same shape.
2 Note that *N* may not always equal *n*, such as in cross attention ( *N ̸* = *n* ) or in prefix-tuning ( *N > n* due to the prefix pretended to the key matrix) [46].
3 In this subsection we omit the superscript ( *h* ) for simplicity since the discussion is limited within a single head


-----

17

the input query vector *q* *i* through *{k* *j* *}* *[n]* *j* =1 [, which behaves as a kernel estimator [][47][], [][48][]. Specifically, for a single query]

vector *q* *i*, a Nadaraya–Watson kernel estimator [49, Definition 5.39] models its representation as,


*f* ( *q* *i* ) =


*n*

*κ* ( *q* *i* *,* *k* *j* )

� *ℓ* *j* ( *q* *i* ) *c* *j* *,* where *ℓ* *j* ( *q* *i* ) := *N* *.* (B.6)

*j* =1 � *j* *[′]* =1 *[κ]* [(] *[q]* *[i]* *[, k]* *[j]* *[′]* [)]


Here, *κ* ( *·, ·* ) is a kernel function, and *c* *j* ’s are the coefficients ( *c* *j* can either be a scalar or a vector in different applications)

that are learned during training. In this estimator, *{k* *j* *}* *[n]* *j* =1 [serve as the] *[ supporting points]* [ which help construct the representation]

for an input *q* *i* .

For kernel function *κ* ( *x, y* ) = exp � *⟨x, y⟩* */* *[√]* *p* �, we slightly abuse the notation *κ* ( ***Q*** *,* ***K*** ) to represent an *n* -by- *N* empirical

kernel matrix, whose element in the *i* -th row and the *j* -th column is *κ* ( *q* *i* *, k* *j* ) *, ∀i ∈* [ *n* ] *, j ∈* [ *N* ]. With these notations, the

representation of the transformed output will be,

***D*** *[−]* [1] *κ* ( ***Q*** *,* ***K*** ) ***C*** *,* (B.7)

where ***D*** is a diagonal matrix for row normalization in Eq. (B.6), and ***C*** is an *N* -by- *p* matrix whose *j* -th row is *c* *j* .

*D. Potential Pathways to the Incorporation of Sample Weights into Distribution Embedding*

Considering the correspondence between Equation (B.7) and the standard softmax attention in Equation (B.3), we are indeed

able incorporate the sample weights in an empirical distribution to attention (as mentioned in Section III-B). The new character

will allow a transformer to embed arbitrary empirical distributions and generate layer embeddings for hypernetworks.

Originally in transformers, all the tokens are assumed to share the equal weights, while a general empirical distribution allows

non-uniform atom masses. To address the issue, we can make an analogy between self-attention and the Nadaraya–Watson

kernel estimator in Equation (B.6).

We first rewrite *ℓ* *j* ( *q* *i* )’s in Equation (B.6) to highlight the implicitly assumed uniform sample weights:


*ℓ* *j* ( *q* *i* ) = � *Nj* *[′]* *κ* =1 ( *q* *[κ]* *i* *,* [(] *k* *[q]* *j* *[i]* *[, k]* ) *[j]* *[′]* [)] =


1
*N* *[·][ κ]* [(] *[q]* *[i]* *[, k]* *[j]* [)]
� *Nj* *[′]* =1 *N* 1 *[·][ κ]* [(] *[q]* *[i]* *[, k]* *[j]* *[′]* [)] *,*


which allows an immediate extension in the weighted case; given the normalized sample weights ***m*** = *{m* 1 *, m* 2 *, . . ., m* *n* *}*

with [�] *[N]* *j* =1 *[m]* *[j]* [ = 1][, we can modify the coefficients] *[ ℓ]* *[j]* [(] *[q]* *[i]* [)][’s in Equation (][B.6][) as]

*m* *j* *· κ* ( *q* *i* *,* *k* *j* )
*ℓ* *j* ( *q* *i* ) = � *Nj* *[′]* =1 *[m]* *[j]* *[′]* *[ ·][ κ]* [(] *[q]* *[i]* *[, k]* *[j]* *[′]* [)] *.*

Ultimately, the weighted attention is expressed as follows:


***Q*** ***K*** *T*
***D*** *[−]* [1] exp
� *√p*


***Q*** ***K*** *T*
diag ( *N* ***m*** ) ***V*** = softmax diag (ln *N* ***m*** ) ***V*** *,*
� � *√p* �


where the row normalization matrix ***D*** is reloaded as diag �exp � ***Q*** ~~*√*~~ ***K*** *p* *[T]* � *N* ***m*** �.

For the output ***H*** of the whole transformer, we can apply a weighted average pooling to obtain the final embedding:

***z*** = ***H*** *[T]* ***m*** *.* (B.8)

The embedding ***z*** will then be passed to the hypernetwork for generating the transport map.


-----

18

*E. Architecture Comparison with Existing Methods*

We noted that our proposed paradigm shared some similarities with the general ideas of C OND OT and Meta OT. Therefore,

we shall briefly discuss the differences between our methods and theirs.

The setting of C OND OT is based on a regression formulation. Therefore, the input source and target measures must be

paired. Our scheme is more flexible compared to theirs in this regard, as it allows the numbers of source and target measures

to differ. Further, the transport maps in C OND OT are modulated by having an additional context variable as an input, while

our transport maps are generated from properly trained hypernetworks. Lastly, we devised an end-to-end pipeline to extract

information from source measures by transformers, while C OND OT relied on externally given information to obtain the context

variables.

The Meta OT model, on the other hand, makes use of hypernetworks to generate the transport maps. Therefore, it is more

comparable to our proposed method. In Meta OT, however, the distributions are passed directly to the hypernetworks as inputs,

which means they must be concatenated, padded, or resized if their size mismatch. The transformer module in HOTET resolves

this matter and extract the information more efficiently, as explained in Section III-B.

A PPENDIX C

M ISCELLANIES

*A. Notations*

We denote by *E* the embedding module, and *F, G* the hynernetworks in HOTET, respectively. Given distributions *µ* and

*ν*, we use *T* *µ→ν* to denote the true OT map that pushforwards *µ* to *ν*, and omit the subscript when the context is clear.

Accordingly, we use *φ, ψ* to denote the potential functions and *f, g* to denote the networks used for approximating *φ, ψ* .

*B. Hyperparameters*

For the detail settings, Tables VIII to X show the hyperparameters we used in our experiments.

|Model|LR Batch Size Total Iterations|
|---|---|
|HOTET MetaOT MM-B|10−3 1024 5000 10−3 1024 5000 10−3 1024 5000|



TABLE VIII: Hyperparamter in the W2B experiment.

|Model|LR Batch Size (data) Batch Size (distributions) Total Iterations|
|---|---|
|HOTET MetaOT MMv2|10−3 1024 (dim=2,4,8,16) / 256 (dim=32, 64) 8 5000 10−3 1024 (dim=2,4,8,16) / 256 (dim=32, 64) 8 5000 10−3 1024 N/A 5000|



TABLE IX: Hyperparamter in the OT maps prediction experiment.

|Model|LR Batch Size (data) Batch Size (images) Total Iterations|
|---|---|
|HOTET (one-to-one) HOTET (multi-to-one)|10−3 1024 N/A 5000 10−3 1024 8 5000|



TABLE X: Hyperparamter in the color transfer experiment.


-----

