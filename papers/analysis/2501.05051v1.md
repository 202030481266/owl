# 2501.05051V1 - 论文总结



# Title and authors of the Paper  
Title: On the Generalizability of Transformer Models to Code Completions of Different Lengths  
Authors: Nathan Cooper (Stability AI, USA), Rosalia Tufano, Gabriele Bavota (Università della Svizzera italiana, Switzerland), Denys Poshyvanyk (William & Mary, USA)  

# Main Goal and Fundamental Concept  
The primary objective of the research is to investigate whether Transformer models trained on code completions of specific sequence lengths can generalize to code completions of unseen lengths (shorter or longer) during inference. The core hypothesis is that positional encoding schemes (originally proposed in NLP to improve length generalization) may not effectively extend to encoder-decoder Transformers used in code-related tasks like code completion. The study aims to validate this by evaluating four positional encoding schemes (Sinusoidal, xPOS, ALiBi, and T5) across two programming languages (Python and Java) and different completion tasks (statement-level for Java, block-level for Python).  

# Technical Approach  
The study employs a large empirical evaluation with the following key steps:  
1. **Dataset Construction**: Four datasets (short, medium, long, and mixed) were built for Python and Java. These datasets feature code completions with controlled input lengths (tokens) but consistent prediction complexity (11 masked tokens). The mixed dataset combines short, medium, and long instances.  
2. **Model Training**: 32 Transformer models (4 positional encodings × 4 datasets × 2 languages) were trained using fixed hyperparameters (Adam optimizer, cosine scheduler, 5 epochs) to ensure comparability.  
3. **Evaluation**: Models were tested on unseen short, medium, and long test sets. Performance was measured using metrics including Exact Match (EM), ChrF, and RougeL, which are proxies for code generation quality.  

# Distinctive Features  
- **Focus on Encoder-Decoder Transformers**: Unlike most NLP studies (which use decoder-only models), this work evaluates encoder-decoder architectures common in code tasks.  
- **Cross-Language and Task Analysis**: The study spans two languages (Python, Java) and two completion tasks (statement-level, block-level), enhancing generalizability.  
- **Comprehensive Scheme Comparison**: Four positional encoding schemes (Sinusoidal, xPOS, ALiBi, T5) are evaluated, covering absolute and relative positional encoding types.  

# Experimental Setup and Results  
- **Datasets**: Derived from GitHub projects (4M Python, 4.5M Java functions), filtered to exclude duplicates and long sequences (>1024 tokens). Split into train/validation/test sets (80%/10%/10%).  
- **Key Results**:  
  - All positional encodings suffer significant performance degradation (e.g., EM drops by 84% on average) when tested on unseen lengths.  
  - T5 outperforms other schemes (e.g., higher EM, ChrF, RougeL scores) but still fails to generalize to unseen lengths (e.g., 13.4% EM drop for long test sets).  
  - Training on mixed-length datasets reduces performance gaps but does not eliminate generalization issues.  

# Advantages and Limitations  
**Advantages**:  
- Provides a systematic benchmark for evaluating length generalization in code completion models.  
- Identifies T5 as the most effective positional encoding scheme for code tasks, despite its limitations.  
- Highlights mixed-length training as a practical compromise to reduce performance variability.  

**Limitations**:  
- Relies on proxy metrics (EM, ChrF, RougeL) rather than functional correctness, which is harder to measure.  
- Focuses on two languages and tasks; results may not extend to other code-related tasks (e.g., code summarization).  
- Training costs limit scalability to larger models or datasets.  

# Conclusion  
The study concludes that current positional encoding schemes (including those from NLP) do not generalize well to code completions of unseen lengths. While T5 performs best, all schemes exhibit significant performance degradation. Training on mixed-length datasets is a safer practical choice, but no "shortcut" exists to avoid training on representative lengths. Future work should explore novel architectures or modifications to improve length generalization in code-specific Transformers.