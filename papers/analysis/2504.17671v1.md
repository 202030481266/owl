# 2504.17671V1 - 论文总结



# Title and authors of the Paper  
**Title:** DATA-DRIVEN CALIBRATION OF PREDICTION SETS IN LARGE VISION-LANGUAGE MODELS BASED ON INDUCTIVE CONFORMAL PREDICTION  
**Authors:** Yuanchang Ye, Yanwen Wei (Zhejiang University of Finance & Economics, HangZhou, China)  

# Main Goal and Fundamental Concept  
The primary objective of this research is to mitigate hallucinations in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks, ensuring reliable and statistically guaranteed outputs. The core hypothesis is that by leveraging Split Conformal Prediction (SCP), a model-agnostic uncertainty quantification framework, LVLMs can generate prediction sets with controlled error rates (user-specified risk level $\alpha$), thereby addressing the critical issue of overconfident, inaccurate outputs in safety-critical applications.  

# Technical Approach  
The study employs a Split Conformal Prediction (SCP) framework, which involves three key steps:  
1. **Data Partitioning:** Data is split into calibration and test sets.  
2. **Nonconformity Score Calculation:** For each calibration sample, a nonconformity score $S(x,y) = 1 - \hat{f}(y|x)$ is computed, where $\hat{f}(y|x)$ is the model’s confidence in predicting answer $y$ for input $x$.  
3. **Prediction Set Construction:** Using the calibration set’s nonconformity scores, a quantile threshold $\tau$ (based on the $(1-\alpha)$ quantile) is determined. For test inputs, the prediction set includes all answers $y$ where $S(x_{\text{test}}, y) \leq \tau$, ensuring the true answer is covered with probability $\geq 1-\alpha$.  

This approach requires no model retraining and assumes only data exchangeability (weaker than i.i.d.), making it generalizable to pretrained LVLMs.  

# Distinctive Features  
Key innovations include:  
- **Statistical Guarantees:** Rigorous control of marginal coverage, ensuring empirical error rates remain strictly below $\alpha$.  
- **Dynamic Set Sizing:** Prediction set sizes adjust inversely with $\alpha$ (smaller $\alpha$ allows larger sets, filtering low-confidence outputs).  
- **Model-Agnosticism:** Applies to any pretrained LVLM without retraining, avoiding distributional assumptions or external validation.  
- **Cross-Modal Adaptation:** Extends SCP to multimodal VQA tasks, addressing unique challenges of visual-textual interaction.  

# Experimental Setup and Results  
**Datasets:** Evaluated on ScienceQA (K-12 education) and MMMU (university-level exams), with 21K+ and 11.5K samples, respectively.  
**Models:** Tested 8 LVLMs (e.g., LLaVA-1.5, Qwen2-VL, InternVL2) across 4 model groups.  
**Key Results:**  
- **Empirical Error Control:** For all $\alpha$, empirical error rates (e.g., ScienceQA with $\alpha=0.6$) remained strictly below $\alpha$, validating SCP’s coverage guarantees.  
- **Prediction Set Dynamics:** Set sizes decreased monotonically with increasing $\alpha$, aligning with theoretical expectations (e.g., Qwen2-VL-7B-Instruct showed saturated error rates beyond $\alpha=0.6$).  
- **Robustness:** Performance was stable across calibration-test split ratios (e.g., 0.5 split), underscoring real-world applicability.  

# Advantages and Limitations  
**Advantages:**  
- **Reliability:** Provides statistically valid coverage guarantees, critical for safety-critical domains (healthcare, autonomous systems).  
- **Scalability:** Model-agnostic and distribution-free, requiring minimal computational overhead (no retraining).  
- **Flexibility:** Adaptable to diverse LVLMs and datasets, with tunable $\alpha$ for risk tolerance.  

**Limitations:**  
- **Data Exchangeability:** Relies on exchangeable data; non-exchangeable data may degrade guarantees.  
- **Score Sensitivity:** Prediction set size anomalies (e.g., InternVL2-1B at $\alpha=0.1$) highlight sensitivity to nonconformity score distributions, requiring careful calibration.  

# Conclusion  
This work introduces a SCP-based framework to quantify uncertainty in LVLMs for VQA, offering rigorous statistical guarantees, dynamic prediction set control, and robustness across models and datasets. By bridging theoretical reliability with practical applicability, it provides a scalable solution for mitigating hallucinations in safety-critical multimodal AI systems.