# 2504.17376V1 - 论文总结



# Summary of Academic Paper's Technical Approach  

## Title and authors of the Paper  
Title: *On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration*  
Authors: Maoyang Xiang, Ramesh Fernando, Bo Wang (Singapore University of Technology and Design)  

## Main Goal and Fundamental Concept  
**Primary Objective**: To enable efficient deployment of the Qwen2.5-0.5B large language model (LLM) on edge devices (specifically the Xilinx Kria KV260 platform) by addressing challenges of high computational demands, memory bandwidth constraints, and energy consumption.  

**Core Idea**: Leverage model compression (via Activation-aware Weight Quantization, AWQ) and hardware acceleration (via FPGA parallelism) to reduce memory footprint, optimize data transfer, and accelerate compute-intensive operations, enabling real-time LLM inference on resource-constrained edge devices.  

## Technical Approach  
The methodology combines software and hardware optimizations:  

1. **Software Optimization (Model Compression)**:  
   - Uses **AWQ** (Activation-aware Weight Quantization) to compress model weights to low precision (INT4/INT3) while preserving accuracy by protecting "salient" weights (1% of weights critical for performance).  
   - Implements a customized **AWQ MACRO** packing scheme: groups quantized weights, scales, and zero values into 128-bit blocks, enabling efficient memory bandwidth utilization and on-the-fly dequantization during inference.  
   - Adopts a Group Size (GS) of 64 (vs. default 128) to improve accuracy on the WNLI benchmark.  

2. **Hardware Optimization (FPGA Acceleration)**:  
   - Designs a **pipelined accelerator** on the Xilinx Kria KV260’s FPGA (Programmable Logic, PL) to handle matrix multiplications (dominant workload in LLM inference).  
   - Uses a 4-channel AXI interface for high-throughput data streaming to 4 parallel MACRO MAC units.  
   - Implements an 8×8 Processing Element (PE) array with an adder tree for parallel dequantization and multiply-accumulate (MAC) operations.  
   - Hybrid execution: Offloads compute-heavy tasks (matrix multiplications) to FPGA; uses ARM Cortex-A53 CPU (Processing System, PS) for lighter tasks (non-linear operations).  

## Distinctive Features  
- **AWQ-FPGA Synergy**: Integrates AWQ compression with FPGA acceleration, addressing both memory and compute bottlenecks unique to edge deployment.  
- **Custom AWQ MACRO**: Optimizes weight packing to align with FPGA memory access patterns, minimizing bandwidth usage and enabling pipelined dequantization.  
- **Hybrid CPU-FPGA Execution**: Dynamically balances workload between CPU (light tasks) and FPGA (compute-heavy tasks), maximizing resource utilization.  

## Experimental Setup and Results  
**Setup**: Evaluated on the Xilinx Kria KV260 platform (ARM Cortex-A53 CPU + FPGA). Baseline: CPU-only inference with compiler optimizations. Metrics: model size, throughput (tokens/sec), and a composite benchmark score (accuracy, memory, prefill/decode throughput).  

**Results**:  
- **Model Compression**: Reduced model size from 988 MB to 443.81 MB (55.1% compression).  
- **Throughput**: Improved from 2.8 tokens/sec (baseline) to 5.1 tokens/sec (nearly doubling performance).  
- **Benchmark Score**: Achieved 0.55 (vs. 0.4 baseline), combining accuracy (WNLI benchmark), memory efficiency, and throughput gains.  

## Advantages and Limitations  
**Advantages**:  
- High compression rate reduces memory footprint, easing edge deployment constraints.  
- FPGA acceleration significantly improves throughput for compute-heavy matrix operations.  
- Hybrid execution optimizes resource use, balancing CPU and FPGA workloads.  

**Limitations**:  
- MAC operations are performed in FP32 (KV260 lacks native lower-precision FP support), potentially limiting further efficiency gains.  
- Platform-specific (Xilinx Kria KV260), reducing portability to other edge devices.  

## Conclusion  
The proposed framework combines AWQ-based model compression with FPGA acceleration to enable efficient on-device LLM inference. By optimizing memory bandwidth (via AWQ MACRO) and accelerating matrix operations (via a PE array), it achieves a 55.1% compression rate and 5.1 tokens/sec throughput—outperforming CPU-only baselines. While limited by platform specificity and FP32 constraints, the work demonstrates a viable path for deploying LLMs on resource-constrained edge devices.